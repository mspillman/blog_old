{
  
    
        "post0": {
            "title": "Solving crystal structures with GALLOP and profile $\chi^2$",
            "content": "Introduction . The correlated integrated intensity $ chi^2$ figure of merit has been shown to be equivalent to the profile $ chi^2$, but is more efficient to calculate in CPU-based code. This is due to the sparsity of the inverse covariance matrix (typically only 1-5% of the elements will be non-zero if using DASH&#39;s 20% correlation cut-off), which means that only a small number of non-zero elements need to be multiplied and summed. Code that exploits this property can therefore obtain very high performance - this is the approach embodied in the fortran code used in DASH. . The intensity $ chi^2$ figure of merit in GALLOP does not exploit the sparsity of the inverse covariance matrix; instead the matrix is treated as dense and a full matrix multiplication is performed. This is because very fast matrix multiplication routines which make effective use of the parallel processing capabilities of GPUs are available in cuBLAS. Sparse matrices are currently less well supported, and though there has been a lot of progress, performance remains generally worse. . This got me thinking: given that we aren&#39;t currently able to make best use of the sparsity of the inverse covariance matrix in GALLOP and instead treat it as a dense matrix, can the profile $ chi^2$ also be implemented in a manner that is amenable to GPU acceleration, and if so, how does the performance compare to the intensity $ chi^2$ figure of merit that is currently used? . To tackle this, we&#39;ll take a look at the output files produced by DASH during its Pawley fitting procedure, and work out how the full profile can be (rapidly) reconstructed. . DASH Pawley output files . Following Pawley refinement in DASH, several files are produced: . .dsl - wavelength, peak shapes and Pawley refinement settings | .hcv - hkl indices, intensities and (inverse) esds as well as the off-diagonal elements of the inverse correlation matrix, from which the inverse covariance matrix is constructed. | .pik - the (background subtracted) profile, unit intensity peaks, esds (we&#39;ll look at this in more detail below) | .sdi - unit cell, space group, Pawley profile $ chi^2$ figure of merit | .tic - hkl, twotheta | . GALLOP contains functions that read these files, which can be found here. . To calculate the profle $ chi^2$, we&#39;ll need to take a closer look at the PIK file. . PIK file . This file contains all of the information we&#39;ll need to calculate the profile $ chi^2$. . In general, the lines follow the following structure: . twotheta intensity ESD Number of peaks contributing to this point . If the number of peaks contributing to a particular point is zero, then this is just background noise, and the next line will have the same format. Here&#39;s an example of a section of background intensity from the PIK file for verapamil hydrochloride: . 5.016000 -323.4935 202.1800 0 5.031000 -382.2603 201.2000 0 5.045000 -315.5720 201.4600 0 5.060000 -250.9787 201.7000 0 . However, if the number of peaks that contribute to a point is greater than 0, then the next line(s) contain information about the contributing peaks. If there are N peaks that contribute intensity to a particular point, then the next line(s) will have the following structure: . twotheta intensity ESD Number of peaks contributing to this point N x [peak number intensity for peak number] . where &quot;peak number&quot; is the position of the particular peak in a sorted list of all of the peak $2 theta$ positions (i.e. peak number 3 is the peak with the third lowest $2 theta$ value.) The peak number and intensity information may break over multiple lines, and continues until the intensity and peak number of all N peaks contributing to a particular point have been recorded. . For example, here&#39;s section where there is 1 contributing peak (which happens to be the first peak in the data): . 4.350000 744.3560 232.3900 1 1 4.513631 4.364000 639.3544 230.9700 1 1 5.134259 4.379000 1007.128 234.2900 1 1 5.837606 . Here&#39;s a section with two contributing peaks (which are the second and third peaks in the data): . 8.653000 5611.787 179.4200 2 2 22.49174 3 0.1523584 8.668000 6297.480 185.9700 2 2 26.03695 3 0.1624220 8.682000 5904.059 181.7700 2 2 24.64359 3 0.1726878 . And here&#39;s a section with four contributing peaks (which are the 59th, 60th, 61st and 62nd peaks in the data): . 25.09800 883.4489 79.31000 4 59 0.9365445 60 0.1982842 61 0.1636752 62 2.1087736E-02 25.11300 1260.722 81.62000 4 59 1.462635 60 0.3181552 61 0.2449987 62 2.4525421E-02 25.12700 1757.970 84.58000 4 59 2.065944 60 0.5192419 61 0.3785602 62 2.8390534E-02 . Using the read_DASH_pik function in GALLOP, we can parse a .pik file, and examine the individual peaks and the full diffraction profile. . import numpy as np import matplotlib.pyplot as plt from gallop import files profile, baseline_peaks, n_contributing_peaks = files.dash.read_DASH_pik(&quot;Verap.pik&quot;) . The profile numpy array contains three columns: twotheta, intensity and ESD. . print(&quot;Twotheta Intensity ESD&quot;) print(profile) plt.figure(figsize=(12,8)) plt.plot(profile[:,0], profile[:,1]) plt.xlabel(&quot;$2 theta$&quot;) plt.ylabel(&quot;Intensity&quot;) plt.show() . . Twotheta Intensity ESD [[ 4.35 744.356 232.39 ] [ 4.364 639.3544 230.97 ] [ 4.379 1007.128 234.29 ] ... [ 40.167 104.9547 45.65 ] [ 40.181 -19.50696 44.27 ] [ 40.196 42.59625 44.96 ]] . The baseline_peaks numpy array has shape (n-peaks, n-points) where n-points is the number of points in the profile and where the Nth row contains the intensity associated with contributing peak N. Let&#39;s plot them all on the same axes: . print(&quot;Baseline peaks array shape:&quot;,baseline_peaks.shape) print(baseline_peaks) plt.figure(figsize=(12,8)) for i in range(baseline_peaks.shape[0]): plt.plot(profile[:,0],baseline_peaks[i]) plt.xlabel(&quot;$2 theta$&quot;) plt.ylabel(&quot;Intensity&quot;) plt.show() . . Baseline peaks array shape: (252, 2475) [[4.513631 5.134259 5.837606 ... 0. 0. 0. ] [0. 0. 0. ... 0. 0. 0. ] [0. 0. 0. ... 0. 0. 0. ] ... [0. 0. 0. ... 0.01736796 0.01483574 0.01280165] [0. 0. 0. ... 0.01911421 0.0161166 0.01376026] [0. 0. 0. ... 0.01852708 0.01551832 0.01318077]] . This doesn&#39;t look like the observed data! . However, this is intentional. The intensities in the baseline_peaks array have been scaled to correct for peak multiplicity, Lorentz polarisation factor and form-factor fall off and then divided by their Pawley extracted intensity. . This then allows the calculated profile to be rapidly reconstructed by multiplying each of the rows in the baseline_peaks array by the corresponding intensity (i.e. intensities calculated during SDPD or the Pawley extracted intensities), then summing each column in the array to account for intensity contribution from multiple peaks. The profile $ chi^2$ can then be calculated from the reconstructed profile. . Let&#39;s have a go at reconstructing the profile from the Pawley-refined intensities: . from gallop.structure import Structure mystructure = Structure(name=&quot;verapamil_hydrochloride&quot;) mystructure.add_data(&quot;verap.sdi&quot;, source=&quot;DASH&quot;) # Here we reconstruct the profile by multiplying by the Pawley intensities, then summing each column calc_profile = (mystructure.intensities.reshape(-1,1) * baseline_peaks).sum(axis=0) plt.figure(figsize=(12,8)) plt.plot(profile[:,0], profile[:,1]) plt.plot(profile[:,0], calc_profile) plt.legend([&quot;Obs&quot;, &quot;Calc&quot;]) plt.xlabel(&quot;$2 theta$&quot;) plt.ylabel(&quot;Intensity&quot;) plt.show() . . If we use intensities calculated during SDPD with GALLOP then we will need to scale the calculated pattern in order to properly calculate the $ chi^2$ value. We calculate the scaling factor using the equation below: . $$ c = frac{ sum{y_i^{obs}}}{ sum{y_i^{calc}}} $$ . This will then allow us calculate the profile $ chi^2$ value, via: . $$ chi^2_{profile} = frac{ sum{ frac{(cy^{calc}_i - y^{obs}_i)^2}{( sigma_i^{obs})^2}}}{N - P + C} $$ . where $N$ = the total number of observations ($i$), $P$ = the number of parameters refined and $C$ is the number of constraints used in the refinement. For the $ chi^2$ calculation, we will by default consider only the points which have at least one Bragg peak contributing to the intensity of that point as recommended here (pdf). . # Generate a mask that selects only the points containing contributions from at least one Bragg peak subset = n_contributing_peaks &gt; 0 # Calculate the calculated pattern scaling factor scale = profile[:,1][subset].sum() / calc_profile[subset].sum() # The &quot;-2&quot; in the denominator is because DASH refines two background terms during the Pawley refinement by default prof_chisquared = ((scale*calc_profile[subset] - profile[:,1][subset])**2 / profile[:,2][subset]**2).sum() / (calc_profile[subset].shape[0] - 2) print(&quot;Profile chi-squared =&quot;,prof_chisquared) . . Profile chi-squared = 3.5833747277018166 . Optimising against the profile $ chi^2$ . Now that we know how to calculate the profile $ chi^2$, we should be able to write a function to replace the intensity $ chi^2$ figure of merit used during the local optimisation code in GALLOP. . Following on from the theme of my previous posts, we&#39;ll stick with verapamil hydrochloride as our test structure. All the data required to replicate this work can be downloaded here. . First, let&#39;s create our structure object and add the data and Z-matrices to it. . import torch from gallop.structure import Structure from gallop.optim import local from gallop.optim import Swarm mystructure = Structure(name=&quot;VerapamilHCl&quot;, ignore_H_atoms=True) mystructure.add_data(&quot;files/Verap.sdi&quot;, source=&quot;DASH&quot;) mystructure.add_zmatrix(&quot;files/CURHOM_1.zmatrix&quot;) mystructure.add_zmatrix(&quot;files/CURHOM_2.zmatrix&quot;) . Added Z-matrix with Filename: files/CURHOM_1.zmatrix Non-H atoms: 1 refinable torsions: 0 Degrees of freedom: 3 Added Z-matrix with Filename: files/CURHOM_2.zmatrix Non-H atoms: 33 refinable torsions: 13 Degrees of freedom: 20 (7 + 13) . Let&#39;s now write a function that calculates the profile $ chi^2$. We can then use a pytorch just-in-time compilation decorator to hopefully boost performance a bit. . We&#39;ll be calculating many profiles at the same time, so let&#39;s check the logic involved. We will have two pytorch tensors, which will need multiplication then subsequent summation. The calculated_intensities pytorch tensor has shape (n_particles, n_intensities) and the baseline_peaks array has shape (n_intensities, n_points). For each particle, what we want to do is multiply each row of the baseline_peaks by the corresponding calculated intensity, then sum each of the columns to calculate the profile. This is a straightforward matrix multiplication, which can be expressed in torch using the @ operator. The end result will be a pytorch tensor with shape (n_particles, n_points). . @torch.jit.script def get_prof_chisquared(calculated_intensities, baseline_peaks, scale_numerator, weights, profile): &quot;&quot;&quot; Calculates profile chisquared from a set of calculated intensities, and baseline peaks derived from a DASH Pawley PIK file. Args: calculated_intensities (PyTorch Tensor): Intensities calculated during SDPD with GALLOP Shape (n_particles, n_intensities) baseline_peaks (PyTorch Tensor): Baseline peaks extracted from the DASH PIK file Shape (n_intensities, n_points) scale_numerator (PyTorch Tensor): Sum of the observed intensities for all points Shape (1,) weights (PyTorch Tensor): Tensor of (1/esd)**2 values, Precalculated to save time. Shape (n_points,). profile (PyTorch Tensor): Tensor of the PIK profile, Shape (n_points, 3) Returns: PyTorch Tensor: A tensor with shape (n_particles,) containing the profile chi-square values &quot;&quot;&quot; # Calculate the profiles by matrix multiplication calc_profiles = calculated_intensities @ baseline_peaks # Calculate the scaling factors for the calculated profiles scale_denominator = calc_profiles.sum(dim=1).unsqueeze(1) scale = scale_numerator / scale_denominator # The &quot;-2&quot; in the denominator is because DASH refines two background terms # during the Pawley refinement by default prof_chisquared = ((weights*((scale*calc_profiles - profile[:,1])**2) ).sum(dim=1).reshape(-1,1) / (profile.shape[0]-2)).squeeze(1) return prof_chisquared . We&#39;ll now need to write our function for the GALLOP loop. One thing that&#39;s worth adding is the ability to reduce the number of points in the profile. This may be useful to help boost performance if the profile has been collected with a very small step-size. This is visualised in the plot below, which shows the first peak in the verapamil hydrochloride data, plotted with different step sizes along the profile. . position = profile[:,0][baseline_peaks[0] &gt; 0] first_peak = baseline_peaks[0][baseline_peaks[0] &gt; 0] fig, ax = plt.subplots(4, figsize=(10,10)) colour = [&quot;b&quot;, &quot;r&quot;, &quot;g&quot;, &quot;k&quot;] for i in range(4): ax[i].scatter(position[::2**i],first_peak[::2**i], c=colour[i]) ax[i].plot(position[::2**i],first_peak[::2**i], c=colour[i]) ax[i].legend([f&quot;step = {2**i}&quot;]) ax[i].set_ylabel(&quot;Intensity&quot;) ax[i].set_xlim(position.min(), position.max()) ax[i].set_xlabel(&quot;$2 theta$&quot;) plt.show() . . This plot shows that significantly reducing the number of points in the profile, even at the lowest angle (where peaks are likely to be sharper) still retains the peak shape, so may allow us to cut the number of points we need to evaluate without reducing the accuracy of our results. This approach gives a consistent step size over the whole profile - there are probably more sophisticated ways of sampling from the profile that might work even better, perhaps non-uniform sampling that is guided by increasing peak-width with $2 theta$. . Now we&#39;ll write our GALLOP loop - effectively, this is a slightly modified version of what is on the GALLOP github repository, wrapped in a function to allow easy comparison of the profile and intensity $ chi^2$ functions with the same code. . import tqdm import time from gallop import zm_to_cart from gallop import intensities from gallop import tensor_prep from gallop import chi2 def gallop_solve(mystructure, n_particles, n_swarms, learning_rate, gallop_iters, local_iterations, step=1, use_profile_chisquared=True, seed=None): &quot;&quot;&quot; Solve structures with GALLOP, and easily toggle between profile chisquared and intensity chisquared. Args: mystructure (Structure): GALLOP structure object swarm (Swarm): GALLOP swarm object n_particles (int): number of particles n_swarms (int): number of swarms learning_rate (float): Learning rate gallop_iters (int): Number of GALLOP iterations local_iterations (int): Number of local iterations per GALLOP iteration step (int, optional): Slicing of the profile array to select fractions of the data. Step = 2 would select every other point in the profile. Defaults to 1 (i.e. all points in the profile) use_profile_chisquared (bool, optional): Toggle between profile and intensity chisquared. Defaults to True. seed (int): Defaults to None. Random seed to use for numpy. &quot;&quot;&quot; if seed is not None: np.random.seed(seed) # Let&#39;s first create our swarm object and generate the random starting # positions swarm = Swarm(mystructure, n_particles=n_particles, n_swarms=n_swarms) print(&quot;Generating start positions&quot;) external, internal = swarm.get_initial_positions() # Get the Torch tensors we&#39;ll need for calculating the profile chisquared subset = mystructure.n_contributing_peaks &gt; 0 baseline_peaks = torch.from_numpy( mystructure.baseline_peaks[:,subset][:,::step] ).type(torch.float32).cuda() profile = torch.from_numpy( mystructure.profile[subset][::step] ).type(torch.float32).cuda() scale_numerator = profile[:,1].sum() weights = profile[:,2]**(-2) print(&quot;Running GALLOP&quot;) # The main GALLOP loop start_time = time.time() for i in range(gallop_iters): tensors = tensor_prep.get_all_required_tensors(mystructure, external=external, internal=internal) optimizer = torch.optim.Adam([tensors[&quot;zm&quot;][&quot;external&quot;], tensors[&quot;zm&quot;][&quot;internal&quot;]], lr=learning_rate, betas=[0.9,0.9]) local_iters = tqdm.trange(local_iterations) for j in local_iters: # Zero the gradients before each iteration otherwise they accumulate optimizer.zero_grad() asymmetric_frac_coords = zm_to_cart.get_asymmetric_coords(**tensors[&quot;zm&quot;]) calculated_intensities = intensities.calculate_intensities( asymmetric_frac_coords, **tensors[&quot;int_tensors&quot;]) if use_profile_chisquared: chisquared = get_prof_chisquared(calculated_intensities, baseline_peaks, scale_numerator, weights, profile) else: chisquared = chi2.calc_chisqd(calculated_intensities, **tensors[&quot;chisqd_tensors&quot;]) # pytorch needs a scalar from which to calculate the gradients, here use # the sum of all values - as all of the chi-squared values are independent, # the gradients will be correctly propagated to the relevant DoFs. L = torch.sum(chisquared) # For the last iteration, don&#39;t step the optimiser, otherwise the chi2 # value won&#39;t correspond to the DoFs if j != local_iterations - 1: # Backwards pass through computational graph gives the gradients L.backward() optimizer.step() # Print out some info during the runs if j == 0 or (j+1) % 10 == 0: local_iters.set_description( &quot;GALLOP iter {:04d} LO iter {:04d} min chi2 {:.1f}&quot;.format( i+1, j+1, chisquared.min().item())) # Save the results in a dictionary which is expected by the files and swarm # functions. The tensors should be converted to CPU-based numpy arrays. result = { &quot;external&quot; : tensors[&quot;zm&quot;][&quot;external&quot;].detach().cpu().numpy(), &quot;internal&quot; : tensors[&quot;zm&quot;][&quot;internal&quot;].detach().cpu().numpy(), &quot;chi_2&quot; : chisquared.detach().cpu().numpy(), &quot;GALLOP Iter&quot; : i } # Output a CIF of the best result files.save_CIF_of_best_result(mystructure, result, start_time) # Swarm update step external, internal = swarm.update_position(result=result, verbose=False) return time.time() - start_time . Let&#39;s give this a go! In the first instance, we&#39;ll use the full profile (i.e. step = 1), then have a go at increasing the step size. I&#39;ll be running this on a machine with an Nvidia RTX2060 GPU, and a fairly old i5 CPU. . n_particles = 20000 n_swarms = 20 learning_rate = 3e-2 gallop_iters = 10 local_iterations = 500 step = 1 use_profile_chisquared = True total_time_prof_1 = gallop_solve(mystructure, n_particles, n_swarms, learning_rate, gallop_iters, local_iterations, step=step, use_profile_chisquared=use_profile_chisquared) print(&quot;Total time (profile, step 1):&quot;,np.around(total_time_prof_1 / 60, 2),&quot;min&quot;) . Generating start positions Total degrees of freedom: 23 Total external degrees of freedom: 10 (pos: 6 rot: 4 ) Total internal degrees of freedom: 13 . 100%|██████████| 20/20 [00:00&lt;00:00, 689.69it/s] . Running GALLOP . GALLOP iter 0001 LO iter 0500 min chi2 114.1: 100%|██████████| 500/500 [01:00&lt;00:00, 8.22it/s] GALLOP iter 0002 LO iter 0500 min chi2 44.3: 100%|██████████| 500/500 [00:59&lt;00:00, 8.35it/s] GALLOP iter 0003 LO iter 0500 min chi2 20.1: 100%|██████████| 500/500 [00:59&lt;00:00, 8.41it/s] GALLOP iter 0004 LO iter 0500 min chi2 19.6: 100%|██████████| 500/500 [00:59&lt;00:00, 8.40it/s] GALLOP iter 0005 LO iter 0500 min chi2 19.2: 100%|██████████| 500/500 [00:59&lt;00:00, 8.40it/s] GALLOP iter 0006 LO iter 0500 min chi2 19.3: 100%|██████████| 500/500 [00:59&lt;00:00, 8.40it/s] GALLOP iter 0007 LO iter 0500 min chi2 19.4: 100%|██████████| 500/500 [00:59&lt;00:00, 8.39it/s] GALLOP iter 0008 LO iter 0500 min chi2 19.1: 100%|██████████| 500/500 [00:59&lt;00:00, 8.40it/s] GALLOP iter 0009 LO iter 0500 min chi2 19.1: 100%|██████████| 500/500 [01:01&lt;00:00, 8.19it/s] GALLOP iter 0010 LO iter 0500 min chi2 19.1: 100%|██████████| 500/500 [00:59&lt;00:00, 8.40it/s] . Total time (profile, step 1): 10.04 min . step = 2 total_time_prof_2 = gallop_solve(mystructure, n_particles, n_swarms, learning_rate, gallop_iters, local_iterations, step=step, use_profile_chisquared=use_profile_chisquared) print(&quot;Total time (profile, step 2):&quot;,np.around(total_time_prof_2 / 60, 2),&quot;min&quot;) . Generating start positions . 100%|██████████| 20/20 [00:00&lt;00:00, 800.13it/s] . Running GALLOP . GALLOP iter 0001 LO iter 0500 min chi2 110.5: 100%|██████████| 500/500 [00:54&lt;00:00, 9.09it/s] GALLOP iter 0002 LO iter 0500 min chi2 77.8: 100%|██████████| 500/500 [00:55&lt;00:00, 8.96it/s] GALLOP iter 0003 LO iter 0500 min chi2 71.7: 100%|██████████| 500/500 [00:55&lt;00:00, 8.98it/s] GALLOP iter 0004 LO iter 0500 min chi2 70.9: 100%|██████████| 500/500 [00:55&lt;00:00, 8.99it/s] GALLOP iter 0005 LO iter 0500 min chi2 34.7: 100%|██████████| 500/500 [00:55&lt;00:00, 9.00it/s] GALLOP iter 0006 LO iter 0500 min chi2 19.6: 100%|██████████| 500/500 [00:55&lt;00:00, 8.98it/s] GALLOP iter 0007 LO iter 0500 min chi2 19.3: 100%|██████████| 500/500 [00:55&lt;00:00, 8.99it/s] GALLOP iter 0008 LO iter 0500 min chi2 19.4: 100%|██████████| 500/500 [00:55&lt;00:00, 8.99it/s] GALLOP iter 0009 LO iter 0500 min chi2 19.1: 100%|██████████| 500/500 [00:55&lt;00:00, 8.99it/s] GALLOP iter 0010 LO iter 0500 min chi2 19.2: 100%|██████████| 500/500 [00:55&lt;00:00, 9.00it/s] . Total time (profile, step 2): 9.32 min . step = 4 total_time_prof_4 = gallop_solve(mystructure, n_particles, n_swarms, learning_rate, gallop_iters, local_iterations, step=step, use_profile_chisquared=use_profile_chisquared) print(&quot;Total time (profile, step 4):&quot;,np.around(total_time_prof_4 / 60, 2),&quot;min&quot;) . Generating start positions . 100%|██████████| 20/20 [00:00&lt;00:00, 740.83it/s] . Running GALLOP . GALLOP iter 0001 LO iter 0500 min chi2 109.1: 100%|██████████| 500/500 [00:52&lt;00:00, 9.49it/s] GALLOP iter 0002 LO iter 0500 min chi2 40.2: 100%|██████████| 500/500 [00:52&lt;00:00, 9.48it/s] GALLOP iter 0003 LO iter 0500 min chi2 40.5: 100%|██████████| 500/500 [00:52&lt;00:00, 9.45it/s] GALLOP iter 0004 LO iter 0500 min chi2 39.3: 100%|██████████| 500/500 [00:53&lt;00:00, 9.31it/s] GALLOP iter 0005 LO iter 0500 min chi2 27.5: 100%|██████████| 500/500 [00:52&lt;00:00, 9.61it/s] GALLOP iter 0006 LO iter 0500 min chi2 20.0: 100%|██████████| 500/500 [00:49&lt;00:00, 10.02it/s] GALLOP iter 0007 LO iter 0500 min chi2 19.6: 100%|██████████| 500/500 [00:49&lt;00:00, 10.08it/s] GALLOP iter 0008 LO iter 0500 min chi2 19.4: 100%|██████████| 500/500 [00:49&lt;00:00, 10.04it/s] GALLOP iter 0009 LO iter 0500 min chi2 19.4: 100%|██████████| 500/500 [00:50&lt;00:00, 9.99it/s] GALLOP iter 0010 LO iter 0500 min chi2 19.4: 100%|██████████| 500/500 [00:49&lt;00:00, 10.10it/s] . Total time (profile, step 4): 8.6 min . step = 8 total_time_prof_8 = gallop_solve(mystructure, n_particles, n_swarms, learning_rate, gallop_iters, local_iterations, step=step, use_profile_chisquared=use_profile_chisquared) print(&quot;Total time (profile, step 8):&quot;,np.around(total_time_prof_8 / 60, 2),&quot;min&quot;) . Generating start positions . 100%|██████████| 20/20 [00:00&lt;00:00, 800.10it/s] . Running GALLOP . GALLOP iter 0001 LO iter 0500 min chi2 100.4: 100%|██████████| 500/500 [00:48&lt;00:00, 10.39it/s] GALLOP iter 0002 LO iter 0500 min chi2 40.6: 100%|██████████| 500/500 [00:48&lt;00:00, 10.41it/s] GALLOP iter 0003 LO iter 0500 min chi2 40.8: 100%|██████████| 500/500 [00:49&lt;00:00, 10.12it/s] GALLOP iter 0004 LO iter 0500 min chi2 21.5: 100%|██████████| 500/500 [00:48&lt;00:00, 10.29it/s] GALLOP iter 0005 LO iter 0500 min chi2 21.2: 100%|██████████| 500/500 [00:48&lt;00:00, 10.24it/s] GALLOP iter 0006 LO iter 0500 min chi2 21.2: 100%|██████████| 500/500 [00:51&lt;00:00, 9.79it/s] GALLOP iter 0007 LO iter 0500 min chi2 21.2: 100%|██████████| 500/500 [00:52&lt;00:00, 9.53it/s] GALLOP iter 0008 LO iter 0500 min chi2 21.0: 100%|██████████| 500/500 [00:52&lt;00:00, 9.51it/s] GALLOP iter 0009 LO iter 0500 min chi2 21.1: 100%|██████████| 500/500 [00:54&lt;00:00, 9.11it/s] GALLOP iter 0010 LO iter 0500 min chi2 21.0: 100%|██████████| 500/500 [00:54&lt;00:00, 9.19it/s] . Total time (profile, step 8): 8.54 min . Performance comparison to intensity $ chi^2$ . Let&#39;s now run it again, using the standard intensity $ chi^2$ figure of merit, and plot the results . step = 1 use_profile_chisquared = False total_time_int = gallop_solve(mystructure, n_particles, n_swarms, learning_rate, gallop_iters, local_iterations, step=step, use_profile_chisquared=use_profile_chisquared) print(&quot;Total time (intensity):&quot;,np.around(total_time_int / 60, 2),&quot;min&quot;) . Generating start positions . 100%|██████████| 20/20 [00:00&lt;00:00, 606.07it/s] . Running GALLOP . GALLOP iter 0001 LO iter 0500 min chi2 422.9: 100%|██████████| 500/500 [00:54&lt;00:00, 9.15it/s] GALLOP iter 0002 LO iter 0500 min chi2 247.6: 100%|██████████| 500/500 [00:55&lt;00:00, 9.06it/s] GALLOP iter 0003 LO iter 0500 min chi2 212.9: 100%|██████████| 500/500 [00:55&lt;00:00, 9.04it/s] GALLOP iter 0004 LO iter 0500 min chi2 166.9: 100%|██████████| 500/500 [00:55&lt;00:00, 9.03it/s] GALLOP iter 0005 LO iter 0500 min chi2 83.3: 100%|██████████| 500/500 [00:55&lt;00:00, 9.01it/s] GALLOP iter 0006 LO iter 0500 min chi2 58.0: 100%|██████████| 500/500 [00:55&lt;00:00, 9.02it/s] GALLOP iter 0007 LO iter 0500 min chi2 57.9: 100%|██████████| 500/500 [00:55&lt;00:00, 9.03it/s] GALLOP iter 0008 LO iter 0500 min chi2 56.7: 100%|██████████| 500/500 [00:55&lt;00:00, 9.03it/s] GALLOP iter 0009 LO iter 0500 min chi2 56.7: 100%|██████████| 500/500 [00:55&lt;00:00, 9.02it/s] GALLOP iter 0010 LO iter 0500 min chi2 57.1: 100%|██████████| 500/500 [00:55&lt;00:00, 9.01it/s] . Total time (intensity): 9.27 min . plt.plot([1,2,4,8],[total_time_prof_1, total_time_prof_2, total_time_prof_4, total_time_prof_8]) plt.plot([1,2,4,8],[total_time_int,total_time_int,total_time_int,total_time_int]) plt.xlabel(&quot;Profile step&quot;) plt.ylabel(&quot;Time (seconds)&quot;) plt.legend([&quot;Profile $ chi^2$&quot;,&quot;Intensity $ chi^2$&quot;]) plt.show() . . We see that from the perspective of wall-clock time, using the implementation of profile $ chi^2$ in this notebook with the full profile incurs an approximately 10 % performance penalty relative to the intensity $ chi^2$ which GALLOP uses by default. However, as we decrease the number of points in the profile, we see the time decrease to the extent that it ends up being about 10 % faster than the intensity $ chi^2$ figure of merit! . However, given that the y-axis as plotted starts around 510 seconds, the time saving overall is not huge. This is because the biggest performance bottleneck in the GALLOP code is the internal to Cartesian coordinate conversion. So even if the figure of merit calculation follows the exponential decay function seen in the graph, then at most, we can expect around a 10 % reduction in time. . In terms of the quality of the results obtained, I checked the 15 molecule RMSD using Mercury - results have been plotted below. There&#39;s not much difference between them - only at the hundredths of ångströms level. . profile_fifteen_mol_rmsd = [0.105, 0.108, 0.108, 0.113] intensity_fifteen_mol_rmsd = [0.112] all_data = profile_fifteen_mol_rmsd + intensity_fifteen_mol_rmsd labels = [&quot;prof_1&quot;, &quot;prof_2&quot;, &quot;prof_4&quot;, &quot;prof_8&quot;, &quot;int&quot;] plt.bar(labels, all_data) plt.ylabel(&quot;15 molecule RMSD / Å&quot;) plt.show() . . One final consideration is the memory use - in my testing, I did not see an appreciable difference between the profile $ chi^2$ with different step sizes and the intensity $ chi^2$. I suspect this is because much like with the speed of the code, the internal -&gt; Cartesian conversion process is the biggest contributor to memory use. I will look into this more in the future though! . Conclusions . We&#39;ve seen in this post that it&#39;s relatively easy to understand the PIK file produced by DASH after its Pawley fitting procedure, and use it to rapidly calculate full diffraction profiles. Using these reconstructed profiles, we can then calculate the profile $ chi^2$ and use this figure of merit in the optimisation of crystal structures. As it turns out, the performance of this is pretty good! The 10 % reduction in speed relative to the intensity $ chi^2$ we saw in the example we looked at here is not huge, but may still merit an update to GALLOP to include it as an option. . Extending this work . One obvious thing to do would be to see if this can also be done easily with GSAS-II output files, or indeed other programs that are capable of Le Bail fits. . Another interesting possibility would be to have a go using the integrated profile strategy employed by FOX (pdf) which should afford improved performance. .",
            "url": "https://mspillman.github.io/blog/pxrd/gallop/profile/2022/01/08/Profile-Chi-Squared-Optimisation.html",
            "relUrl": "/pxrd/gallop/profile/2022/01/08/Profile-Chi-Squared-Optimisation.html",
            "date": " • Jan 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Installing and running GALLOP on cloud GPUs",
            "content": "Introduction . GALLOP is designed to run on GPUs and TPUs. Users that don&#39;t have local access to GPUs, or who want to gain access to faster GPUs without needing to purchase their own can make use of cloud computing resources. In this post we&#39;ll look at how to set up a GPU-equipped virtual machine on Google&#39;s Compute Engine cloud service, and then use it to run GALLOP jobs both through the graphical browser-based interface and via python mode. . One feature offered by GCE is preemptible instances, which significantly lowers the cost of using resources. There are some limitations that come with this, but they are usually not a concern for GALLOP jobs that take &lt; 24 hours to complete. It&#39;s also worth noting that new customers to Google Cloud get $300 free credits to try out their services, so it&#39;s well worth having a go. You will need a credit/debit card to gain access to GPUs even though you won&#39;t be charged until the free credits run out. You will also need to request an increase to your GPU quota before they can be used - this only took a few minutes to be approved for me. . Lastly, I&#39;m not sponsored or endorsed by Google at all, I just like this service. There are many other cloud GPU providers who may be worth a look to see if they suit your requirements. If you know of any good ones, please get in contact with me, especially if you want to help with providing instructions for other users. . Starting a VM . Log onto your Google Cloud Dashboard. In the top-left hand corner, you&#39;ll see the now ubiquitous (at least on Google products) three lines menu button. Click this, and a menu will appear. We are interested in the Compute Engine, so hover over that and an additional menu expansion will appear. Click on VM instances. . . Once you&#39;ve done that, we will create our GPU-backed VM! Click on the blue Create Instance button: . . You&#39;ll be taken to a menu where we will configure our machine. I am going to select the us-west1 region, though you can choose whichever region you like (provided that you have a GPU-quota in that region). . We then need to make sure we choose a GPU-backed machine, so click on the blue GPU option under Machine family, then select which type of GPU and the number of GPUs you require. For this tutorial, we&#39;ll go for a state of the art NVIDIA Tesla A100 GPU. Once you select this, you&#39;ll notice a price estimate in the top right hand corner, which is quite expensive! Don&#39;t worry, this will come down once we switch to a preemptible instance type. . . There is no need to change the machine type or CPU platform, so our next task is to change the Boot disk. The default is a Debian GNU/Linux environment, we&#39;ll change this so that pytorch (one of the libraries that GALLOP depends on) is pre-installed. . To do this, click the &quot;change&quot; button under Boot disk: . . Once you do that, a menu will appear. Under Operating system, select Deep Learning on Linux, then in the Version menu, select Deep Learning Image: PyTorch 1.9 m82 CUDA 110. You should end up with something like this: . . Once happy, press Select. . The last thing we&#39;ll do is set our instance to be preemptible. This will bring the hourly cost down dramatically, though it comes with some restrictions relative to a non-preemptible instance. See here for more details. . Near the bottom of the page, a blue expandable menu titled &quot;NETWORKING, DISKS, SECURITY, MANAGEMENT, SOLE-TENANCY&quot; is available. Click this to expand. Then click the Management expandable section. Now look down this list for the &quot;Availability policy&quot; menu, which has a drop-down list titled Preemptibility. Click this, and select &quot;On&quot;. The price estimate in the top-right corner should reduce significantly! . . Once you have completed all of the steps above, we are ready to create the instance. Press the blue Create button at the bottom of the page. You will be taken to a new screen which displays the status of the instance, and will allow us to connect to it via a browser-based SSH window. It takes a few minutes for the instances to become accessible, but once it&#39;s ready, you should see something like this: . . To connect to our instance, on the right hand side you&#39;ll see an arrow next to &quot;SSH&quot;. Click this and then select &quot;Open in browser window&quot;. A new window will appear, which will connect to your instance. It will ask you if you want to initiate a connection - press the blue &quot;Connect&quot; button. After a short wait, you will be greeted with the following screen: . . Installing the CUDA driver and GALLOP . Installing the CUDA driver . Once SSH&#39;d into the VM for the first time, I typed y, then pressed enter to install the CUDA driver. This unfortunately failed (I&#39;m not sure why, but this happens frequently! Thankfully, we can fix it quite easily). If yours doesn&#39;t fail, proceed onto the GALLOP installation described below, otherwise follow these steps to get the CUDA driver working. . Run the following commands one by one: . sudo rm /var/lib/apt/lists/lock &amp;&amp; sudo rm /var/cache/apt/archives/lock &amp;&amp; sudo rm /var/lib/dpkg/lock* sudo dpkg --configure -a sudo /opt/deeplearning/install-driver.sh . The last command in particular may take a couple of minutes to run. If any errors occur during these steps, then depending on the error, you may need to run a few more commands to resolve them. . One possible issue is: . dpkg: error: parsing file &#39;/var/lib/dpkg/updates/0003&#39; near line 0: newline in field name &#39;#padding&#39; . I solved this issue by running sudo rm /var/lib/dpkg/0003, though you may need to replace 0003 with whatever number you have on your error. . Another potential issue is the second command complains about the google cloud sdk: . Errors were encountered while processing: google-cloud-sdk . If this occurs, run the following commands and hopefully it&#39;ll work. The first command can take a few minutes to run. . sudo apt-get upgrade google-cloud-sdk -y sudo dpkg --configure -a sudo /opt/deeplearning/install-driver.sh . If you get any other errors, I found several examples of other people experiencing the same thing on Google so I suggest searching for your error message. Feel free to contact me by email or on Twitter and I can try to help. . Installing GALLOP and running in Python mode . Once the CUDA driver is installed, we&#39;re ready to install GALLOP! Run the following command to grab the code from github and install it to the VM. . git clone https://github.com/mspillman/gallop.git &amp;&amp; cd gallop &amp;&amp; pip install . . Once these commands are finished, GALLOP is now installed. If you&#39;re running in python mode, then you can upload a script (and diffraction data &amp; ZMs) using the &quot;gear&quot; menu in the top right. Just below you&#39;ll also see an option for downloading files which will be of use to obtain your results. . . You may also be interested in using SSH to access JupyterLab . https://cloud.google.com/vertex-ai/docs/workbench/user-managed/ssh-access . Running GALLOP via the Browser interface . To use the browser interface, we&#39;ll need to make use of ngrok which is available for free. Run the following command to download and unzip the ngrok executable: . wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-386.zip &amp;&amp; unzip ngrok*.zip . We then need to setup ngrok with our authoken. . Run the following command, but ensure you paste in your authtoken in the space I&#39;ve indicated. This will be pre-populated in &quot;step 2&quot; on your ngrok homepage. . ./ngrok authtoken [authtoken goes here] . Once this is done, we will first start the browser interface running in a background process: . streamlit run gallop_streamlit.py &amp;&gt;/dev/null &amp; . . The process ID displayed will allow you to kill streamlit if needed with the command kill 9943 command, where you replace 9943 with the process ID that is displayed. . Lastly, to get our ngrok link and access the GALLOP browser interface, run: . ./ngrok http 8501 . And the following will appear: . . Click on the Forwarding URL and you will be taken to the GALLOP browser interface. . Note - when I tried it, Google Chrome complained that it suspected it was some sort of phishing attempt but Firefox opened the URL without complaint. Not sure what happened there, but if you get a concerning red screen, I suggest switching to Firefox! . Once finished . Once you&#39;re finished, you may wish to stop, suspend or delete your instance in order to avoid incurring any additional fees. Press the small three-dots menu on the right hand side of your instances page, and various options will appear. . . Conclusions . In this post, we&#39;ve seen how to get a preemptible GPU-equipped VM running on Google Compute Engine, and install the CUDA driver and GALLOP on it. .",
            "url": "https://mspillman.github.io/blog/gallop/cloud/gce/2021/11/18/Google-Cloud.html",
            "relUrl": "/gallop/cloud/gce/2021/11/18/Google-Cloud.html",
            "date": " • Nov 18, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Estimating molecular volumes to aid in powder X-ray diffraction indexing",
            "content": "Introduction . An article published in 2001 by D. W. M. Hofmann describes how crystallographic databases can be used to derive the average volume occupied by atoms of each element in crystal structures. Using his tabulated values, it&#39;s possible to rapidly estimate the volume occupied by a given molecule, and use this to aid indexing of powder diffraction data. This is particularly useful for laboratory diffraction data, which is generally associated with lower figures of merit such as de Wolff&#39;s $M_{20}$ and Smith and Snyder&#39;s $F_N$, which can make discriminating between alternative options more challenging. Other volume estimation methods, notably the 18 Å³ rule are also commonly used, though Hofmann&#39;s volumes give generally more accurate results. . I&#39;ve put together a freely available web-app, HofCalc, which can be used to conveniently obtain these estimates. It should display reasonably well on mobile devices as well as PCs/laptops. You can access it at the following address: . https://hofcalc.herokuapp.com . . This post will explain how it works, and will look at some examples of how it can be used in practice. I&#39;m grateful to Norman Shankland who provided invaluable feedback and assistance with debugging of the app. . Hofmann volumes . After applying various filters to crystal structures deposited in the CSD, Hofmann ended up with a dataset comprised of 182239 structures. Hofmann only considers the elements up to atomic number 100 (fermium) in his work, and assumes that the volume of the unit cell is equivalent to: . $$V_{est} = sum limits_{i=1}^{100} n_i bar{v_i}(1+ bar{ alpha}T) = bold{n bar{v}}(1+ bar{ alpha}T)$$ . Where $n_i$ is the number of atoms of element $i$ in the unit cell, and $ bar{v_i}$ is the average volume occupied by an atom of element $i$. He also assumes that atomic volumes vary linearly with temperature. . He split the dataset into 20 subsets, then used an iterative least-squares method to solve the above equation for each of the subsets. This allowed him to find the average volumes occupied by atoms of each element, and due to the splitting of the data into subsets, he also obtains their standard deviations. The coefficient of thermal expansion, $ bar{ alpha}$, was found to be $0.95 times 10^{-4} K^{-1}$. This temperature correction factor then allowed him to provide the average volumes for all of the elements represented in the CSD at 298 K. . You can download a .json file containing the 298 K volumes here. . Comparison with other atomic volumes . Let&#39;s compare Hofmann&#39;s volumes to those obtained from other sources. Hofmann&#39;s article compares his volumes to those derived by Mighell and coworkers, which were published in 1987. As additional comparison points, which highlight the importance of using crystallographic volumes in this context, I also downloaded some atomic radii data from Wikipedia and converted these into atomic volumes (assuming spherical atoms). Sources for these radii may be found at the bottom of the Wikipedia article. . Click on the coloured boxes in the top left to view individual types of volume, and shift-click to add other volumes back in again. . import json import pandas as pd import numpy as np import altair as alt with open(&quot;files/Hofmann-volumes.json&quot;) as hv: hofmann_volumes = json.load(hv) hv.close() vols = [] for i, key in enumerate(hofmann_volumes.keys()): vols.append([i+1, key, hofmann_volumes[key]]) df = pd.DataFrame(vols) df.columns = [&quot;Atomic number&quot;, &quot;Element&quot;, &quot;Hofmann&quot;] df.reset_index(drop=True, inplace=True) df.replace(&quot;N/A&quot;, np.NaN, inplace=True) wikiradii = pd.read_excel(&quot;files/wikipedia_radii.xlsx&quot;) wikiradii.replace(&quot;&quot;, np.NaN, inplace=True) radtype = [&quot;Mighell&quot;, &quot;Empirical&quot;,&quot;Calculated&quot;,&quot;vdW&quot;,&quot;Covalent-single&quot;,&quot;Covalent-triple&quot;, &quot;Metallic&quot;] for r in radtype: # Radii are in pm so /100 to convert to angstroms. if r == &quot;Mighell&quot;: df[r] = wikiradii[r].values.astype(float) else: df[r] = (4*np.pi/3)*(wikiradii[r].values.astype(float)/100)**3 # Convert our dataframe to long-form as this is what is expected by altair dflong = df.melt(&quot;Atomic number&quot;, var_name=&quot;Volume&quot;, value_vars=[&quot;Hofmann&quot;] + radtype) # Restore the element symbols to the long dataframe element = [] for an in dflong[&quot;Atomic number&quot;]: element.append(df[&quot;Element&quot;][df[&quot;Atomic number&quot;] == an].item()) dflong[&quot;Element&quot;] = element click = alt.selection_multi(encodings=[&quot;color&quot;]) # scatter plot, modify opacity based on selection scatter = alt.Chart(dflong).mark_point().encode( x=alt.X(&#39;Element:N&#39;,sort=dflong[&quot;Atomic number&quot;].values), y=alt.Y(&quot;value:Q&quot;, axis=alt.Axis(title=&#39;Volume / Å³&#39;)), tooltip=[&#39;Element&#39;, &#39;Volume:N&#39;, &#39;value&#39;], opacity=alt.value(0.85), color=&quot;Volume:N&quot; ).transform_filter(click).properties(width=650, height=500).interactive() # legend legend = alt.Chart(dflong).mark_rect().encode( y=alt.Y(&#39;Volume:N&#39;, axis=alt.Axis(title=&#39;Select volume&#39;), sort=[4,6,0,1,2,3,5,7]), color=alt.condition(click, &#39;Volume:N&#39;, alt.value(&#39;lightgray&#39;), legend=None), ).properties( selection=click, ) chart = (legend | scatter) chart . . As you can see, the volumes of Hofmann and Mighell differ significantly from those I derived from the atomic radii. . Let&#39;s print out some of the statistics describing the data, as well compare the coefficient of variation for each type of volume. . df.describe()[[&quot;Hofmann&quot;]+radtype] . . Hofmann Mighell Empirical Calculated vdW Covalent-single Covalent-triple Metallic . count 84.000000 | 100.00000 | 91.000000 | 86.000000 | 55.000000 | 95.000000 | 71.000000 | 68.000000 | . mean 39.586310 | 42.65840 | 17.084519 | 23.849111 | 37.865054 | 13.250696 | 6.963913 | 19.946436 | . std 13.460829 | 17.12722 | 12.248898 | 20.652779 | 33.870162 | 8.900926 | 3.301590 | 12.541749 | . min 5.080000 | 5.40000 | 0.065450 | 0.124788 | 7.238229 | 0.137258 | 0.623615 | 5.884949 | . 25% 31.000000 | 30.87500 | 10.305995 | 7.377367 | 19.870146 | 7.329463 | 4.988916 | 11.039115 | . 50% 39.300000 | 41.00000 | 12.770051 | 19.334951 | 27.833137 | 11.008442 | 6.538266 | 17.157285 | . 75% 49.250000 | 53.50000 | 23.632685 | 36.484963 | 39.070796 | 19.160766 | 9.204406 | 24.469830 | . max 74.000000 | 85.00000 | 77.951815 | 110.850435 | 176.533179 | 52.306127 | 16.837592 | 77.951815 | . import matplotlib.pyplot as plt plt.figure(figsize=(8,5)) ((df.describe().loc[&quot;std&quot;] / df.describe().loc[&quot;mean&quot;])[[&quot;Hofmann&quot;]+radtype]).plot.bar() plt.ylabel(&quot;Coefficient of variation&quot;) plt.show() . . We see a much lower coefficient of variation for Hofmann&#39;s volumes than the others. . HofCalc - using the web app . HofCalc makes use of two key python libraries to process chemical formulae (pyvalem) and resolve chemical names (PubChemPy) prior to processing. This allows the app to have a really convenient interface for specifying queries (see below), which enables users to easily mix and match between formulae and names to obtain the information they need. . Formulae and names . Basic use . The simplest option is to enter the chemical formula or name of the material of interest. Names are resolved by querying PubChem, so common abbreviations for solvents can often be used e.g. DMF. Note that formulae can be prefixed with a multiple, e.g. 2H2O . Search term Type $V_{Hofmann}$ . ethanol | name | 69.61 | . CH3CH2OH | formula | 69.61 | . water | name | 21.55 | . 2H2O | formula | 43.10 | . Multiple search terms . It is also possible to search for multiple items simultaneously, and mix and match name and formulae by separating individual components with a semicolon. This means that for example, &#39;amodiaquine dihydrochloride dihydrate&#39; can also be entered as &#39;amodiaquine; 2HCl; 2H2O&#39;. . Search term Total $V_{Hofmann}$ . carbamazepine; L-glutamic acid | 497.98 | . zopiclone; 2H2O | 496.02 | . C15H12N2O; CH3CH2COO-; Na+ | 419.79 | . sodium salicylate; water | 204.21 | . amodiaquine dihydrochloride dihydrate | 566.61 | . amodiaquine; 2HCl; 2H2O | 566.61 | . More complex examples - hemihydrates . In cases where fractional multiples of search components are required, such as with hemihydrates, care should be taken to check the evaluated chemical formula for consistency with the expected formula. . Search term Evaluated as $V_{Hofmann}$ Divide by Expected Volume . Calcium sulfate hemihydrate | Ca2 H2 O9 S2 | 253.07 | 2 | 126.53 | . calcium; calcium; sulfate; sulfate; water | Ca2 H2 O9 S2 | 253.07 | 2 | 126.53 | . calcium; sulfate; 0.5H2O | Ca1 H1.0 O4.5 S1 | 126.53 | - | 126.53 | . Codeine phosphate hemihydrate | C36 H50 N2 O15 P2 | 1006.77 | 2 | 503.38 | . codeine; codeine; phosphoric acid; phosphoric acid; water | C36 H50 N2 O15 P2 | 1006.77 | 2 | 503.38 | . codeine; phosphoric acid; 0.5H2O | C18 H25.0 N1 O7.5 P1 | 503.38 | - | 503.38 | . Charged species in formulae . Charges could potentially interfere with the parsing of chemical formulae. For example, two ways of representing an oxide ion: . Search term Evaluated as . O-2 | 1 x O | . O2- | 2 x O | . Whilst is is recommended that charges be omitted from HofCalc queries, if including charges in your queries, ensure that the correct number of atoms has been determined in the displayed atom counts or the downloadable summary file. For more information on formatting formulae, see the pyvalem documentation. . Temperature . The temperature, $T$ (in kelvin) is automatically included in the volume calculation via the following equation: . $$V = sum{n_{i}v_{i}}(1 + alpha(T - 298))$$ . Where $n_{i}$ and $v_{i}$ are the number and Hofmann volume (at 298 K) of the $i$th element in the chemical formula, and $ alpha = 0.95 times 10^{-4} K^{-1}$. . Unit cell volume . If the volume of a unit cell is supplied, then the unit cell volume divided by the estimated molecular volume will also be shown. . Search term $V_{cell}$ $V_{Hofmann}$ $ frac{V_{cell}}{V_{Hofmann}}$ . zopiclone, 2H2O | 1874.61 | 496.02 | 3.78 | . verapamil, HCl | 1382.06 | 667.57 | 2.07 | . Summary Files . Each time HofCalc is used, a downloadable summary file is produced. It is designed to serve both as a record of the query for future reference and also as a method to sense-check the interpretation of the entered terms, with links to the PubChem entries where relevant. An example of the contents of the summary file for the following search terms is given below. . Search term = carbamazepine; indomethacin . T = 293 K . Unit cell volume = 2921.6 Å³ . { &quot;combined&quot;: { &quot;C&quot;: 34, &quot;H&quot;: 28, &quot;N&quot;: 3, &quot;O&quot;: 5, &quot;Cl&quot;: 1 }, &quot;individual&quot;: { &quot;carbamazepine&quot;: { &quot;C&quot;: 15, &quot;H&quot;: 12, &quot;N&quot;: 2, &quot;O&quot;: 1 }, &quot;indomethacin&quot;: { &quot;C&quot;: 19, &quot;H&quot;: 16, &quot;Cl&quot;: 1, &quot;N&quot;: 1, &quot;O&quot;: 4 } }, &quot;user_input&quot;: [ &quot;carbamazepine&quot;, &quot;indomethacin&quot; ], &quot;PubChem CIDs&quot;: { &quot;carbamazepine&quot;: 2554, &quot;indomethacin&quot;: 3715 }, &quot;PubChem URLs&quot;: { &quot;carbamazepine&quot;: &quot;https://pubchem.ncbi.nlm.nih.gov/compound/2554&quot;, &quot;indomethacin&quot;: &quot;https://pubchem.ncbi.nlm.nih.gov/compound/3715&quot; }, &quot;individual_volumes&quot;: { &quot;carbamazepine&quot;: 303.86, &quot;indomethacin&quot;: 427.77 }, &quot;V_Cell / V_Hofmann&quot;: 3.99, &quot;Temperature&quot;: 293, &quot;Hofmann Volume&quot;: 731.62, &quot;Hofmann Density&quot;: 1.35 } . Case study: CT-DMF2 . The crystal structure of chlorothiazide N,N-dimethylformamide, a.k.a CT-DMF2, was solved from laboratory powder diffraction data back in 2007. I decided to try re-indexing the diffraction data to see if HofCalc would be of use. . Using the DASH interface to DICVOL, the following unit cells are suggested: . . Both monoclinic and triclinic cells are obtained with very different unit cell volumes. Whilst the figures of merit certainly push towards accepting the conclusion of a monoclinic unit cell, it&#39;s worth checking to see if this makes sense given the expected composition of the material. In addition, there may be more than one dimethylformamide molecule crystallising with the chlorothiazide - HofCalc may be able to shed some light there too. . The paper states that the solvate was formed by recrystallisation of chlorothiazide from DMF solvent, so it seems logical to try the following permutations: . chlorothiazide alone | chlorothiazide + 1 DMF | chlorothiazide + 2 DMF (etc) | HofCalc query $V_{Hofmann}$ $V_{cell}$ $ frac{V_{cell}}{V_{Hofmann}}$ . chlorothiazide | 284.73 | 2422 (triclinic) | 8.51 | . chlorothiazide | 284.73 | 3950 (monoclinic) | 13.87 | . chlorothiazide; DMF | 385.09 | 2422 (triclinic) | 6.29 | . chlorothiazide; DMF | 385.09 | 3950 (monoclinic) | 10.26 | . chlorothiazide; DMF; DMF | 485.45 | 2422 (triclinic) | 4.99 | . chlorothiazide; DMF; DMF | 485.45 | 3950 (monoclinic) | 8.14 | . chlorothiazide; DMF; DMF; DMF | 585.81 | 2422 (triclinic) | 4.13 | . chlorothiazide; DMF; DMF; DMF | 585.81 | 3950 (monoclinic) | 6.74 | . If we exclude those results with $ frac{V_{cell}}{V_{mol}}$ ratios &gt; 0.25 away from a (crystallographically sensible) whole number, we can see from the table that the most favourable compositions are CT + 2xDMF (monoclinic) and CT + 3xDMF (triclinic). Given the higher figure of merit for the monoclinic unit cell, it seems reasonable to take this forward and attempt space-group determination. Doing this in DASH identifies the most probable space group as $P2_1/c$, which then implies $Z&#39;=2$. This is indeed the correct result. . If we compare this to the commonly used 18 Å³ rule, we end up with the following results: . Possible composition $V_{18Å^{3}}$ $V_{cell}$ $ frac{V_{cell}}{V_{18Å^{3}}}$ . chlorothiazide | 306 | 2422 (triclinic) | 7.92 | . chlorothiazide | 306 | 3950 (monoclinic) | 12.91 | . chlorothiazide; DMF | 396 | 2422 (triclinic) | 6.12 | . chlorothiazide; DMF | 396 | 3950 (monoclinic) | 9.97 | . chlorothiazide; DMF; DMF | 486 | 2422 (triclinic) | 4.98 | . chlorothiazide; DMF; DMF | 486 | 3950 (monoclinic) | 8.13 | . chlorothiazide; DMF; DMF; DMF | 576 | 2422 (triclinic) | 4.20 | . chlorothiazide; DMF; DMF; DMF | 576 | 3950 (monoclinic) | 6.86 | . Again, CT + 2xDMF is in the candidates to check, however, using the 18 Å³ rule, a triclinic pure chlorothiazide unit cell also becomes a viable possibility. Had there been a less clear distinction in the indexing figure-of-merit, this may have resulted in time being wasted on testing this additional possibility. . Conclusions . Hofmann&#39;s volumes give more accurate estimates of molecular volumes in crystals, and should be used in preference to the 18 Å³ rule where possible. . To make this easier for people, the HofCalc web-app can be used to very rapidly and conveniently obtain these estimates. .",
            "url": "https://mspillman.github.io/blog/pxrd/indexing/2021/11/10/Hofcalc.html",
            "relUrl": "/pxrd/indexing/2021/11/10/Hofcalc.html",
            "date": " • Nov 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Visualising hypersurfaces and optimisation trajectories",
            "content": "Introduction . In this post, we&#39;ll be using the GALLOP Python API to to enable us to visualise $ chi^2$ hypersurfaces. These hypersurfaces are functions of the molecular positions, orientations and conformations within the unit cell. As such, their dimensionality is typically high, meaning that we cant visualise them directly. However, we can visualise low-dimensional slices through them. We&#39;ve seen an example of one of these slices an earlier post. . We&#39;ll write a simple function that allows us to specify two or three vector directions, then sample $ chi^2$ at points on a grid between these directions. We&#39;ll also have a go at visualising the trajectories taken by particles initialised at each point on the grid as they are optimised with Adam, the default local optimiser in GALLOP. . For ease, we&#39;ll continue using verapamil hydrochloride as our structure of interest. You can download the DASH fit files and Z-matrices I&#39;ll be using here. . Choice of central point and slice directions . Central point . Usually when we are trying to solve the crystal structure of an unknown small molecule, we are most concerned with finding the global minimum of the $ chi^2$ hypersurface. Therefore, we&#39;ll start off investigating the region around the global minimum. Due to the repeating nature of a crystallographic lattice, and symmetries within unit cells, there are infinitely many points that correspond to the global minimum of the hypersurface - therefore, the coordinates below may not be the same as coordinates you obtain in your own solutions of verapamil hydrochloride. . When GALLOP is used to solve a crystal structure, the degrees of freedom corresponding to the solution are automatically added to the CIF as comments. For example, here is the header of a CIF for verapamil hydrochloride: . # Generated using pymatgen and GALLOP # GALLOP External Coords = [0.68394345,0.45950916,-0.114383094,0.2456631,-0.29722878,-0.10422839,0.3358067,0.7402369,0.7596887,-0.16798618 # GALLOP Internal Coords = -1.2276717,2.9165819,-0.96692395,-1.2034712,3.2261908,2.1036072,-3.2097023,-3.0328763,1.5661795,-0.0071008434,-0.1824131,-0.05715108,0.27950087 # Profile chisqd = 16.741 data_Verap_0006_56.244_chisqd_252_refs_3.6_mins.cif . So as long as we&#39;ve been able to solve the crystal structure of interest with GALLOP, we&#39;ll have easy access to the coordinates of a global minimum point. . We can also plot around a random point on the hypersurface to give a comparison. . Slice directions . In this post, we&#39;ll look at two options for plotting. The first and most obvious will be to plot along some of the degrees of freedom in the structure to see how they interact. . The other option will be to choose random unit vectors and slice the surface along them to get a feel for how all of the degrees of freedom interact. In high dimensional spaces, randomly generated pairs of vectors will be approximately orthogonal so it should be possible to get reasonable levels of independence between the axes in our plots. . Data loading and functions . We need to import some libraries, then load our data and Z-matrices. . import numpy as np from gallop.structure import Structure from gallop import tensor_prep from gallop import zm_to_cart from gallop import intensities from gallop import chi2 mystructure = Structure(name=&quot;Verapamil&quot;, ignore_H_atoms=True) mystructure.add_data(&quot;files/Verap.sdi&quot;, source=&quot;DASH&quot;) mystructure.add_zmatrix(&quot;files/CURHOM_1.zmatrix&quot;, verbose=False) mystructure.add_zmatrix(&quot;files/CURHOM_2.zmatrix&quot;, verbose=False) mystructure.get_total_degrees_of_freedom(verbose=False) # verapamil hydrochloride coordinates of global minimum from CIF. g_external = np.array([0.68394345,0.45950916,-0.114383094,0.2456631,-0.29722878, -0.10422839,0.3358067,0.7402369,0.7596887,-0.16798618]) g_internal = np.array([-1.2276717,2.9165819,-0.96692395,-1.2034712,3.2261908, 2.1036072,-3.2097023,-3.0328763,1.5661795,-0.0071008434, -0.1824131,-0.05715108,0.27950087]) global_min = np.hstack([g_external,g_internal]) . . We also need a function to produce our vectors, grid and points at which to sample $ chi^2$. . def get_vectors_and_angle(n_vectors, structure): # Generate random unit vectors and calculate the angles between them vectors = np.random.uniform(-1,1,(n_vectors, structure.total_degrees_of_freedom)) vectors /= np.linalg.norm(vectors) if n_vectors == 2: angle = np.array(np.rad2deg(np.arccos(np.dot(vectors[0], vectors[1])))) else: angle = np.array([ np.rad2deg(np.arccos(np.dot(vectors[0], vectors[1]))), np.rad2deg(np.arccos(np.dot(vectors[0], vectors[2]))), np.rad2deg(np.arccos(np.dot(vectors[1], vectors[2]))) ]) return vectors, angle def get_points(structure, global_minimum, dims=&quot;random&quot;, n_vectors=2, n_points=50): if dims != &quot;random&quot;: assert len(dims) in [2,3], &quot;Number of dims must be 2 or 3&quot; if dims == &quot;random&quot;: # Randomly generate a set of n_vectors, and continue randomly generating # them until the angles between them are all in the range 88-92 degrees. vectors, angle = get_vectors_and_angle(n_vectors, structure) while np.abs(90 - angle).max() &gt; 2: vectors, angle = get_vectors_and_angle(n_vectors, structure) gridpoints = [np.linspace(-1,1,n_points) for i in range(n_vectors)] print(&quot;Angle:&quot;,angle) else: vectors = np.zeros((len(dims), structure.total_degrees_of_freedom), dtype=np.float32) gridpoints = [] for i, d in enumerate(dims): global_minimum[d] *= 0 vectors[i,d] = 1 # If we are looking at the unit cell dimensions, then we want to # ensure that the range of data we plot captures the range the parameter # can take. if d &lt; structure.total_position_degrees_of_freedom: gridpoints.append(np.linspace(0,1,n_points)) elif d &lt; structure.total_external_degrees_of_freedom: gridpoints.append(np.linspace(-1,1,n_points)) else: gridpoints.append(np.linspace(-np.pi,np.pi,n_points)) if len(gridpoints) == 2: xx, yy = np.meshgrid(gridpoints[0], gridpoints[1]) points = global_minimum + xx.ravel().reshape(-1,1)*vectors[0] + yy.ravel().reshape(-1,1)*vectors[1] grid = [xx, yy] else: xx, yy, zz = np.meshgrid(gridpoints[0], gridpoints[1], gridpoints[2]) points = global_minimum + xx.ravel().reshape(-1,1)*vectors[0] + yy.ravel().reshape(-1,1)*vectors[1] + zz.ravel().reshape(-1,1)*vectors[2] grid = [xx, yy, zz] external = points[:,:structure.total_external_degrees_of_freedom] internal = points[:,structure.total_external_degrees_of_freedom:] return external, internal, grid, gridpoints . . Next we&#39;ll need a function to calculate $ chi^2$ at each point on our grid . def get_chi_squared(mystructure, external, internal): tensors = tensor_prep.get_all_required_tensors(mystructure, external=external, internal=internal, requires_grad=False) asymmetric_frac_coords = zm_to_cart.get_asymmetric_coords(**tensors[&quot;zm&quot;]) calculated_intensities = intensities.calculate_intensities( asymmetric_frac_coords, **tensors[&quot;int_tensors&quot;]) chisquared = chi2.calc_chisqd(calculated_intensities, **tensors[&quot;chisqd_tensors&quot;]) chisquared = chisquared.detach().cpu().numpy() return chisquared . . And lastly, a function to produce our plots . from IPython.display import HTML import plotly.graph_objects as go def get_plot(chisquared, grid, gridpoints, dim1=&quot;x&quot;, dim2=&quot;y&quot;, percentile=50): if len(grid) == 2: # create figure fig = go.Figure() # Add surface trace fig.add_trace( go.Contour(x=gridpoints[0], y=gridpoints[1], z=chisquared.reshape(grid[0].shape), colorscale=&quot;Inferno&quot;) ) # Update plot sizing fig.update_layout( width=700, height=600, autosize=False, margin=dict(t=0, b=0, l=0, r=0), template=&quot;plotly_white&quot;, ) # Update 3D scene options fig.update_scenes( aspectratio=dict(x=1, y=1, z=0.8), aspectmode=&quot;manual&quot; ) # Add dropdown fig.update_layout( updatemenus=[ dict( buttons=list([ dict( args=[&quot;type&quot;, &quot;contour&quot;], label=&quot;Contour&quot;, method=&quot;restyle&quot; ), dict( args=[&quot;type&quot;, &quot;surface&quot;], label=&quot;3D Surface&quot;, method=&quot;restyle&quot; ) ]), direction=&quot;down&quot;, pad={&quot;r&quot;: 10, &quot;t&quot;: 10}, showactive=True, x=0.1, xanchor=&quot;left&quot;, y=1.1, yanchor=&quot;top&quot; ), ], xaxis=dict( title=dim1, ), yaxis=dict( title=dim2, ), scene_camera_eye=dict(x=-1, y=-3, z=0.9), ) else: fig = go.Figure(data=go.Volume( x=grid[0].flatten(), y=grid[1].flatten(), z=grid[2].flatten(), value=chisquared, isomin=np.percentile(chisquared, 0), isomax=np.percentile(chisquared, percentile), opacity=0.1, # needs to be small to see through all surfaces surface_count=10, # needs to be a large number for good volume rendering reversescale=True, )) fig.update_layout( width=700, height=600, autosize=False, margin=dict(t=0, b=0, l=0, r=0), template=&quot;plotly_white&quot;, ) return fig . . Plotting degrees of freedom . 2D slices . Now that all of our functions are ready, let&#39;s first have a go at plotting some 2D slices through the surface, in the region of the global minimum. . In an earlier post, we looked at the verapamil position along $a$ and $c$. In this post, let&#39;s take a look at some of the quaternion components for the verapamil: q1 and q2. . external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(6,7), n_points=100) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, dim1=&quot;Verapamil q1&quot;, dim2=&quot;Verapamil q2&quot;) fig.show() . . . . Now let&#39;s take a look at two of the torsion angles in verapamil, how about torsion 1 and 2, which correspond to the following torsions (using the CURHOM atom-labels): . N1 C9 C10 C11 | C12 C11 C10 C9 | external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(10,11), n_points=100) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, dim1=&quot;N1 C9 C10 C11&quot;, dim2=&quot;C12 C11 C10 C9&quot;) fig.show() . . . . In the examples above, the surfaces that result are relatively smooth and have relatively few local minima. . As a comparison point, let&#39;s regenerate the plots above, but this time instead of visualising around the global minimum, let&#39;s use random coordinates instead. . np.random.seed(314159) random_pos = np.random.uniform(0,1,mystructure.total_position_degrees_of_freedom) random_rot = np.random.uniform(-1,1,mystructure.total_rotation_degrees_of_freedom) random_rot /= np.linalg.norm(random_rot) random_tors = np.random.uniform(-np.pi,np.pi,mystructure.total_internal_degrees_of_freedom) random_point = np.hstack([random_pos, random_rot, random_tors]) print(&quot;Random coordinates:&quot;, random_point) figs = [] for dims in [[6,7, &quot;Verapamil q1&quot;, &quot;Verapamil q2&quot;], [10,11, &quot;N1 C9 C10 C11&quot;, &quot;C12 C11 C10 C9&quot;]]: external, internal, grid, gridpoints = get_points(mystructure, np.copy(random_point), dims=dims[:2], n_points=100) chisquared = get_chi_squared(mystructure, external, internal) figs.append(get_plot(chisquared, grid, gridpoints, dim1=dims[2], dim2=dims[3])) for fig in figs: fig.show() . . Random coordinates: [ 0.81792331 0.5510463 0.41977535 0.09869185 0.81102075 0.9673564 -0.77218306 0.58012779 0.20160524 0.16291222 -0.04343231 -0.1460866 -1.48303189 -1.19629038 2.56325063 -1.97929866 -1.95884954 -2.58621942 2.3512614 0.89477677 2.21686588 0.20772398 2.38385342] . . . . . Things aren&#39;t looking quite so smooth as they were before! . This may partly be due to the scaling effect of no longer having the (very deep) global minimum present. To test that, let&#39;s replot our earlier torsion angle plot, but limit the minimum $ chi^2$ to the 5th percentile value. With this, we get about the same range of $ chi^2$ values as in the torsion angle plot around the random point. As we can see, the surface around the global minimum still looks more smooth, albeit with a few more shallow local minima now visible. . external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(10,11), n_points=100) chisquared = get_chi_squared(mystructure, external, internal) chisquared[chisquared &lt; np.percentile(chisquared, 5)] = np.percentile(chisquared, 5) fig = get_plot(chisquared, grid, gridpoints, dim1=&quot;N1 C9 C10 C11&quot;, dim2=&quot;C12 C11 C10 C9&quot;) fig.show() . . . . 3D slices . Now let&#39;s turn our attention to 3D slices through the surface. We&#39;ll use a 3D volume plot from the plotly library to visualise three different dimensions at the same time. Due to the exponential increase in number of points we&#39;ll have to evaluate, the grid resolution will be coming down a bit! To make things a bit easier to see, I&#39;ve reversed the colourscale used, so now orange and yellow represent regions of low $ chi^2$. . Let&#39;s see the position of the chloride ion within the unit cell, with all other degrees of freedom fixed at the global minimum. For clarity, we&#39;ll only visualise the isosurfaces below the fiftieth percentile of the $ chi^2$ values. . external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(3,4,5), n_points=25) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, percentile=50) fig.show() . . . . Let&#39;s do the same thing for another selection of degrees of freedom, in this case, verapamil along $a$, the third quaternion component and the 4th torsion angle, which corresponds to C13 C12 C11 C10. . external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(3,8,13), n_points=25) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, percentile=50) fig.show() . . . . It&#39;s a bit harder to tell what&#39;s going on in these plots - we have to infer the gradient from the coloured isosurfaces. However, to my eye, they still look relatively smooth. Let&#39;s take the same random point as before and plot the same 3D-slices and see if the slices look less smooth. . figs = [] for dims in [[3,4,5], [3,8,13]]: external, internal, grid, gridpoints = get_points(mystructure, np.copy(random_point), dims=dims[:3], n_points=25) chisquared = get_chi_squared(mystructure, external, internal) figs.append(get_plot(chisquared, grid, gridpoints, percentile=50)) for fig in figs: fig.show() . . . . . . Hard to tell! But to me they look more complex. . Plotting random directions . So far, our plots have looked at slicing the hypersurface along known directions such as fractional coordinates or torsion angles. Now we&#39;ll generate some random (almost) orthogonal unit vectors and slice the surface in those directions. . np.random.seed(42) external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), n_vectors=2, n_points=100) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, dim1=&quot;Random 1&quot;, dim2=&quot;Random 2&quot;) fig.show() . . Angle: 91.52349584729872 . . . np.random.seed(43) external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), n_vectors=3, n_points=25) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, percentile=50) fig.show() . . Angle: [89.56994448 91.07328889 91.30510482] . . . Let&#39;s repeat the above plots, this time centred around the random point we generated previously. . np.random.seed(42) external, internal, grid, gridpoints = get_points(mystructure, np.copy(random_point), n_vectors=2, n_points=100) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, dim1=&quot;Random 1&quot;, dim2=&quot;Random 2&quot;) fig.show() . . Angle: 91.52349584729872 . . . np.random.seed(43) external, internal, grid, gridpoints = get_points(mystructure, np.copy(random_point), n_vectors=3, n_points=25) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, percentile=50) fig.show() . . Angle: [89.56994448 91.07328889 91.30510482] . . . Trajectories . Now we&#39;ll have a go at visualising the paths taken by the particles as they are optimised. We will limit ourselves to 2D slices for ease. . GALLOP actually includes a convenience function for recording the trajectories during the local optimisation steps - just add: . minimiser_settings[&quot;save_trajectories&quot;] = True . to the minimiser settings before calling the minimise function, and the result dictionary will have an entry under the key trajectories. . However, we&#39;ll need some slightly different characteristics for our plotting: we need a minimisation function that allows us to fix all of the degrees of freedom other than the ones we are plotting or allow all of the degrees of freedom to refine as usual. . Minimisation function . import tqdm import torch def minimise(structure, external, internal, dims, fix=True, n_iterations=100, lr=0.01): trajectories = [] if fix: tensors = tensor_prep.get_all_required_tensors(structure, external=external, internal=internal, requires_grad=False) grid = torch.from_numpy(np.hstack([external, internal])[:,dims] ).type(torch.float32).cuda() grid.requires_grad = True alldof = torch.cat([tensors[&quot;zm&quot;][&quot;external&quot;], tensors[&quot;zm&quot;][&quot;internal&quot;]], dim=-1) optimizer = torch.optim.Adam([grid], lr=lr, betas=[0.9,0.9]) else: tensors = tensor_prep.get_all_required_tensors(structure, external=external, internal=internal, requires_grad=True) optimizer = torch.optim.Adam([tensors[&quot;zm&quot;][&quot;external&quot;], tensors[&quot;zm&quot;][&quot;internal&quot;]], lr=lr, betas=[0.9,0.9]) local_iters = range(n_iterations) for j in local_iters: # Zero the gradients before each iteration otherwise they accumulate optimizer.zero_grad() if fix: grid_dofs = torch.cat([alldof[:,:dims[0]], grid[:,0].unsqueeze(1), alldof[:,dims[0]+1:dims[1]], grid[:,1].unsqueeze(1), alldof[:,dims[1]+1:] ], dim=-1) tensors[&quot;zm&quot;][&quot;external&quot;] = grid_dofs[:,:structure.total_external_degrees_of_freedom] tensors[&quot;zm&quot;][&quot;internal&quot;] = grid_dofs[:,structure.total_external_degrees_of_freedom:] asymmetric_frac_coords = zm_to_cart.get_asymmetric_coords(**tensors[&quot;zm&quot;]) calculated_intensities = intensities.calculate_intensities( asymmetric_frac_coords, **tensors[&quot;int_tensors&quot;]) chisquared = chi2.calc_chisqd(calculated_intensities, **tensors[&quot;chisqd_tensors&quot;]) trajectories.append([tensors[&quot;zm&quot;][&quot;external&quot;].detach().cpu().numpy(), tensors[&quot;zm&quot;][&quot;internal&quot;].detach().cpu().numpy(), chisquared.detach().cpu().numpy()]) # For the last iteration, don&#39;t step the optimiser, otherwise the chi2 # value won&#39;t correspond to the DoFs if j != n_iterations - 1: L = torch.sum(chisquared) L.backward() optimizer.step() return trajectories . . We&#39;ll also need a function to plot the resultant trajectories. It&#39;ll be useful to compare the slice of the surface before optimisation to the starting points that reached a solution by the end. . import matplotlib.pyplot as plt def plot_trajectories(mystructure, dims, global_minimum, lr=0.01, fix=True, n_iterations=100, n_points=50): external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_minimum), dims=dims, n_points=n_points) trajectories = minimise(mystructure, external, internal, dims, fix=fix, n_iterations=n_iterations, lr=lr) coords = [] chi2s = [] for t in trajectories: coords.append(np.hstack(t[:2])[:,dims]) chi2s.append(t[2]) coords = np.dstack(coords) fig, ax = plt.subplots(1,3,figsize=(44,12)) ax[0].set_title(&quot;$ chi^2$ surface&quot;) ax[0].contour(gridpoints[0], gridpoints[0], chi2s[0].reshape(grid[0].shape), cmap=&quot;viridis&quot;, levels=20) chi2temp = chi2s[-1] - chi2s[-1].min() col=plt.cm.viridis(chi2temp/chi2s[-1].max()) col ax[1].set_title(&quot;Particle trajectories&quot;) ax[1].scatter(coords[:,0,0], coords[:,1,0], color=col[0], s=10, alpha=.125) for i in range(coords.shape[0]): ax[1].plot(coords[i,0,:], coords[i,1,:], color=col[i], alpha=0.125) percent_solved = np.around(100*(chi2s[-1]&lt;60).sum()/chi2s[-1].shape[0], 1) ax[2].set_title(f&quot;Final $ chi^2$ - {percent_solved} % solved&quot;) ax[2].contourf(gridpoints[0], gridpoints[0], chi2s[-1].reshape(grid[0].shape), cmap=&quot;viridis&quot;, levels=20) plt.show() return coords, chi2s, grid, gridpoints . . Let&#39;s now visualise the trajectories taken if we slice along the verapamil position along $a$ and $b$, as well as torsion 1 and torsion 2. The upper plot in each case shows the trajectories if all of the degrees of freedom other than those being plotted are fixed. The lower plot shows the trajectories taken if everything is allowed to refine (as would be normal in GALLOP). . The plots might appear a little small on the blog - if you want larger views, right click on the image and open them in a new tab. . dims=(3,4) print(dims,&quot;others fixed&quot;) fixed_trajectories_34 = plot_trajectories(mystructure, dims, np.copy(global_min), lr=0.01, fix=True, n_iterations=100, n_points=50) print(dims, &quot;others free&quot;) free_trajectories_34 = plot_trajectories(mystructure, dims, np.copy(global_min), lr=0.01, fix=False, n_iterations=100, n_points=50) dims=(10,11) print(dims,&quot;others fixed&quot;) fixed_trajectories_1011 = plot_trajectories(mystructure, dims, np.copy(global_min), lr=0.01, fix=True, n_iterations=100, n_points=50) print(dims, &quot;others free&quot;) free_trajectories_1011 = plot_trajectories(mystructure, dims, np.copy(global_min), lr=0.01, fix=False, n_iterations=100, n_points=50) . . (3, 4) others fixed . (3, 4) others free . (10, 11) others fixed . (10, 11) others free . As we can see, as we allow the other degrees of freedom to refine, we end up with different behaviour. In the example of the fractional coordinates (dims = 3,4) can now see two points within the unit cell that constitute a global minimum after refinement - this is because in the second plot, the chloride ion is free to move to accommodate the different positions of the verapamil molecule. . Another point of interest is the effects of momentum in the Adam optimiser are clearly visible - the trajectories in some cases backtrack after moving in a particular direction for a while. This is because momentum has carried them &quot;uphill&quot;. This property can allow the local optimisation algorithm in GALLOP to escape shallow local minima, as well as pass quickly through flat regions of the hypersurface. . Lastly, let&#39;s animate the trajectories! . from matplotlib import animation def generate_animation(coords, chi2s, grid, gridpoints, name=&quot;fig.gif&quot;, type=&quot;frac&quot;): # First set up the figure, the axis, and the plot element we want to animate fig = plt.figure(figsize=(10,10)) ax = plt.axes(xlim=(gridpoints[0].min(), gridpoints[0].max()), ylim=(gridpoints[1].min(), gridpoints[1].max())) ax.contour(gridpoints[0], gridpoints[1], chi2s[0].reshape(grid[0].shape), cmap=&quot;viridis&quot;, levels=20) scatter = ax.scatter(coords.T[0, 0, :], coords.T[0, 1, :], c=chi2s[-1], s=5, alpha=0.25) # animation function. This is called sequentially def animate(i, coords, chi2s): #scatter.set_offsets(coords.T[i, :, :].T) if type == &quot;frac&quot;: scatter.set_offsets(coords.T[i, :, :].T % 1) elif type == &quot;torsion&quot;: scatter.set_offsets(np.arctan2(np.sin(coords.T[i, :, :].T), np.cos(coords.T[i, :, :].T))) #scatter.set_array(chi2s[i]) scatter.set_array(chi2s[-1]) return scatter, ani = animation.FuncAnimation(fig, animate, frames=range(coords.T.shape[0]), blit=True, fargs=(coords, chi2s), interval=100,) ani.save(name) plt.show() generate_animation(*fixed_trajectories_34, name=&quot;images/animation_34.gif&quot;, type=&quot;frac&quot;) generate_animation(*fixed_trajectories_1011, name=&quot;images/animation_1011.gif&quot;, type=&quot;torsion&quot;) . . Verapamil along a and b . Verapamil torsion 1 and 2 . Things seem to move much more slowly in the torsion angle example - I suspect this is because the gradient on the &quot;flat&quot; region is low enough that it takes a while for the particles to pick up speed! . Conclusions . Using GALLOP and relatively little boilerplate code, it&#39;s possible to easily plot slices through $ chi^2$ hypersurfaces, and visialise trajectories taken by the local optimiser. . In a future post, we&#39;ll look at trying to extend our visualisation to full GALLOP solutions including the particle swarm optimmisation steps. We&#39;ll make use of dimensionality reduction techniques such as Principal Component Analysis and Uniform Manifold Approximation to allow us to visualise all of the degrees of freedom at once! .",
            "url": "https://mspillman.github.io/blog/gallop/pxrd/python/2021/11/07/Slices-of-hypersurfaces.html",
            "relUrl": "/gallop/pxrd/python/2021/11/07/Slices-of-hypersurfaces.html",
            "date": " • Nov 7, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Solving structures with the GALLOP Python API - basic use",
            "content": "Introduction . In my previous post, I went over how to use the GALLOP browser interface to solve the crystal structure of verapamil hydrochloride. . In this post, I&#39;ll go over the basic way to use GALLOP Python API to solve the crystal structure of verapamil hydrochloride. The complete set of fit files I&#39;ll be using are available as a zip archive you can download here. . In a future post, I&#39;ll look at more sophisticated ways of using the GALLOP Python API to customise the optimisaition procedure. . Install GALLOP and import libraries . This cell is only needed if you are going to run this blog via colab - it will install gallop onto the colab virtual machine. . #!git clone https://github.com/mspillman/gallop.git &amp;&amp; cd gallop &amp;&amp; pip install . . Let&#39;s now import the libraries we&#39;ll need for our initial solution of verapamil hydrochloride. . import time import numpy as np import matplotlib.pyplot as plt from gallop.structure import Structure from gallop.optim import local from gallop.optim import Swarm . Create a Structure object and add the data to it . Our next job is to create a GALLOP structure object. The Structure class is used to store all of the information needed for the local optimisation procedure. We can specify a name for the Structure here, which will be used for any files we write out later (e.g. CIFs. We can also set the parameter that tells GALLOP to ignore the positions of hydrogen atoms during local optimisation. This significantly increases both the speed and the total number of particles that can be simulataneously evaluated, so only set this to False if you really need to! . mystructure = Structure(name=&quot;VerapamilHCl&quot;, ignore_H_atoms=True) . Next up, we should add some diffraction data to our Structure object. . Currently, GALLOP accepts data that has been fitted by DASH, GSAS-II and TOPAS. In the future, I&#39;m planning to add the ability to include SHELX-style data which may be of interest for those working with high-pressure single crystal diffraction data. . We need to tell the Structure object what program was used to fit the diffraction data so it knows what to look for: . If using diffraction data fitted by DASH, then we supply the filename for the .sdi and indicate that the source of the data is DASH | If using diffraction data fitted by GSAS-II, then we supply the filename for the .gpx and indicate that the source of the data is GSAS-II | If using diffraction data fitted by TOPAS, then we supply the filename for the .out and indicate that the source of the data is TOPAS | . We can check that the data have been read in correctly by printing out the unit cell parameters and the first few peak intensities. . mystructure.add_data(&quot;files/Verap.sdi&quot;, source=&quot;DASH&quot;) print(&quot;Unit cell:&quot;, mystructure.unit_cell) print(&quot;Intensities 1-5:&quot;,mystructure.intensities[:5]) . Unit cell: [ 7.08991 10.59464 19.20684 100.1068 93.7396 101.561 ] Intensities 1-5: [ 85.705 235.032 0.614 -6.39 225.05 ] . Next we need to add the Z-matrices to the structure object. The Z-matrices are expected to be in the format used by DASH. For more information on this format, see here. . This will automatically print out some information about the Z-matrices by default, though you can supply the argument verbose=False if you&#39;d like to suppress that. . mystructure.add_zmatrix(&quot;files/CURHOM_1.zmatrix&quot;) mystructure.add_zmatrix(&quot;files/CURHOM_2.zmatrix&quot;) . Added Z-matrix with Filename: files/CURHOM_1.zmatrix Non-H atoms: 1 Refineable torsions: 0 Degrees of freedom: 3 Added Z-matrix with Filename: files/CURHOM_2.zmatrix Non-H atoms: 33 Refineable torsions: 13 Degrees of freedom: 20 (7 + 13) . You may have noticed that the verapamil molecule is listed as having 20 degrees of freedom, with 7 external degrees of freedom and 13 torsions. The reason for the 7 in this case is because GALLOP makes use of quaternions to represent the molecular orientation. This gives an additional (redundant) degree of freedom relative to using Euler angles, and hence there are three parameters for the molecular position and four parameters for its orientation. . Initialise a Particle Swarm . Next thing we&#39;ll need is a Particle Swarm optimiser. To do this, we initialise a Swarm object, and then use it to generate the initial external and internal degrees of freedom for our structure. . We need to specify the total number of particles, and how many swarms these should be divided into. Verapamil hydrochloride is relatively simple for GALLOP, so let&#39;s go for 10,000 particles split into 10 swarms (i.e. 1000 particles per swarm). . swarm = Swarm(mystructure, n_particles=10000, n_swarms=10) . Now let&#39;s use the swarm to generate the initial external and internal degrees of freedom. By default, this will use Latin hypercube sampling rather than uniform sampling as it gives a more even coverage of the hypersurface. If you want to use uniform sampling you can supply the argument method=&quot;uniform&quot; to the function below. . We can also include Mogul Distribution Bias information to this function if available, which will bias the initial torsion angles to match the distribution obtained in the CSD. This is accomplished by using DASH to create a DASH batch file (.dbf) which we supply as an additional argument, MDB=&quot;filename.dbf&quot;. . external, internal = swarm.get_initial_positions(method=&quot;latin&quot;, MDB=None) . 100%|██████████| 10/10 [00:00&lt;00:00, 617.70it/s] . The degrees of freedom are organised as follows: . External: Position (x,y,z) for ZM 1 - N | Quaternions (q1,q2,q3,q4) for ZM 1 - N | | Internal: Torsion (t1,...,tn) for ZM 1 - N | | . So for verapamil hydrochloride, we have the following structure to the external DoFs: $$[x_{Cl},y_{Cl},z_{Cl},x_{V},y_{V},z_{V},q1_{V},q2_{V},q3_{V},q4_{V}]$$ . Only the verapamil Z-matrix has any torsion angles, so all DoFs in the internal array correspond to verapamil torsons. . Let&#39;s plot a couple of these DoFs to ensure we have the expected even distribution. Positions are generated in the range [0,1]. Quaternions are generated in the range [-1,1] and torsions are generated in the range [$- pi$, $ pi$]. This is particularly useful if we are using MDB to ensure the resultant distribution matches that expected. . fig, ax = plt.subplots(1,3, figsize=(12,4)) ax[0].hist(external[:,0], rwidth=0.7) ax[0].set_title(&quot;Chloride $x$&quot;) ax[0].set_xlabel(&quot;Fractional coordinate&quot;) ax[1].hist(external[:,7], rwidth=0.7) ax[1].set_title(&quot;Verapamil $q_2$&quot;) ax[1].set_xlabel(&quot;Quaternion&quot;) ax[2].hist(np.rad2deg(internal[:,0]), rwidth=0.7) ax[2].set_title(&quot;Verapamil $ tau_1$&quot;) ax[2].set_xlabel(&quot;Torsion angle&quot;) plt.show() . . These are reassuringly boring plots! . Set up the run parameters and find the learning rate . The next thing we&#39;ll need to do is set up the parameters we want to use for the runs (i.e. number of iterations etc) and also (optionally) use the learning rate finder to come up with a reasonable first attempt learning rate for this structure. . First thing we&#39;ll do is automatically generate a settings dictionary, and then modify those settings if desired. We&#39;ll print out the keys for the dictionary and see if anything needs changing. . minimiser_settings = local.get_minimiser_settings(mystructure) print(minimiser_settings.keys()) . dict_keys([&#39;n_reflections&#39;, &#39;include_dw_factors&#39;, &#39;chi2_solved&#39;, &#39;n_iterations&#39;, &#39;n_cooldown&#39;, &#39;learning_rate&#39;, &#39;learning_rate_schedule&#39;, &#39;verbose&#39;, &#39;use_progress_bar&#39;, &#39;print_every&#39;, &#39;check_min&#39;, &#39;dtype&#39;, &#39;device&#39;, &#39;optimizer&#39;, &#39;loss&#39;, &#39;eps&#39;, &#39;save_CIF&#39;, &#39;streamlit&#39;, &#39;torsion_shadowing&#39;, &#39;Z_prime&#39;, &#39;use_restraints&#39;, &#39;include_PO&#39;, &#39;PO_axis&#39;]) . Most of these should be fine left at their default values. In some cases, you may wish to try solving with fewer reflections than are available in your dataset (perhaps in order to reduce GPU memory use). In such a scenario, you can set the number of reflections to use by modifying the &#39;n_reflections&#39; dictionary value. You can find out about what the other parameters do in the docstring for the gallop.optim.local.minimise() function. . Here, we&#39;ll stick with the default values, which will use all reflections available in the data, the Adam optimiser, will run for 500 local optimisation iterations and will automatically save a CIF of the best structure found after each iteration. . Our next task will be to find a reasonable learning rate using the learning rate finder. Here we set multiplication_factor=None so it is calculated for us (as discussed below). . learning_rate = local.find_learning_rate(mystructure, external=external, internal=internal, minimiser_settings=minimiser_settings, multiplication_factor=None) plt.figure(figsize=(8,6)) plt.plot(learning_rate[0], learning_rate[1]) plt.xlabel(&quot;Learning rate&quot;) plt.ylabel(&quot;$ sum{ chi^2}$&quot;) plt.show() . GALLOP iter 0000 LO iter 0200 min chi2 625.0: 100%|██████████| 200/200 [00:13&lt;00:00, 15.12it/s] . As discussed in my introduction to GALLOP post, we will derive the learning rate from the minimum point on this curve. The learning_rate result obtained above is a list, which contains the following entries: . Trial learning rate values (x-axis) | Losses (y-axis) | The multiplication factor which scales the best learning rate found | The scaled learning rate - we can use this directly, by setting:minimiser_settings[&quot;learning_rate&quot;] = learning_rate[3] . | However, let&#39;s do the scaling process ourselves to see what it looks like. . lrs = learning_rate[0].copy() losses = learning_rate[1].copy() multiplication_factor = learning_rate[2] learning_rate_to_use = learning_rate[3] lrs -= lrs.min() lrs /= lrs.max() losses -= losses.min() losses /= losses.max() minpoint = np.argmin(losses) plt.plot(lrs[minpoint:]-lrs[minpoint:].min(), lrs[minpoint:]-lrs[minpoint:].min(),&quot;:&quot;,alpha=0.5,c=&quot;k&quot;) plt.plot(lrs[minpoint:]-lrs[minpoint:].min(), 0.5*(lrs[minpoint:]-lrs[minpoint:].min()),&quot;-.&quot;, alpha=0.5,c=&quot;k&quot;) plt.plot(lrs[minpoint:]-lrs[minpoint:].min(), 0.25*(lrs[minpoint:]-lrs[minpoint:].min()),&quot;--&quot;, alpha=0.5,c=&quot;k&quot;) plt.plot(lrs[minpoint:]-lrs[minpoint:].min(), losses[minpoint:]) gradient = ((losses[-1] - losses[minpoint]) / (lrs[-1] - lrs[minpoint])) plt.plot(lrs[minpoint:]-lrs[minpoint:].min(), gradient*(lrs[minpoint:]-lrs[minpoint:].min()), c=&quot;r&quot;) plt.xlabel(&#39;normalised learning rate&#39;) plt.ylabel(&#39;rescaled sum&#39;) plt.legend([&quot;y=x&quot;,&quot;y=0.5x&quot;,&quot;y=0.25x&quot;,&quot;rescaled sum&quot;, &quot;approx&quot;], loc=2, prop={&#39;size&#39;: 8}) plt.show() . . As can be seen, the gradient of the red line approximating the blue curve is fairly shallow - less than 0.25. As a result, this tells us that this particular structure is relatively insensitive to the learning rate, so we can use a relatively large learning rate and still expect good performance. . Therefore, we use a multiplication factor of 1.0, meaning that our learning rate will be $1.0 times alpha_{min}$ where $ alpha_{min}$ is the learning rate corresponding to the minimum point on the curve in the previous plot. . best_learning_rate = learning_rate[0][minpoint] minimiser_settings[&quot;learning_rate&quot;] = best_learning_rate . Running GALLOP . We&#39;ve now got everything we need sorted, all we need to do is write a very simple loop that will perform the GALLOP iterations. . The local.minimise() function returns a dictionary with keys external, internal, chi_2 and potentially others depending on arguments supplied. These results are read in by the Swarm object and used to generate a new set of external and internal degrees of freedom. . Let&#39;s have a go at running GALLOP for 10 iterations. . start_time = time.time() # Now we have the GALLOP loop for i in range(10): # First do the local optimisation - notice the **minimiser_settings argument # which takes in the dictionary we created earlier result = local.minimise(mystructure, external=external, internal=internal, run=i, start_time=start_time, **minimiser_settings) # Particle swarm update generates new positions to be optimised external, internal = swarm.update_position(result=result) . GALLOP iter 0001 LO iter 0500 min chi2 389.8: 100%|██████████| 500/500 [00:33&lt;00:00, 15.02it/s] GALLOP iter 0002 LO iter 0500 min chi2 56.3: 100%|██████████| 500/500 [00:32&lt;00:00, 15.21it/s] GALLOP iter 0003 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:32&lt;00:00, 15.24it/s] GALLOP iter 0004 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:32&lt;00:00, 15.62it/s] GALLOP iter 0005 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:33&lt;00:00, 15.13it/s] GALLOP iter 0006 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:31&lt;00:00, 15.68it/s] GALLOP iter 0007 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:32&lt;00:00, 15.59it/s] GALLOP iter 0008 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:33&lt;00:00, 15.03it/s] GALLOP iter 0009 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:33&lt;00:00, 15.04it/s] GALLOP iter 0010 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:32&lt;00:00, 15.30it/s] . Analysing the results . Let&#39;s re-plot the same histograms we made earlier and see how much things have changed. We&#39;ll plot the output directly from the local optimiser as well as the suggested next positions given by the particle swarm that would be used if we were running an additional GALLOP iteration. We&#39;ll also print out how many swarms reached a solution. . swarm_best_chi2 = np.array(swarm.best_subswarm_chi2) print(&quot;Number of swarms that solved the structure:&quot;, (swarm_best_chi2 &lt; 60).sum()) fig, ax = plt.subplots(1,3, figsize=(12,4)) ax[0].hist(result[&quot;external&quot;][:,0], rwidth=0.7) ax[0].hist(external[:,0], rwidth=0.7) ax[0].set_title(&quot;Chloride $x$&quot;) ax[0].set_xlabel(&quot;Fractional coordinate&quot;) ax[1].hist(result[&quot;external&quot;][:,7], rwidth=0.7) ax[1].hist(external[:,7], rwidth=0.7) ax[1].set_title(&quot;Verapamil $q_2$&quot;) ax[1].set_xlabel(&quot;Quaternion&quot;) ax[2].hist(np.rad2deg(result[&quot;internal&quot;][:,0]), rwidth=0.7) ax[2].hist(np.rad2deg(internal[:,0]), rwidth=0.7) ax[2].set_title(&quot;Verapamil $ tau_1$&quot;) ax[2].set_xlabel(&quot;Torsion angle&quot;) plt.legend([&quot;LO&quot;,&quot;PSO&quot;], loc=&quot;upper right&quot;, bbox_to_anchor=(1.4,1)) plt.show() . . Number of swarms that solved the structure: 2 . Unsurprisingly, these distributions have changed thanks to the optimisation. Also note that some of the blue bars sit outside the range of the orange bars - this is because the local optimiser is unbounded whereas the PSO is set to produce starting points within specified ranges. . We&#39;ll look at the distributions in more detail in a minute, however, let&#39;s first take a look at the structure. We can read over the CIFs of the best structure found after each iteration, and then pick one of these to visualise. . import glob cifs = glob.glob(mystructure.name+&quot;*.cif&quot;) for i, fn in enumerate(cifs): print(i+1, fn) . 1 VerapamilHCl_0001_389.846_chisqd_252_refs_0.6_mins.cif 2 VerapamilHCl_0002_56.267_chisqd_252_refs_1.1_mins.cif 3 VerapamilHCl_0003_56.247_chisqd_252_refs_1.7_mins.cif 4 VerapamilHCl_0004_56.245_chisqd_252_refs_2.2_mins.cif 5 VerapamilHCl_0005_56.244_chisqd_252_refs_2.8_mins.cif 6 VerapamilHCl_0006_56.243_chisqd_252_refs_3.3_mins.cif 7 VerapamilHCl_0007_56.244_chisqd_252_refs_3.8_mins.cif 8 VerapamilHCl_0008_56.244_chisqd_252_refs_4.4_mins.cif 9 VerapamilHCl_0009_56.244_chisqd_252_refs_5.0_mins.cif 10 VerapamilHCl_0010_56.244_chisqd_252_refs_5.5_mins.cif . Let&#39;s visualise the first solution with $ chi^2 &lt; 60$, which was obtained after iteration 2. . import py3Dmol from IPython.display import HTML hide_H = True structure_to_display = 2 print(cifs[structure_to_display-1]) with open(cifs[structure_to_display-1], &quot;r&quot;) as cif: lines = [] for line in cif: if hide_H: splitline = list(filter( None,line.strip().split(&quot; &quot;))) if splitline[0] != &quot;H&quot;: lines.append(line) else: lines.append(line) cif.close() cif = &quot; n&quot;.join(lines) view = py3Dmol.view() view.addModel(cif, &quot;cif&quot;, {&quot;doAssembly&quot; : True, &quot;normalizeAssembly&quot;:True, &#39;duplicateAssemblyAtoms&#39;:True}) view.setStyle({&#39;sphere&#39;:{&quot;scale&quot;:0.15}, &#39;stick&#39;:{&quot;radius&quot;:0.25}}) view.addUnitCell() view.zoomTo() view.render() HTML(view.startjs + &quot; n&quot; + view.endjs + &quot; n&quot;) . . VerapamilHCl_0002_56.267_chisqd_252_refs_1.1_mins.cif . You appear to be running in JupyterLab (or JavaScript failed to load for some other reason). You need to install the 3dmol extension: jupyter labextension install jupyterlab_3dmol . Finally, let&#39;s try and get a feel for how the optimised positions of the particles are distributed. The interactive plot below allows you to explore the distribution of optimised positions for each combination of the degrees of freedom. As you can see, the optimised particles tend to cluster around specific combinations of values - this isn&#39;t too surprising. Once a swarm has located the global minimum, all of the other particles in the swarm will begin to move in that direction causing large numbers of particles to have very similar degrees of freedom after a few additional iterations. . Note that due to a quirk of the library I&#39;m using to generate the plots and interactive widget, the first plot you see is the x-coordinate of the chloride ion plotted against itself. This effectively gives a diagonal line which is equivalent to a histogram of the chloride x-coordinate distribution. . import pandas as pd import plotly.graph_objects as go # Combine all the DoFs into a single DataFrame. all_df = pd.DataFrame(np.hstack([result[&quot;external&quot;], result[&quot;internal&quot;]])) # Label the columns so we know what each one is. all_df.columns=[&quot;x_cl&quot;,&quot;y_cl&quot;,&quot;z_cl&quot;, &quot;x_v&quot;,&quot;y_v&quot;,&quot;z_v&quot;, &quot;q1&quot;,&quot;q2&quot;,&quot;q3&quot;,&quot;q4&quot;, &quot;t1&quot;,&quot;t2&quot;,&quot;t3&quot;,&quot;t4&quot;,&quot;t5&quot;,&quot;t6&quot;,&quot;t7&quot;, &quot;t8&quot;,&quot;t9&quot;,&quot;t10&quot;,&quot;t11&quot;,&quot;t12&quot;,&quot;t13&quot;] positions = [&quot;x_cl&quot;,&quot;y_cl&quot;,&quot;z_cl&quot;,&quot;x_v&quot;,&quot;y_v&quot;,&quot;z_v&quot;] quaternions = [&quot;q1&quot;,&quot;q2&quot;,&quot;q3&quot;,&quot;q4&quot;] torsions = [&quot;t1&quot;,&quot;t2&quot;,&quot;t3&quot;,&quot;t4&quot;,&quot;t5&quot;,&quot;t6&quot;,&quot;t7&quot;, &quot;t8&quot;,&quot;t9&quot;,&quot;t10&quot;,&quot;t11&quot;,&quot;t12&quot;,&quot;t13&quot;] # Normalise the data so everything sits in its expected range. # Positions into range 0-1, quaternions set to be unit quaternions and torsions # into range -180 to 180. all_df[positions] = all_df[positions] % 1 all_df[quaternions] /= np.sqrt( (all_df[quaternions]**2).sum(axis=1).values.reshape(-1,1)) all_df[torsions] = np.rad2deg(np.arctan2(np.sin(all_df[torsions]), np.cos(all_df[torsions]))) # Now generate the figure fig = go.Figure() # We&#39;ll use a histogram2dContour plot fig.add_trace(go.Histogram2dContour( x=all_df[&quot;x_cl&quot;], y=all_df[&quot;x_cl&quot;], colorscale = &#39;Viridis&#39;, contours_showlabels = False, nbinsx=10, nbinsy=10, ncontours=20, )) # Add the drop-down menus for selecting the data to plot button_list_x = [] button_list_y = [] for dof in all_df.columns: button_list_x.append(dict( args=[&quot;x&quot;, [all_df[dof].values]], label=dof, method=&quot;restyle&quot; )) button_list_y.append(dict( args=[&quot;y&quot;, [all_df[dof].values]], label=dof, method=&quot;restyle&quot; )) fig.update_layout( updatemenus=[ dict( buttons=button_list_x, direction=&quot;up&quot;, pad={&quot;r&quot;: 10, &quot;t&quot;: 10}, showactive=True, x=0.45, xanchor=&quot;left&quot;, y=-.05, yanchor=&quot;top&quot; ), dict( buttons=button_list_y, direction=&quot;down&quot;, pad={&quot;r&quot;: 10, &quot;t&quot;: 10}, showactive=True, x=-0.18, xanchor=&quot;left&quot;, y=.95, yanchor=&quot;top&quot; ), ] ) # Add the annotations to label the drop-down menus fig.update_layout( annotations=[ dict(text=&quot;x axis&quot;, x=0.52, xref=&quot;paper&quot;, y=-.07, yref=&quot;paper&quot;, align=&quot;left&quot;, showarrow=False), dict(text=&quot;y axis&quot;, x=-.15, xref=&quot;paper&quot;, y=.98, yref=&quot;paper&quot;, showarrow=False), ]) fig.update_layout( width=700, height=700, autosize=False, margin=dict(t=100, b=0, l=0, r=0), ) fig.show() . . . . Let&#39;s compare one of these, say for example the x coordinate for the verapamil molecule and the y coordinate of the chloride ion, and see what the distribution looks like if we include or exclude particles with low values of $ chi^2$ from consideration. . import seaborn as sns xaxis = &quot;x_v&quot; yaxis = &quot;y_cl&quot; limit = 400 n_low = (result[&quot;chi_2&quot;]&lt;=limit).sum() n_high = (result[&quot;chi_2&quot;]&gt;limit).sum() fig, ax = plt.subplots(1,3,figsize=(18,6)) sns.kdeplot(ax=ax[0], x=all_df[xaxis], y=all_df[yaxis],) ax[0].set_title(&quot;All particles&quot;) sns.kdeplot(ax=ax[1], x=all_df[xaxis][result[&quot;chi_2&quot;]&lt;=limit], y=all_df[yaxis][result[&quot;chi_2&quot;]&lt;=limit],) ax[1].set_title(f&quot;{n_low} particles with $ chi^2 leq {limit}$&quot;) sns.kdeplot(ax=ax[2], x=all_df[xaxis][result[&quot;chi_2&quot;]&gt;limit], y=all_df[yaxis][result[&quot;chi_2&quot;]&gt;limit],) ax[2].set_title(f&quot;{n_high} particles with $ chi^2 &gt; {limit}$&quot;) plt.show() . . As we can see, the particles with higher $ chi^2$ values are not as tightly clustered as those with low $ chi^2$ values, and are therefore it&#39;s less likely that their swarms are stuck in deep minima. Reassuringly, there seem to be peaks in the densities at approximately the same coordinates as we see in the low $ chi^2$ distribution, which suggests that if we were to leave GALLOP running for longer, we&#39;d be in with a good chance of obtaining more solutions. . Conclusions . In this post, we&#39;ve been over how to use the GALLOP Python API to solve the crystal structure of verapamil hydrochloride, and done some preliminary exploration of the results. . In future posts, we&#39;ll look at more advanced methods of using the Python API and spend a bit more time diving into the results. .",
            "url": "https://mspillman.github.io/blog/gallop/pxrd/python/2021/11/03/Solving-structures-with-GALLOP-Python-API.html",
            "relUrl": "/gallop/pxrd/python/2021/11/03/Solving-structures-with-GALLOP-Python-API.html",
            "date": " • Nov 3, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Solving structures with the GALLOP browser interface",
            "content": "Introduction . In my previous post, I went over the rationale behind GALLOP and some details about the algorithm. . In this post, I&#39;ll go over how to use GALLOP to solve the crystal structure of verapamil hydrochloride, a calcium channel blocker used in the treatment of arryhythmias. If you want to have a go yourself, you can download the diffraction data in xye format here or my complete set of fit files in a zip archive here. If you don&#39;t have access to DASH, I recommend downloading the full set of fit files so you have access to the Z-matrices which may otherwise be tedious to generate by hand. . Whilst not relevant for verapamil hydrochloride which crystallises in $P bar{1}$, note that currently GALLOP requires the space group to be in the standard setting so if you have a known unit cell in a particular non-standard space group, you should transform it to the standard setting before fitting your diffraction data for GALLOP. . Fitting PXRD data . Fitting data with DASH . Follow the procedure laid out in the DASH user guide tutorial section [pdf] to fit the diffraction data. You should also use DASH to generate your Z-matrices if you do not have them already. To do this, you will need a 3D molecular representation of verapamil hydrochloride which can be read in by DASH and converted to a Z-matrix (ZM). If you don&#39;t want to generate one yourself, I recommend using the CSD reference code CURHOM as the basis for your ZMs. . Once the Pawley refinement has finished, save the files as normal, and ensure you have access to the resultant .sdi, .dsl, .hcv, .pik and .tic files. If you do not want to include Mogul Distribution Bias to bias the starting points for the torsion angles, and you already have ZMs available, you can close DASH at this stage. . If you wish to make use of MDB information and/or need to generate ZMs, continue on through the wizard to the settings for solving the structure with simulated annealing. Read in your molecule file to generate ZMs if required. If you wish to use MDB in your SDPD attempt, follow the instructions in section 10.3.4 in the DASH user guide (linked above). You will then need to generate a single DASH batch file using the Create batch file option highlighted in the figure below: . . The resultant .dbf file should be made available along with the Pawley fit files produced earlier and ZMs ready for use in GALLOP. . Fitting data with GSAS-II . As mentioned in the introduction, I recommend having the DASH produced Z-matrices (ZMs) available to save time generating one in the correct format from scratch. Eventually, I&#39;d like to include the ability to generate ZMs natively in GALLOP, but that&#39;s a few updates down the line at the moment! . For a reference of the format expected if you do want to have a go at making your own, please see here for an overview. . The GSAS-II tutorials should be used as a guide for how to proceed with indexing data and subsequent Pawley refinement - follow to end of Step 2 . GSAS-II by default uses the Analytic Hessian optimisation method. According to the documentation: &quot;It uses a custom-developed least-squares minimizer that uses singular-value decomposition (SVD) to reduce the errors caused by correlated variables and the Levenberg-Marquardt algorithm to up-weight diagonal Hessian terms when refinements fail to lower $ chi^{2}$&quot;. The alterations to the Pawley covariance matrix this introduces causes errors when used in GALLOP (for example, the covariance matrix cannot be inverted or the inverse covariance matrix is not positive-definite, resulting in the potential for negative values of $ chi^2$). To fix this issue, I suggest the following procedure: . Pawley fit the data as directed in the GSAS-II Tutorials - the analytic Hessian method can be used for this. | One happy with the fit, set all refined parameters apart from peak intensities (e.g. peak shape, background etc) as unrefineable. | See figure below: i. Reset all intensities using Pawley create, ii. The intensities will be set back to 100, but not flagged for refinement. iii. Set all intensities to refine (highlight column by clicking the refine column heading, then press Y on your keyboard), iii. Go to Controls and select Analytic Jacobian from the drop-down menu. | Refine | Once refinement is complete, save the project file, and ensure that the .gpx GSAS-II project file and the ZMs are available for use in GALLOP. | . Solving structures with the browser interface . Below is a video (direct link) showing the browser interface being used to solve the structure of CT-DMF2, which has 42 degrees of freedom: The video covers a few of the main features we&#39;ll be looking at. . If you have installed GALLOP locally, then to open the program interface, open a command prompt and type the following commands: . C: Users Username&gt; conda activate gallop (gallop) C: Users Username&gt; gallop . Your web browser will automatically open and the following interface should appear: . . If you are running GALLOP on a cloud environment, then follow the general procedure laid out in the notebooks linked here. . I will write another blog post in the future showing how to deploy GALLOP on the Google Compute Engine cloud environment. . Once GALLOP is running, we will be using the following steps to solve the crystal structure of verapamil hydrochloride: . Upload files | Modify GALLOP parameters | Solve the structure | Download solutions and close GALLOP | 1. Upload files . The radio button in the above screen shot already has &quot;Upload files&quot; selected, which then has an additional context menu to choose the Pawley refinement program you have used. . Select the program you used to fit the diffraction data, and then either drag and drop your DASH or GSAS-II fit files and Z-matrices (ZMs) onto the uploader widget, or select &quot;Browse files&quot; and navigate to the folder containing your fit files and ZMs and select them all for upload. If you wish to use MDB to bias the initial torsion angles used in GALLOP, you should also upload the .dbf file produce earlier. Note that this will only be used to set the MDB torsion angle biasing, and none of the other settings included in the MDB will be used by GALLOP. . You should end up with something that looks like this: . . 2. Modify GALLOP parameters . With only 23 degrees of freedom, verapamil hydrochloride is a relatively simple crystal structure for GALLOP and the default settings should be sufficient to solve it. However, we will make a small change to increase our chance of success. . Click on the Particle Swarm menu in the side bar to expand it. We will then increase the number of swarms from 10 to 20 either by using the + symbol to increment the number, or by deleting the 10 and typing in 20: . . We should also decrease the number of iterations GALLOP will do - 10 should be sufficient. Open the General menu in the side bar, and change the Total number of iterations per run to 10. . . Once we&#39;ve done this, then we should be ready to solve the structure. . 3. Solve the structure . Once you are happy that all the files needed have been uploaded, and you are satisified with the settings for GALLOP to use, press the Solve button. Note that from this point forward, changing any of the settings whilst GALLOP is running will stop the run. You can still open the expandable menus to view settings or extra information provided by GALLOP. . A number of expandable data menus will appear, and a progress bar will appear that tracks the progress of the learning rate finder discussed in my previous post. Once this has finished, the main GALLOP iterations will begin, with their own progress bar to track their progress. Once the first iteration has finished, some additional items will appear on screen. Two expandable boxes (discussed below) will appear, followed by a download link with the text &quot;CIFs for run 1&quot;. Lastly, a table of results for each iteration is displayed, as is an interactive figure showing the $ chi^2$ value found by each of the particles in each of the swarms. You can use your scroll wheel to zoom in, and click to drag to explore this plot without interrupting GALLOP. . . The Show structure expandable item allows you to view an interactive plot of the structure found during the last iteration. Click and drag to rotate, and use your scroll wheel to zoom in and out. This figure will automatically update after each iteration, and plots the best structure found during the last iteration - note that this is not necessarily the best structure found so far. . . If you are using DASH for Pawley fitting, the Show profile expandable item will also be visible. This allows you to see the fit to the diffraction data obtained in the last iteration. . . For the data fit files I have provided, a solution has $ chi^{2}_{int}$ &lt; 60. If you fitted your own data, this will differ. . 4. Download solutions and close GALLOP . You can download CIFs at any time using the link. If you wish to stop GALLOP at any point, you can press the Stop button that appears in the top right corner of the browser window when GALLOP is running. . The link will give you a zip archive containing a CIF of the best solution found after each iteration, and a .json file which gives details of the settings used for the GALLOP run. . Once you are finished, you can safely close the browser window. If running on your local machine, you can then close down the command line window you opened earlier. If running on a cloud notebook, you may wish to shut down the notebook (if using Colab or Kaggle) to conserve your useage quota, or if using a paid service, you may wish to shut down your VM in order to reduce costs. . Conclusions . We&#39;ve covered the general process of how to use GALLOP to solve crystal structures. There are many more settings that can be adjusted to tweak how GALLOP runs. I encourage you to have a poke about the side-bar menus to see if anything jumps out at you. In future blog posts, I&#39;ll highlight some of the advanced settings that can be used, which can provide performance improvements for certain types of crystal structure. . In my next post, I will cover how to use the Python API to solve crystal structures. Whilst this requires a bit of Python knowledge, it gives more flexibility than the browser interface and can be used to do some cool stuff! .",
            "url": "https://mspillman.github.io/blog/gallop/pxrd/python/2021/11/02/Solving-structures-with-GALLOP-browser-interface.html",
            "relUrl": "/gallop/pxrd/python/2021/11/02/Solving-structures-with-GALLOP-browser-interface.html",
            "date": " • Nov 2, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Introduction to GALLOP",
            "content": "Introduction . This post is intended to give an overview of GALLOP, an algorithm I recently published alongside my friend, PhD supervisor and colleague, Prof. Kenneth Shankland. If you aren&#39;t familiar with global optimisation (GO) methods for crystal structure determination from powder diffraction data (SDPD), I recommend the following resources: . Experimental Analysis of Powder Diﬀraction Data | An overview of currently used structure determination methods for powder diffraction data | The principles underlying the use of powder diffraction data in solving pharmaceutical crystal structures | . Abbreviations I&#39;ll be using: . SDPD = Crystal structure determination from powder diffraction data | PXRD = Powder X-ray diffraction data | GO = Global optimisation | LO = Local optimisation | GPU = Graphics processing unit | CPU = Central processing unit | SA = Simulated annealing | PSO = Particle swarm optimisation | ML = Machine learning | . Background . GALLOP is the culmination of several years of work, which kicked off back in 2010 with an article published by Kenneth Shankland and co-workers, who showed that contrary to the wisdom at the time, local optimisation (LO) algorithms were capable of solving the crystal structures of small molecules, provided that several tens of thousands of attempts from random starting positions on the $ chi^{2}$ hypersurface were performed. In addition to solving the crystal structure, this also gives the locations of the stationary points on the hypersurface. . Interestingly, they showed that using this method, the global minimum on the hypersurface was located more frequently than any other minimum. This indicates that &quot;the topology of the surface is such that the net ‘catchment’ area of stationary points with very low values of $ chi^{2}$ is significantly larger than that of the vast majority of stationary points.&quot; The figure below, taken from the article, shows the distribution of $ chi^{2}$ values for stationary points on the 15-dimensional hypersurface for capsaicin. . . I carried on investigating this method as part of my PhD, and my results confirmed that this approach is effective at solving crystal structures, even of high-complexity (up to 42 degrees of freedom!). However, despite the intriguing results, the approach was not adopted on a wide scale by the SDPD community, perhaps because the performance it offers is approximately the same existing GO-based programs. The code I was using was written in C++, a language I do not know at all well, so I was unable to contribute much to its further development. . A few years after finishing my PhD, I decided I wanted to try writing my own SDPD code in Python. Whilst Python is notoriously slow, my rationale was that Python is much easier to learn than C++, so should provide a lower barrier to entry for people seeking to try out new ideas for SDPD. My first prototype used numpy to try to speed up the code, and borrowed heavily from pymatgen, a fantastic open-source library with lots of crystallographic functionality. Eventually with some help from Kenneth, I had a system which allowed me to easily try out lots of different algorithms, such as those included in scipy.optimize, which features a variety of local and global optimisation algorithms. . In parallel to this, it seemed like every day incredible new results from the field of deep learning were coming out, showing state-of-the-art performance in wide variety of domains. Most neural networks are trained using backpropagation, an algorithm which makes use of automatic differentiation to calculate the gradient of the cost function (which provides a measure of how well the neural network is performing its task) with respect to the parameters of the neural network. Variants of stochastic gradient descent are then used to modify the parameters of the neural network in order to improve the performance of the neural network as measured by the cost function. Whilst neural networks have been in use for over half a century, part of the reason for the explosion in activity was the availability of GPUs and tools to leverage their parallel processing capabilities. . I took an interest in this, and quickly realised that most of the libraries used for this work had well supported python APIs. Some of them, such as PyTorch, are so similar to numpy that it seemed logical to try to port my code to make use of these libraries. This would give both GPU-acceleration and automatic differentiation capabilities for relatively little effort! . Rationale for GALLOP . With my code now capable of running on GPUs, it might seem obvious to implement GPU-versions of commonly used existing algorithms for SDPD such as simulated annealing (SA), parallel tempering and others. However, despite the parallel processing capabilities of GPUs, I found that the performance with GO methods is not particularly good (at least with my code!). Using SA as an example, then yes, it&#39;s possible to run thousands of simultaneous runs on a single GPU, but the number of iterations that can be performed per second is laughably slow in comparison to performance on a CPU. Therefore, because algorithms like SA take a large number of iterations converge, the performance benefits of parallel processing are offset by the amount of time needed to process the large number of iterations required to reach the global minimum. . In contrast to GO algorithms, LO algorithms with access to gradients converge much more rapidly. The automatic differentiation capabilities provided by PyTorch allow gradients to be calculated rapidly, without any additional code to be written. The gradients so obtained are exactly equivalent to the analytical gradient, and are obtained much more rapidly than the approximate gradient that would be obtained via the method of finite differences. Therefore, when processing large numbers of LO runs on a GPU, because they converge much more rapidly than GO methods, you don&#39;t need to wait for a long time to know if any of the runs have been successful! . The next piece of the puzzle is the idea that even if the global minimum is not located (i.e. the structure hasn&#39;t yet been solved), the previously optimised positions may contain some useful information about the crystal structure. This might not be obvious at first, but let&#39;s try to convince ourselves by taking a look at the hypersurface of verapamil hydrochloride, a structure with 23 degrees of freedom. This interactive figure shows a 2D slice through the hypersurface with all degrees of freedom set to their correct crystallographic values, apart from the fractional coordinates of the verapamil molecule along a and b, which form the axes plotted here. . . . I&#39;ll write another blog post in the future showing how to use the GALLOP code to generate plots like this. . There are several local minima present, each of which represents an incorrect structure that has either 21 or 22 of its 23 degrees of freedom correctly determined. Despite this, the $ chi^{2}$ at each local minimum gives no indication that the result is so close to correct. This means that even failed runs with high values of $ chi^{2}$ may be close to the global minimum. If we could accumulate and exploit this information to influence where the next set of local optimisations start, then it might be possible to save time in searching for the global minimum. . With this idea in mind, I tried a few things to attempt to recycle information from &quot;failed&quot; local optimisation attempts, including using kernel density estimation to try to resample various degrees of freedom depending based on the density of solutions that ended up with particular coordinates. This definitely showed an improvement relative to random sampling, but proved inconsistent in terms of the level of improvement obtained. Perhaps I&#39;ll revisit this in a future blog post as I still think there&#39;s something there that may be of use. . Eventually, I ended up trying particle swarm optimisation (PSO) to attempt to recycle the optimised positions. A few things attracted me to PSO: . It&#39;s a simple algorithm to implement - just a few lines of code got it working as a proof of concept | The algorithm maintains a memory of &quot;good&quot; solutions so there&#39;s less risk of the algorithm moving in a bad direction and getting stuck there | It&#39;s shown great performance in a wide variety of domains | The performance improvement with PSO included was immediately obvious. . The last thing that was needed was a name. GPU-Accelerated LocaL Optimisation and Particle swarm provides both a description of the algorithm and an acronym that gives a hat-tip to DASH. Perfect! . The GALLOP algorithm . As described above, GALLOP is a hybrid algorithm which contains two components, a local optimiser and a particle swarm optimiser. These are described in more detail below. . Local optimisation . The local optimisation algorithm used in GALLOP by default is Adam. This algorithm is very popular for training neural networks, and efficient implementations are available in almost every deep learning library. . Adam incorporates two distinct innovations that improve its performance relative to (stochastic) gradient descent. . Adam has a per-parameter adaptive step size rather than a single global step size used for all parameters. This is useful as different degrees of freedom will have different effects on $ chi^2$ for the same percentage change in the parameter value. For example, the translation of a whole molecule within a unit cell affects the position of more scattering atoms than changing a torsion angle. What&#39;s nice is that Adam automatically adjusts the step size for each parameter as it goes, meaning that a suitable step size is used throughout optimisation. | Adam incorporates momentum, which helps it to escape shallow local minima, pass rapidly through flat regions of the hypersurface and dampens uncesessary oscillations in the optimisation trajectory. For an excellent overview of momentum (with a focus on ML applications), see this article: https://distill.pub/2017/momentum/ | Adam . Using the gradient obtained by automatic differentiation, $ bold{g}_t$, Adam stores exponentially decaying averages of the gradients, $ bold{m}_t$, and squared gradients, $ bold{v}_{t}$, which are then used in conjunction with the overall step size, $ alpha$, to give a suitable step size for each parameter being optimised. The parameters $ beta_1$ and $ beta_2$ are numbers less than one that control the rate at which the past gradients and squared gradients respectively decay. . $$ bold{m}_t = beta_{1} bold{m}_{t-1} + (1- beta_{1}) bold{g}_t $$ . $$ bold{v}_t = beta_{2} bold{v}_{t-1} + (1- beta_2) bold{g}_t^2$$ . Because $ bold{m}_t$ and $ bold{v}_{t}$ are initialised as vectors of zeros, the authors of Adam use the following corrective terms to reduce the effect of this biasing, which can be particularly problematic in the early stages of optimisation: . $$ hat{ bold{m}}_t = frac{ bold{m}_t}{1- beta_1}$$ . $$ hat{ bold{v}}_t = frac{ bold{v}_t}{1- beta_2}$$ . These bias corrected terms are then used to update the parameters to be optimised, $ bold{x}_t$, where $ epsilon$ is included to prevent numerical errors: . $$ bold{x}_{t+1} = bold{x}_t - frac{ alpha}{ sqrt{ hat{ bold{v}}_t} + epsilon} hat{ bold{m}}_t $$ . The authors of Adam suggest default parameters of $ beta_1 = 0.9$, $ beta_2 = 0.999$ and $ epsilon = 1 times 10^{-8}$. The step size, $ alpha$, must be set by the user. By default GALLOP sets $ beta_2 = 0.9$ which decays the past squared gradients more rapidly, and was found in our testing to be more effective than the default value. . Learning rate finder . To make life easy for end users (and myself), I wanted a way to avoid having to experiment to find a suitable step size ($ alpha$) to use in GALLOP. . The deep learning library, fast.ai includes a heuristic known as the learning rate finder, which is used to set the step size (referred to as the learning rate in ML-parlance) for deep learning experiments automatically. This is used in conjunction with a step-size alteration policy which is carried out during optimisation, as described here. . After some testing and experimentation, GALLOP now makes use of a slightly modified version, as described below. . A set of 200 log-linearly spaced learning rates are initialised, ranging from $1 times 10^{-4}$ and $0.15$. Starting with the smallest, GALLOP is run on the structure of interest, and the step size increased to the next value after every iteration. The sum of the $ chi^2$ values is recorded after each iteration, and subsequently plotted. . The minimum point on this plot, $ alpha_{min}$, is then used to give the step size. It may be scaled after considering the gradient of the line as the step size is increased beyond $ alpha_{min}$. . To do this, the step sizes and $ chi^2$ values are rescaled into the range 0-1. The x-axis is shifted such that $ alpha_{min}$ sits at 0, and the data plotted. If the gradient of the resultant curve (approximated by the red straight line below) is &gt; 0.5, then this is considered steep. A steep gradient implies a high sensitivity to the step size, and hence $ alpha_{min}$ is scaled by a factor of 0.5, i.e. GALLOP runs with a step size of $0.5 alpha_{min}$. A medium gradient (between 0.25 and 0.5) results in multiplication factor of 0.75, whilst a shallow gradient (less than 0.25) implies relative insensitivity to the step size, and hence results in a multiplication factor of 1.0. . In the GALLOP browser interface, this information is provided in a plot: . . In my testing, the learning rates obtained in this manner provide a good first attempt for GALLOP and provide reasonable performance over a wide variety of structures. However, this doesn&#39;t mean that they are optimal, and if a structure isn&#39;t solving, it might be worth looking at this parameter more closely. I tend to find that a learning rate of 0.03 - 0.05 tends to work well as a first attempt for most structures. . Particle Swarm optimisation . Particle Swarm Optimisation has previously been used in the context of SDPD in the program PeckCryst. The algorithm used in GALLOP is different to that used in PeckCryst in a number of ways which I&#39;ll try to highlight below. . The equations for the PSO are simple. The velocity of a particle at step $t+1$ $( bold{v}_{t+1})$, is calculated from the velocity at the previous step $( bold{v}_{t})$ and the position of the particle $( bold{x}_t)$ using the following equation: . $$ bold{v}_{t+1} = omega_{t} bold{v}_{t} + c_{1} bold{R}_1( bold{g}_{best} - bold{x}_t) + c_2 bold{R}_2( bold{x}_{best} - bold{x}_t)$$ . Where $ omega_{t}$ is the inertia of the particle (which controls how much the previous velocity influences the next velocity) and $c_1$ and $c_2$ control the maximum step size in the direction of the best particle in the swarm $( bold{g}_{best})$ and best position previously visited by the particle $( bold{x}_{best})$ respectively. In contrast to PeckCryst which uses scalars, in GALLOP, by default $ bold{R}_1$ and $ bold{R}_2$ are diagonal matrices with their elements drawn independently from a standard uniform distribution. This provides more variability in how the particles move which helps to improve the exploration. In addition, the maximum absolute velocity in GALLOP is limited to 1.0. . GALLOP calculates $ omega$ for particle $i$ by ranking all of the $N$ particles in the swarm in terms of their $ chi^2$ value, and the calculating their inertia using the following equation: . $$ omega_i = 0.4 + frac{Rank_i}{2N} $$ . where $Rank_i$ is the position of particle $i$ in a sorted list of their respective $ chi^2$ values. This gives inertia values in the range 0.4 - 0.9, and means that the best particles slow down, whilst the worst particles in the swarm have higher inertias and hence are able to continue moving more rapidly towards (hopefully) promising areas of the hypersurface. . The degrees of freedom are then updated using the velocity and previous parameters according to: . $$ bold{x}_{t+1} = bold{x}_t + bold{v}_{t+1} $$ . Another difference to PeckCryst is the coordinate transform that is performed in GALLOP. The fractional coordinates are transformed to account for the repeating unit cell and the fact that coordinates of -0.1 and 0.9 are equivalent. The torsion angles are also transformed to ensure that the PSO treats angles of +180 and -180 degrees as equivalent. Molecular orientations, represented in GALLOP with quaternions, do not require any transformation. For local optimisation, the degrees of freedom that are optimised are: . $$ DoF_{(LO)} = [ bold{x}_{positions}, bold{x}_{quaternions}, bold{x}_{torsions}] $$ . These are then transformed as follows for use in the PSO: . $$ DoF_{(PSO)} = [ sin{2 pi bold{x}_{positions}}, cos{2 pi bold{x}_{positions}}, bold{x}_{quaternions}, sin{ bold{x}_{torsions}}, cos{ bold{x}_{torsions}}] $$ . Following the PSO update, these are transformed back for use in LO using the two-argument arctangent function which gives values in the range $- pi$ to $ pi$, which therefore necessitates additional scaling by a factor of $1/2 pi$ for the positions. . GALLOP . Bringing it all together, this flow chart shows how the GALLOP algorithm operates: . . Typically, 500 LO steps are performed prior to a single PSO step. . The results reported here demonstrate a significant improvement in performance relative to DASH. The success rate is &gt;30 times that of DASH, and the GPU-acceleration means that the time taken to process the runs is also significantly lower than can be accomplished without distributed computing for the DASH jobs. . In my next post, I&#39;ll go over how to use GALLOP to solve crystal structures. .",
            "url": "https://mspillman.github.io/blog/gallop/pxrd/python/2021/10/30/Introduction-to-GALLOP.html",
            "relUrl": "/gallop/pxrd/python/2021/10/30/Introduction-to-GALLOP.html",
            "date": " • Oct 30, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hi, I’m Mark. . I’m interested in solving the crystal structures of small molecules using powder diffraction data. . I also like learning about and applying machine learning to interesting problems. . In my spare time, I enjoy BJJ, playing the drums and cooking. .",
          "url": "https://mspillman.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Publications",
          "content": "Journal articles . Mark J. Spillman and Kenneth Shankland. “GALLOP: accelerated molecular crystal structure determination from powder diffraction data.” CrystEngComm (2021) doi: 10.1039/d1ce00978h . Okba Al Rahal, Mridul Majumder, Mark J. Spillman, Jacco van de Streek and Kenneth Shankland. “Co-Crystal Structures of Furosemide:Urea and Carbamazepine:Indomethacin Determined from Powder X-Ray Diffraction Data.” Crystals (2020). doi: 10.3390/cryst10010042 . Daniel Nicholls, Kenneth Shankland, Mark J. Spillman and Carole J. Elleman. “Rietveld-Based Quantitative Phase Analysis of Sugars in Confectionery.” Food Analytical Methods (2018) doi: 10.1007/s12161-018-1243-9 . Philippa B. Cranwell, Fred J. Davis, Joanne M. Elliott, John E. Mckendrick, Elizabeth M. Page and Mark J. Spillman. “Encouraging Independent Thought and Learning in First Year Practical Classes.” New directions in the teaching of physical sciences (2017) doi: 10.29311/NDTPS.V0I12.674 . Amanda R. Buist, David S. Edgeley, Elena A. Kabova, Alan R. Kennedy, Debbie Hooper, David G. Rollo, Kenneth Shankland, and Mark J. Spillman. “Salt and Ionic Cocrystalline Forms of Amides: Protonation of Carbamazepine in Aqueous Media.” Crystal Growth &amp; Design (2015) doi: 10.1021/acs.cgd.5b01223 . Mark J. Spillman, Kenneth Shankland, Adrian C. Williams and Jason C. Cole. “CDASH: a cloud-enabled program for structure solution from powder diffraction data.” Journal of Applied Crystallography (2015) doi: 10.1107/S160057671502049X . Kenneth Shankland, Mark J. Spillman, Elena A. Kabova, David S. Edgeley and Norman Shankland. “The Principles Underlying the Use of Powder Diffraction Data in Solving Pharmaceutical Crystal Structures.” Acta Crystallographica Section C (2013) doi: 10.1107/S0108270113028643 . Amanda R. Buist, Alan R. Kennedy, Kenneth Shankland, Norman Shankland and Mark J. Spillman. “Salt Forms of Amides: Protonation and Polymorphism of Carbamazepine and Cytenamide.” Crystal Growth &amp; Design (2013) doi: 10.1021/cg401341y . Mridul Majumder, Graham Buckton, Clare F. Rawlinson-Malone, Adrian C. Williams, Mark J. Spillman, Elna Pidcock and Kenneth Shankland. “Application of Hydrogen-Bond Propensity Calculations to an Indomethacin Nicotinamide (1 : 1) Co-Crystal.” CrystEngComm (2013) doi: 10.1039/C3CE40367J . Mridul Majumder, Graham Buckton, Clare F. Rawlinson-Malone, Adrian C Williams, Mark J. Spillman, Norman Shankland and Kenneth Shankland. “A Carbamazepine-Indomethacin (1 : 1) Cocrystal Produced by Milling.” CrystEngComm (2011) doi: 10.1039/C1CE05650F . . Books . Matthew J. Almond, Mark J. Spillman and Elizabeth M. Page. “Workbook in Inorganic Chemistry”, Oxford University Press (2017). ISBN: 9780198729501. Publisher . Chapter . Mark J. Spillman, Daniel Nicholls and Kenneth Shankland. “Experimental Analysis of Powder Diffraction Data.” (2020). doi: 10.1142/9789811204579_0001, chapter in “Handbook on Big Data and Machine Learning in the Physical Sciences”, World Scientific (2020) .",
          "url": "https://mspillman.github.io/blog/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mspillman.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}