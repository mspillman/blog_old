{
  
    
        "post0": {
            "title": "Estimating molecular volumes to aid in powder X-ray diffraction indexing",
            "content": "Introduction . An article published in 2001 by D. W. M. Hofmann describes how crystallographic databases can be used to derive the average volume occupied by atoms of each element in crystal structures. Using his tabulated values, it&#39;s possible to rapidly estimate the volume occupied by a given molecule, and use this to aid indexing of powder diffraction data. This is particularly useful for laboratory diffraction data, which is generally associated with lower figures of merit such as de Wolff&#39;s $M_{20}$ and Smith and Snyder&#39;s $F_N$, which can make discriminating between alternative options more challenging. Other volume estimation methods, notably the 18 Å³ rule are also commonly used, though Hofmann&#39;s volumes give generally more accurate results. . I&#39;ve put together a freely available web-app, HofCalc, which can be used to conveniently obtain these estimates. It should display reasonably well on mobile devices as well as PCs/laptops. You can access it at the following address: . https://hofcalc.herokuapp.com . . This post will explain how it works, and will look at some examples of how it can be used in practice. I&#39;m grateful to Norman Shankland who provided invaluable feedback and assistance with debugging of the app. . Hofmann volumes . After applying various filters to crystal structures deposited in the CSD, Hofmann ended up with a dataset comprised of 182239 structures. Hofmann only considers the elements up to atomic number 100 (fermium) in his work, and assumes that the volume of the unit cell is equivalent to: . $$V_{est} = sum limits_{i=1}^{100} n_i bar{v_i}(1+ bar{ alpha}T) = bold{n bar{v}}(1+ bar{ alpha}T)$$ . Where $n_i$ is the number of atoms of element $i$ in the unit cell, and $ bar{v_i}$ is the average volume occupied by an atom of element $i$. He also assumes that atomic volumes vary linearly with temperature. . He split the dataset into 20 subsets, then used an iterative least-squares method to solve the above equation for each of the subsets. This allowed him to find the average volumes occupied by atoms of each element, and due to the splitting of the data into subsets, he also obtains their standard deviations. The coefficient of thermal expansion, $ bar{ alpha}$, was found to be $0.95 times 10^{-4} K^{-1}$. This temperature correction factor then allowed him to provide the average volumes for all of the elements represented in the CSD at 298 K. . You can download a .json file containing the 298 K volumes here. . Comparison with other atomic volumes . Let&#39;s compare Hofmann&#39;s volumes to those obtained from other sources. I downloaded the atomic radii data from wikipedia, which I&#39;ve saved as an Excel spreadsheet which you can download here. I converted these radii into volumes (assuming spherical atoms), and have plotted them alongside Hofmann&#39;s volumes in the interactive chart below. . import json import pandas as pd import numpy as np import altair as alt with open(&quot;files/Hofmann-volumes.json&quot;) as hv: hofmann_volumes = json.load(hv) hv.close() vols = [] for i, key in enumerate(hofmann_volumes.keys()): vols.append([i+1, key, hofmann_volumes[key]]) df = pd.DataFrame(vols) df.columns = [&quot;Atomic number&quot;, &quot;Element&quot;, &quot;Hofmann&quot;] df.reset_index(drop=True, inplace=True) df.replace(&quot;N/A&quot;, np.NaN, inplace=True) wikiradii = pd.read_excel(&quot;files/wikipedia_radii.xlsx&quot;) wikiradii.replace(&quot;&quot;, np.NaN, inplace=True) radtype = [&quot;Empirical&quot;,&quot;Calculated&quot;,&quot;vdW&quot;,&quot;Covalent-single&quot;,&quot;Covalent-triple&quot;, &quot;Metallic&quot;] for r in radtype: # Radii are in pm so /100 to convert to angstroms. df[r] = (4*np.pi/3)*(wikiradii[r].values.astype(float)/100)**3 # Convert our dataframe to long-form as this is what is expected by altair dflong = df.melt(&quot;Atomic number&quot;, var_name=&quot;Volume&quot;, value_vars=[&quot;Hofmann&quot;] + radtype) # Restore the element symbols to the long dataframe element = [] for an in dflong[&quot;Atomic number&quot;]: element.append(df[&quot;Element&quot;][df[&quot;Atomic number&quot;] == an].item()) dflong[&quot;Element&quot;] = element # Select tool modifies opacity of plotted points selection = alt.selection_single( name=&#39;Select type of&#39;, fields=[&#39;Volume&#39;], init={&#39;Volume&#39;: &quot;Hofmann&quot;}, bind={&#39;Volume&#39;: alt.binding_select(options=[&quot;Hofmann&quot;] + radtype)} ) # scatter plot, modify opacity based on selection alt.Chart(dflong).mark_point().add_selection( selection ).encode( x=alt.X(&#39;Element:N&#39;,sort=dflong[&quot;Atomic number&quot;].values), y=alt.Y(&quot;value:Q&quot;, axis=alt.Axis(title=&#39;Volume / Å³&#39;)), tooltip=[&#39;Element&#39;, &#39;Volume:N&#39;, &#39;value&#39;], opacity=alt.condition(selection, alt.value(1.0), alt.value(.1)), color=&quot;Volume:N&quot; ).properties(width=675, height=450).configure_axis( grid=False ).configure_view( strokeWidth=0 ) . . As you can see, Hofmann&#39;s CSD-derived values are fairly different to the other sources. Whilst there are some hints of periodicity, it&#39;s nowhere near as pronounced as for the other volumes. You can also see some missing data in all of the plots; in Hoffman&#39;s case, this is due to the absence of certain elements, such as helium, from the crystal structures in his filtered dataset. . Let&#39;s print out some of the statistics describing the data, as well compare the coefficient of variation for each type of volume. . df.describe()[[&quot;Hofmann&quot;]+radtype] . . Hofmann Empirical Calculated vdW Covalent-single Covalent-triple Metallic . count 84.000000 | 91.000000 | 86.000000 | 55.000000 | 95.000000 | 71.000000 | 68.000000 | . mean 39.586310 | 17.084519 | 23.849111 | 37.865054 | 13.250696 | 6.963913 | 19.946436 | . std 13.460829 | 12.248898 | 20.652779 | 33.870162 | 8.900926 | 3.301590 | 12.541749 | . min 5.080000 | 0.065450 | 0.124788 | 7.238229 | 0.137258 | 0.623615 | 5.884949 | . 25% 31.000000 | 10.305995 | 7.377367 | 19.870146 | 7.329463 | 4.988916 | 11.039115 | . 50% 39.300000 | 12.770051 | 19.334951 | 27.833137 | 11.008442 | 6.538266 | 17.157285 | . 75% 49.250000 | 23.632685 | 36.484963 | 39.070796 | 19.160766 | 9.204406 | 24.469830 | . max 74.000000 | 77.951815 | 110.850435 | 176.533179 | 52.306127 | 16.837592 | 77.951815 | . import matplotlib.pyplot as plt plt.figure(figsize=(8,5)) ((df.describe().loc[&quot;std&quot;] / df.describe().loc[&quot;mean&quot;])[[&quot;Hofmann&quot;]+radtype]).plot.bar() plt.ylabel(&quot;Coefficient of variation&quot;) plt.show() . . We see a much lower coefficient of variation for Hofmann&#39;s volumes than the others. . HofCalc - using the web app . HofCalc makes use of two key python libraries to process chemical formulae (pyvalem) and resolve chemical names (PubChemPy) prior to processing. This allows the app to have a really convenient interface for specifying queries (see below), which enables users to easily mix and match between formulae and names to obtain the information they need. . Formulae and names . Basic use . The simplest option is to enter the chemical formula or name of the material of interest. Names are resolved by querying PubChem, so common abbreviations for solvents can often be used e.g. DMF. Note that formulae can be prefixed with a multiple, e.g. 2H2O . Search term Type $V_{Hofmann}$ . ethanol | name | 69.61 | . CH3CH2OH | formula | 69.61 | . water | name | 21.55 | . 2H2O | formula | 43.10 | . Multiple search terms . It is also possible to search for multiple items simultaneously, and mix and match name and formulae by separating individual components with a semicolon. This means that for example, &#39;amodiaquine dihydrochloride dihydrate&#39; can also be entered as &#39;amodiaquine; 2HCl; 2H2O&#39;. . Search term Total $V_{Hofmann}$ . carbamazepine; L-glutamic acid | 497.98 | . zopiclone; 2H2O | 496.02 | . C15H12N2O; CH3CH2COO-; Na+ | 419.79 | . sodium salicylate; water | 204.21 | . amodiaquine dihydrochloride dihydrate | 566.61 | . amodiaquine; 2HCl; 2H2O | 566.61 | . More complex examples - hemihydrates . In cases where fractional multiples of search components are required, such as with hemihydrates, care should be taken to check the evaluated chemical formula for consistency with the expected formula. . Search term Evaluated as $V_{Hofmann}$ Divide by Expected Volume . Calcium sulfate hemihydrate | Ca2 H2 O9 S2 | 253.07 | 2 | 126.53 | . calcium; calcium; sulfate; sulfate; water | Ca2 H2 O9 S2 | 253.07 | 2 | 126.53 | . calcium; sulfate; 0.5H2O | Ca1 H1.0 O4.5 S1 | 126.53 | - | 126.53 | . Codeine phosphate hemihydrate | C36 H50 N2 O15 P2 | 1006.77 | 2 | 503.38 | . codeine; codeine; phosphoric acid; phosphoric acid; water | C36 H50 N2 O15 P2 | 1006.77 | 2 | 503.38 | . codeine; phosphoric acid; 0.5H2O | C18 H25.0 N1 O7.5 P1 | 503.38 | - | 503.38 | . Charged species in formulae . Charges could potentially interfere with the parsing of chemical formulae. For example, two ways of representing an oxide ion: . Search term Evaluated as . O-2 | 1 x O | . O2- | 2 x O | . Whilst is is recommended that charges be omitted from HofCalc queries, if including charges in your queries, ensure that the correct number of atoms has been determined in the displayed atom counts or the downloadable summary file. For more information on formatting formulae, see the pyvalem documentation. . Temperature . The temperature, $T$ (in kelvin) is automatically included in the volume calculation via the following equation: . $$V = sum{n_{i}v_{i}}(1 + alpha(T - 298))$$ . Where $n_{i}$ and $v_{i}$ are the number and Hofmann volume (at 298 K) of the $i$th element in the chemical formula, and $ alpha = 0.95 times 10^{-4} K^{-1}$. . Unit cell volume . If the volume of a unit cell is supplied, then the unit cell volume divided by the estimated molecular volume will also be shown. . Search term $V_{cell}$ $V_{Hofmann}$ $ frac{V_{cell}}{V_{Hofmann}}$ . zopiclone, 2H2O | 1874.61 | 496.02 | 3.78 | . verapamil, HCl | 1382.06 | 667.57 | 2.07 | . Summary Files . Each time HofCalc is used, a downloadable summary file is produced. It is designed to serve both as a record of the query for future reference and also as a method to sense-check the interpretation of the entered terms, with links to the PubChem entries where relevant. An example of the contents of the summary file for the following search terms is given below. . Search term = carbamazepine; indomethacin . T = 293 K . Unit cell volume = 2921.6 Å³ . { &quot;combined&quot;: { &quot;C&quot;: 34, &quot;H&quot;: 28, &quot;N&quot;: 3, &quot;O&quot;: 5, &quot;Cl&quot;: 1 }, &quot;individual&quot;: { &quot;carbamazepine&quot;: { &quot;C&quot;: 15, &quot;H&quot;: 12, &quot;N&quot;: 2, &quot;O&quot;: 1 }, &quot;indomethacin&quot;: { &quot;C&quot;: 19, &quot;H&quot;: 16, &quot;Cl&quot;: 1, &quot;N&quot;: 1, &quot;O&quot;: 4 } }, &quot;user_input&quot;: [ &quot;carbamazepine&quot;, &quot;indomethacin&quot; ], &quot;PubChem CIDs&quot;: { &quot;carbamazepine&quot;: 2554, &quot;indomethacin&quot;: 3715 }, &quot;PubChem URLs&quot;: { &quot;carbamazepine&quot;: &quot;https://pubchem.ncbi.nlm.nih.gov/compound/2554&quot;, &quot;indomethacin&quot;: &quot;https://pubchem.ncbi.nlm.nih.gov/compound/3715&quot; }, &quot;individual_volumes&quot;: { &quot;carbamazepine&quot;: 303.86, &quot;indomethacin&quot;: 427.77 }, &quot;V_Cell / V_Hofmann&quot;: 3.99, &quot;Temperature&quot;: 293, &quot;Hofmann Volume&quot;: 731.62, &quot;Hofmann Density&quot;: 1.35 } . Case study: CT-DMF2 . The crystal structure of chlorothiazide N,N-dimethylformamide, a.k.a CT-DMF2, was solved from laboratory powder diffraction data back in 2007. I decided to try re-indexing the diffraction data to see if HofCalc would be of use. . Using the DASH interface to DICVOL, the following unit cells are suggested: . . Both monoclinic and triclinic cells are obtained with very different unit cell volumes. Whilst the figures of merit certainly push towards accepting the conclusion of a monoclinic unit cell, it&#39;s worth checking to see if this makes sense given the expected composition of the material. In addition, there may be more than one dimethylformamide molecule crystallising with the chlorothiazide - HofCalc may be able to shed some light there too. . The paper states that the solvate was formed by recrystallisation of chlorothiazide from DMF solvent, so it seems logical to try the following permutations: . chlorothiazide alone | chlorothiazide + 1 DMF | chlorothiazide + 2 DMF (etc) | HofCalc query $V_{Hofmann}$ $V_{cell}$ $ frac{V_{cell}}{V_{Hofmann}}$ . chlorothiazide | 284.73 | 2422 (triclinic) | 8.51 | . chlorothiazide | 284.73 | 3950 (monoclinic) | 13.87 | . chlorothiazide; DMF | 385.09 | 2422 (triclinic) | 6.29 | . chlorothiazide; DMF | 385.09 | 3950 (monoclinic) | 10.26 | . chlorothiazide; DMF; DMF | 485.45 | 2422 (triclinic) | 4.99 | . chlorothiazide; DMF; DMF | 485.45 | 3950 (monoclinic) | 8.14 | . chlorothiazide; DMF; DMF; DMF | 585.81 | 2422 (triclinic) | 4.13 | . chlorothiazide; DMF; DMF; DMF | 585.81 | 3950 (monoclinic) | 6.74 | . If we exclude those results with $ frac{V_{cell}}{V_{mol}}$ ratios &gt; 0.25 away from a (crystallographically sensible) whole number, we can see from the table that the most favourable compositions are CT + 2xDMF (monoclinic) and CT + 3xDMF (triclinic). Given the higher figure of merit for the monoclinic unit cell, it seems reasonable to take this forward and attempt space-group determination. Doing this in DASH identifies the most probable space group as $P2_1/c$, which then implies $Z&#39;=2$. This is indeed the correct result. . If we compare this to the commonly used 18 Å³ rule, we end up with the following results: . Possible composition $V_{18Å^{3}}$ $V_{cell}$ $ frac{V_{cell}}{V_{18Å^{3}}}$ . chlorothiazide | 306 | 2422 (triclinic) | 7.92 | . chlorothiazide | 306 | 3950 (monoclinic) | 12.91 | . chlorothiazide; DMF | 396 | 2422 (triclinic) | 6.12 | . chlorothiazide; DMF | 396 | 3950 (monoclinic) | 9.97 | . chlorothiazide; DMF; DMF | 486 | 2422 (triclinic) | 4.98 | . chlorothiazide; DMF; DMF | 486 | 3950 (monoclinic) | 8.13 | . chlorothiazide; DMF; DMF; DMF | 576 | 2422 (triclinic) | 4.20 | . chlorothiazide; DMF; DMF; DMF | 576 | 3950 (monoclinic) | 6.86 | . Again, CT + 2xDMF is in the candidates to check, however, using the 18 Å³ rule, a triclinic pure chlorothiazide unit cell also becomes a viable possibility. Had there been a less clear distinction in the indexing figure-of-merit, this may have resulted in time being wasted on testing this additional possibility. . Conclusions . Hofmann&#39;s volumes give more accurate estimates of molecular volumes in crystals, and should be used in preference to the 18 Å³ rule where possible. . To make this easier for people, the HofCalc web-app can be used to very rapidly and conveniently obtain these estimates. .",
            "url": "https://mspillman.github.io/blog/pxrd/indexing/2021/11/10/Hofcalc.html",
            "relUrl": "/pxrd/indexing/2021/11/10/Hofcalc.html",
            "date": " • Nov 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Visualising hypersurfaces and optimisation trajectories",
            "content": "Introduction . In this post, we&#39;ll be using the GALLOP Python API to to enable us to visualise $ chi^2$ hypersurfaces. These hypersurfaces are functions of the molecular positions, orientations and conformations within the unit cell. As such, their dimensionality is typically high, meaning that we cant visualise them directly. However, we can visualise low-dimensional slices through them. We&#39;ve seen an example of one of these slices an earlier post. . We&#39;ll write a simple function that allows us to specify two or three vector directions, then sample $ chi^2$ at points on a grid between these directions. We&#39;ll also have a go at visualising the trajectories taken by particles initialised at each point on the grid as they are optimised with Adam, the default local optimiser in GALLOP. . For ease, we&#39;ll continue using verapamil hydrochloride as our structure of interest. You can download the DASH fit files and Z-matrices I&#39;ll be using here. . Choice of central point and slice directions . Central point . Usually when we are trying to solve the crystal structure of an unknown small molecule, we are most concerned with finding the global minimum of the $ chi^2$ hypersurface. Therefore, we&#39;ll start off investigating the region around the global minimum. Due to the repeating nature of a crystallographic lattice, and symmetries within unit cells, there are infinitely many points that correspond to the global minimum of the hypersurface - therefore, the coordinates below may not be the same as coordinates you obtain in your own solutions of verapamil hydrochloride. . When GALLOP is used to solve a crystal structure, the degrees of freedom corresponding to the solution are automatically added to the CIF as comments. For example, here is the header of a CIF for verapamil hydrochloride: . # Generated using pymatgen and GALLOP # GALLOP External Coords = [0.68394345,0.45950916,-0.114383094,0.2456631,-0.29722878,-0.10422839,0.3358067,0.7402369,0.7596887,-0.16798618 # GALLOP Internal Coords = -1.2276717,2.9165819,-0.96692395,-1.2034712,3.2261908,2.1036072,-3.2097023,-3.0328763,1.5661795,-0.0071008434,-0.1824131,-0.05715108,0.27950087 # Profile chisqd = 16.741 data_Verap_0006_56.244_chisqd_252_refs_3.6_mins.cif . So as long as we&#39;ve been able to solve the crystal structure of interest with GALLOP, we&#39;ll have easy access to the coordinates of a global minimum point. . We can also plot around a random point on the hypersurface to give a comparison. . Slice directions . In this post, we&#39;ll look at two options for plotting. The first and most obvious will be to plot along some of the degrees of freedom in the structure to see how they interact. . The other option will be to choose random unit vectors and slice the surface along them to get a feel for how all of the degrees of freedom interact. In high dimensional spaces, randomly generated pairs of vectors will be approximately orthogonal so it should be possible to get reasonable levels of independence between the axes in our plots. . Data loading and functions . We need to import some libraries, then load our data and Z-matrices. . import numpy as np from gallop.structure import Structure from gallop import tensor_prep from gallop import zm_to_cart from gallop import intensities from gallop import chi2 mystructure = Structure(name=&quot;Verapamil&quot;, ignore_H_atoms=True) mystructure.add_data(&quot;files/Verap.sdi&quot;, source=&quot;DASH&quot;) mystructure.add_zmatrix(&quot;files/CURHOM_1.zmatrix&quot;, verbose=False) mystructure.add_zmatrix(&quot;files/CURHOM_2.zmatrix&quot;, verbose=False) mystructure.get_total_degrees_of_freedom(verbose=False) # verapamil hydrochloride coordinates of global minimum from CIF. g_external = np.array([0.68394345,0.45950916,-0.114383094,0.2456631,-0.29722878, -0.10422839,0.3358067,0.7402369,0.7596887,-0.16798618]) g_internal = np.array([-1.2276717,2.9165819,-0.96692395,-1.2034712,3.2261908, 2.1036072,-3.2097023,-3.0328763,1.5661795,-0.0071008434, -0.1824131,-0.05715108,0.27950087]) global_min = np.hstack([g_external,g_internal]) . . We also need a function to produce our vectors, grid and points at which to sample $ chi^2$. . def get_vectors_and_angle(n_vectors, structure): # Generate random unit vectors and calculate the angles between them vectors = np.random.uniform(-1,1,(n_vectors, structure.total_degrees_of_freedom)) vectors /= np.linalg.norm(vectors) if n_vectors == 2: angle = np.array(np.rad2deg(np.arccos(np.dot(vectors[0], vectors[1])))) else: angle = np.array([ np.rad2deg(np.arccos(np.dot(vectors[0], vectors[1]))), np.rad2deg(np.arccos(np.dot(vectors[0], vectors[2]))), np.rad2deg(np.arccos(np.dot(vectors[1], vectors[2]))) ]) return vectors, angle def get_points(structure, global_minimum, dims=&quot;random&quot;, n_vectors=2, n_points=50): if dims != &quot;random&quot;: assert len(dims) in [2,3], &quot;Number of dims must be 2 or 3&quot; if dims == &quot;random&quot;: # Randomly generate a set of n_vectors, and continue randomly generating # them until the angles between them are all in the range 88-92 degrees. vectors, angle = get_vectors_and_angle(n_vectors, structure) while np.abs(90 - angle).max() &gt; 2: vectors, angle = get_vectors_and_angle(n_vectors, structure) gridpoints = [np.linspace(-1,1,n_points) for i in range(n_vectors)] print(&quot;Angle:&quot;,angle) else: vectors = np.zeros((len(dims), structure.total_degrees_of_freedom), dtype=np.float32) gridpoints = [] for i, d in enumerate(dims): global_minimum[d] *= 0 vectors[i,d] = 1 # If we are looking at the unit cell dimensions, then we want to # ensure that the range of data we plot captures the range the parameter # can take. if d &lt; structure.total_position_degrees_of_freedom: gridpoints.append(np.linspace(0,1,n_points)) elif d &lt; structure.total_external_degrees_of_freedom: gridpoints.append(np.linspace(-1,1,n_points)) else: gridpoints.append(np.linspace(-np.pi,np.pi,n_points)) if len(gridpoints) == 2: xx, yy = np.meshgrid(gridpoints[0], gridpoints[1]) points = global_minimum + xx.ravel().reshape(-1,1)*vectors[0] + yy.ravel().reshape(-1,1)*vectors[1] grid = [xx, yy] else: xx, yy, zz = np.meshgrid(gridpoints[0], gridpoints[1], gridpoints[2]) points = global_minimum + xx.ravel().reshape(-1,1)*vectors[0] + yy.ravel().reshape(-1,1)*vectors[1] + zz.ravel().reshape(-1,1)*vectors[2] grid = [xx, yy, zz] external = points[:,:structure.total_external_degrees_of_freedom] internal = points[:,structure.total_external_degrees_of_freedom:] return external, internal, grid, gridpoints . . Next we&#39;ll need a function to calculate $ chi^2$ at each point on our grid . def get_chi_squared(mystructure, external, internal): tensors = tensor_prep.get_all_required_tensors(mystructure, external=external, internal=internal, requires_grad=False) asymmetric_frac_coords = zm_to_cart.get_asymmetric_coords(**tensors[&quot;zm&quot;]) calculated_intensities = intensities.calculate_intensities( asymmetric_frac_coords, **tensors[&quot;int_tensors&quot;]) chisquared = chi2.calc_chisqd(calculated_intensities, **tensors[&quot;chisqd_tensors&quot;]) chisquared = chisquared.detach().cpu().numpy() return chisquared . . And lastly, a function to produce our plots . from IPython.display import HTML import plotly.graph_objects as go def get_plot(chisquared, grid, gridpoints, dim1=&quot;x&quot;, dim2=&quot;y&quot;, percentile=50): if len(grid) == 2: # create figure fig = go.Figure() # Add surface trace fig.add_trace( go.Contour(x=gridpoints[0], y=gridpoints[1], z=chisquared.reshape(grid[0].shape), colorscale=&quot;Inferno&quot;) ) # Update plot sizing fig.update_layout( width=700, height=600, autosize=False, margin=dict(t=0, b=0, l=0, r=0), template=&quot;plotly_white&quot;, ) # Update 3D scene options fig.update_scenes( aspectratio=dict(x=1, y=1, z=0.8), aspectmode=&quot;manual&quot; ) # Add dropdown fig.update_layout( updatemenus=[ dict( buttons=list([ dict( args=[&quot;type&quot;, &quot;contour&quot;], label=&quot;Contour&quot;, method=&quot;restyle&quot; ), dict( args=[&quot;type&quot;, &quot;surface&quot;], label=&quot;3D Surface&quot;, method=&quot;restyle&quot; ) ]), direction=&quot;down&quot;, pad={&quot;r&quot;: 10, &quot;t&quot;: 10}, showactive=True, x=0.1, xanchor=&quot;left&quot;, y=1.1, yanchor=&quot;top&quot; ), ], xaxis=dict( title=dim1, ), yaxis=dict( title=dim2, ), scene_camera_eye=dict(x=-1, y=-3, z=0.9), ) else: fig = go.Figure(data=go.Volume( x=grid[0].flatten(), y=grid[1].flatten(), z=grid[2].flatten(), value=chisquared, isomin=np.percentile(chisquared, 0), isomax=np.percentile(chisquared, percentile), opacity=0.1, # needs to be small to see through all surfaces surface_count=10, # needs to be a large number for good volume rendering reversescale=True, )) fig.update_layout( width=700, height=600, autosize=False, margin=dict(t=0, b=0, l=0, r=0), template=&quot;plotly_white&quot;, ) return fig . . Plotting degrees of freedom . 2D slices . Now that all of our functions are ready, let&#39;s first have a go at plotting some 2D slices through the surface, in the region of the global minimum. . In an earlier post, we looked at the verapamil position along $a$ and $c$. In this post, let&#39;s take a look at some of the quaternion components for the verapamil: q1 and q2. . external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(6,7), n_points=100) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, dim1=&quot;Verapamil q1&quot;, dim2=&quot;Verapamil q2&quot;) fig.show() . . . . Now let&#39;s take a look at two of the torsion angles in verapamil, how about torsion 1 and 2, which correspond to the following torsions (using the CURHOM atom-labels): . N1 C9 C10 C11 | C12 C11 C10 C9 | external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(10,11), n_points=100) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, dim1=&quot;N1 C9 C10 C11&quot;, dim2=&quot;C12 C11 C10 C9&quot;) fig.show() . . . . In the examples above, the surfaces that result are relatively smooth and have relatively few local minima. . As a comparison point, let&#39;s regenerate the plots above, but this time instead of visualising around the global minimum, let&#39;s use random coordinates instead. . np.random.seed(314159) random_pos = np.random.uniform(0,1,mystructure.total_position_degrees_of_freedom) random_rot = np.random.uniform(-1,1,mystructure.total_rotation_degrees_of_freedom) random_rot /= np.linalg.norm(random_rot) random_tors = np.random.uniform(-np.pi,np.pi,mystructure.total_internal_degrees_of_freedom) random_point = np.hstack([random_pos, random_rot, random_tors]) print(&quot;Random coordinates:&quot;, random_point) figs = [] for dims in [[6,7, &quot;Verapamil q1&quot;, &quot;Verapamil q2&quot;], [10,11, &quot;N1 C9 C10 C11&quot;, &quot;C12 C11 C10 C9&quot;]]: external, internal, grid, gridpoints = get_points(mystructure, np.copy(random_point), dims=dims[:2], n_points=100) chisquared = get_chi_squared(mystructure, external, internal) figs.append(get_plot(chisquared, grid, gridpoints, dim1=dims[2], dim2=dims[3])) for fig in figs: fig.show() . . Random coordinates: [ 0.81792331 0.5510463 0.41977535 0.09869185 0.81102075 0.9673564 -0.77218306 0.58012779 0.20160524 0.16291222 -0.04343231 -0.1460866 -1.48303189 -1.19629038 2.56325063 -1.97929866 -1.95884954 -2.58621942 2.3512614 0.89477677 2.21686588 0.20772398 2.38385342] . . . . . Things aren&#39;t looking quite so smooth as they were before! . This may partly be due to the scaling effect of no longer having the (very deep) global minimum present. To test that, let&#39;s replot our earlier torsion angle plot, but limit the minimum $ chi^2$ to the 5th percentile value. With this, we get about the same range of $ chi^2$ values as in the torsion angle plot around the random point. As we can see, the surface around the global minimum still looks more smooth, albeit with a few more shallow local minima now visible. . external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(10,11), n_points=100) chisquared = get_chi_squared(mystructure, external, internal) chisquared[chisquared &lt; np.percentile(chisquared, 5)] = np.percentile(chisquared, 5) fig = get_plot(chisquared, grid, gridpoints, dim1=&quot;N1 C9 C10 C11&quot;, dim2=&quot;C12 C11 C10 C9&quot;) fig.show() . . . . 3D slices . Now let&#39;s turn our attention to 3D slices through the surface. We&#39;ll use a 3D volume plot from the plotly library to visualise three different dimensions at the same time. Due to the exponential increase in number of points we&#39;ll have to evaluate, the grid resolution will be coming down a bit! To make things a bit easier to see, I&#39;ve reversed the colourscale used, so now orange and yello represent regions of low $ chi^2$. . Let&#39;s see the position of the chloride ion within the unit cell, with all other degrees of freedom fixed at the global minimum. For clarity, we&#39;ll only visualise the isosurfaces below the fiftieth percentile of the $ chi^2$ values. . external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(3,4,5), n_points=25) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, percentile=50) fig.show() . . . . Let&#39;s do the same thing for another selection of degrees of freedom, in this case, verapamil along $a$, the third quaternion component and the 4th torsion angle, which corresponds to C13 C12 C11 C10. . external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(3,8,13), n_points=25) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, percentile=50) fig.show() . . . . It&#39;s a bit harder to tell what&#39;s going on in these plots - we have to infer the gradient from the coloured isosurfaces. However, to my eye, they still look relatively smooth. Let&#39;s take the same random point as before and plot the same 3D-slices and see if the slices look less smooth. . figs = [] for dims in [[3,4,5], [3,8,13]]: external, internal, grid, gridpoints = get_points(mystructure, np.copy(random_point), dims=dims[:3], n_points=25) chisquared = get_chi_squared(mystructure, external, internal) figs.append(get_plot(chisquared, grid, gridpoints, percentile=50)) for fig in figs: fig.show() . . . . . . Hard to tell! But to me they look more complex. . Plotting random directions . So far, our plots have looked at slicing the hypersurface along known directions such as fractional coordinates or torsion angles. Now we&#39;ll generate some random (almost) orthogonal unit vectors and slice the surface in those directions. . np.random.seed(42) external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), n_vectors=2, n_points=100) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, dim1=&quot;Random 1&quot;, dim2=&quot;Random 2&quot;) fig.show() . . Angle: 91.52349584729872 . . . np.random.seed(43) external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), n_vectors=3, n_points=25) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, percentile=50) fig.show() . . Angle: [89.56994448 91.07328889 91.30510482] . . . Let&#39;s repeat the above plots, this time centred around the random point we generated previously. . np.random.seed(42) external, internal, grid, gridpoints = get_points(mystructure, np.copy(random_point), n_vectors=2, n_points=100) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, dim1=&quot;Random 1&quot;, dim2=&quot;Random 2&quot;) fig.show() . . Angle: 91.52349584729872 . . . np.random.seed(43) external, internal, grid, gridpoints = get_points(mystructure, np.copy(random_point), n_vectors=3, n_points=25) chisquared = get_chi_squared(mystructure, external, internal) fig = get_plot(chisquared, grid, gridpoints, percentile=50) fig.show() . . Angle: [89.56994448 91.07328889 91.30510482] . . . Trajectories . Now we&#39;ll have a go at visualising the paths taken by the particles as they are optimised. We will limit ourselves to 2D slices for ease. . GALLOP actually includes a convenience function for recording the trajectories during the local optimisation steps - just add: . minimiser_settings[&quot;save_trajectories&quot;] = True . to the minimiser settings before calling the minimise function, and the result dictionary will have an entry under the key trajectories. . However, we&#39;ll need some slightly different characteristics for our plotting: we need a minimisation function that allows us to fix all of the degrees of freedom other than the ones we are plotting or allow all of the degrees of freedom to refine as usual. . Minimisation function . import tqdm import torch def minimise(structure, external, internal, dims, fix=True, n_iterations=100, lr=0.01): trajectories = [] if fix: tensors = tensor_prep.get_all_required_tensors(structure, external=external, internal=internal, requires_grad=False) grid = torch.from_numpy(np.hstack([external, internal])[:,dims] ).type(torch.float32).cuda() grid.requires_grad = True alldof = torch.cat([tensors[&quot;zm&quot;][&quot;external&quot;], tensors[&quot;zm&quot;][&quot;internal&quot;]], dim=-1) optimizer = torch.optim.Adam([grid], lr=lr, betas=[0.9,0.9]) else: tensors = tensor_prep.get_all_required_tensors(structure, external=external, internal=internal, requires_grad=True) optimizer = torch.optim.Adam([tensors[&quot;zm&quot;][&quot;external&quot;], tensors[&quot;zm&quot;][&quot;internal&quot;]], lr=lr, betas=[0.9,0.9]) local_iters = range(n_iterations) for j in local_iters: # Zero the gradients before each iteration otherwise they accumulate optimizer.zero_grad() if fix: grid_dofs = torch.cat([alldof[:,:dims[0]], grid[:,0].unsqueeze(1), alldof[:,dims[0]+1:dims[1]], grid[:,1].unsqueeze(1), alldof[:,dims[1]+1:] ], dim=-1) tensors[&quot;zm&quot;][&quot;external&quot;] = grid_dofs[:,:structure.total_external_degrees_of_freedom] tensors[&quot;zm&quot;][&quot;internal&quot;] = grid_dofs[:,structure.total_external_degrees_of_freedom:] asymmetric_frac_coords = zm_to_cart.get_asymmetric_coords(**tensors[&quot;zm&quot;]) calculated_intensities = intensities.calculate_intensities( asymmetric_frac_coords, **tensors[&quot;int_tensors&quot;]) chisquared = chi2.calc_chisqd(calculated_intensities, **tensors[&quot;chisqd_tensors&quot;]) trajectories.append([tensors[&quot;zm&quot;][&quot;external&quot;].detach().cpu().numpy(), tensors[&quot;zm&quot;][&quot;internal&quot;].detach().cpu().numpy(), chisquared.detach().cpu().numpy()]) # For the last iteration, don&#39;t step the optimiser, otherwise the chi2 # value won&#39;t correspond to the DoFs if j != n_iterations - 1: L = torch.sum(chisquared) L.backward() optimizer.step() return trajectories . . We&#39;ll also need a function to plot the resultant trajectories. It&#39;ll be useful to compare the slice of the surface before optimisation to the starting points that reached a solution by the end. . import matplotlib.pyplot as plt def plot_trajectories(mystructure, dims, global_minimum, lr=0.01, fix=True, n_iterations=100, n_points=50): external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_minimum), dims=dims, n_points=n_points) trajectories = minimise(mystructure, external, internal, dims, fix=fix, n_iterations=n_iterations, lr=lr) coords = [] chi2s = [] for t in trajectories: coords.append(np.hstack(t[:2])[:,dims]) chi2s.append(t[2]) coords = np.dstack(coords) fig, ax = plt.subplots(1,3,figsize=(44,12)) ax[0].set_title(&quot;$ chi^2$ surface&quot;) ax[0].contour(gridpoints[0], gridpoints[0], chi2s[0].reshape(grid[0].shape), cmap=&quot;viridis&quot;, levels=20) chi2temp = chi2s[-1] - chi2s[-1].min() col=plt.cm.viridis(chi2temp/chi2s[-1].max()) col ax[1].set_title(&quot;Particle trajectories&quot;) ax[1].scatter(coords[:,0,0], coords[:,1,0], color=col[0], s=10, alpha=.125) for i in range(coords.shape[0]): ax[1].plot(coords[i,0,:], coords[i,1,:], color=col[i], alpha=0.125) percent_solved = np.around(100*(chi2s[-1]&lt;60).sum()/chi2s[-1].shape[0], 1) ax[2].set_title(f&quot;Final $ chi^2$ - {percent_solved} % solved&quot;) ax[2].contourf(gridpoints[0], gridpoints[0], chi2s[-1].reshape(grid[0].shape), cmap=&quot;viridis&quot;, levels=20) plt.show() return coords, chi2s, grid, gridpoints . . Let&#39;s now visualise the trajectories taken if we slice along the verapamil position along $a$ and $b$, as well as torsion 1 and torsion 2. The upper plot in each case shows the trajectories if all of the degrees of freedom other than those being plotted are fixed. The lower plot shows the trajectories taken if everything is allowed to refine (as would be normal in GALLOP). . The plots might appear a little small on the blog - if you want larger views, right click on the image and open them in a new tab. . dims=(3,4) print(dims,&quot;others fixed&quot;) fixed_trajectories_34 = plot_trajectories(mystructure, dims, np.copy(global_min), lr=0.01, fix=True, n_iterations=100, n_points=50) print(dims, &quot;others free&quot;) free_trajectories_34 = plot_trajectories(mystructure, dims, np.copy(global_min), lr=0.01, fix=False, n_iterations=100, n_points=50) dims=(10,11) print(dims,&quot;others fixed&quot;) fixed_trajectories_1011 = plot_trajectories(mystructure, dims, np.copy(global_min), lr=0.01, fix=True, n_iterations=100, n_points=50) print(dims, &quot;others free&quot;) free_trajectories_1011 = plot_trajectories(mystructure, dims, np.copy(global_min), lr=0.01, fix=False, n_iterations=100, n_points=50) . . (3, 4) others fixed . (3, 4) others free . (10, 11) others fixed . (10, 11) others free . As we can see, as we allow the other degrees of freedom to refine, we end up with different behaviour. In the example of the fractional coordinates (dims = 3,4) can now see two points within the unit cell that constitute a global minimum after refinement - this is because in the second plot, the chloride ion is free to move to accommodate the different positions of the verapamil molecule. . Another point of interest is the effects of momentum in the Adam optimiser are clearly visible - the trajectories in some cases backtrack after moving in a particular direction for a while. This is because momentum has carried them &quot;uphill&quot;. This property can allow the local optimisation algorithm in GALLOP to escape shallow local minima, as well as pass quickly through flat regions of the hypersurface. . Lastly, let&#39;s animate the trajectories! . from matplotlib import animation def generate_animation(coords, chi2s, grid, gridpoints, name=&quot;fig.gif&quot;, type=&quot;frac&quot;): # First set up the figure, the axis, and the plot element we want to animate fig = plt.figure(figsize=(10,10)) ax = plt.axes(xlim=(gridpoints[0].min(), gridpoints[0].max()), ylim=(gridpoints[1].min(), gridpoints[1].max())) ax.contour(gridpoints[0], gridpoints[1], chi2s[0].reshape(grid[0].shape), cmap=&quot;viridis&quot;, levels=20) scatter = ax.scatter(coords.T[0, 0, :], coords.T[0, 1, :], c=chi2s[-1], s=5, alpha=0.25) # animation function. This is called sequentially def animate(i, coords, chi2s): #scatter.set_offsets(coords.T[i, :, :].T) if type == &quot;frac&quot;: scatter.set_offsets(coords.T[i, :, :].T % 1) elif type == &quot;torsion&quot;: scatter.set_offsets(np.arctan2(np.sin(coords.T[i, :, :].T), np.cos(coords.T[i, :, :].T))) #scatter.set_array(chi2s[i]) scatter.set_array(chi2s[-1]) return scatter, ani = animation.FuncAnimation(fig, animate, frames=range(coords.T.shape[0]), blit=True, fargs=(coords, chi2s), interval=100,) ani.save(name) plt.show() generate_animation(*fixed_trajectories_34, name=&quot;images/animation_34.gif&quot;, type=&quot;frac&quot;) generate_animation(*fixed_trajectories_1011, name=&quot;images/animation_1011.gif&quot;, type=&quot;torsion&quot;) . . Verapamil along a and b . Verapamil torsion 1 and 2 . Things seem to move much more slowly in the torsion angle example - I suspect this is because the gradient on the &quot;flat&quot; region is low enough that it takes a while for the particles to pick up speed! . Conclusions . Using GALLOP and relatively little boilerplate code, it&#39;s possible to easily plot slices through $ chi^2$ hypersurfaces, and visialise trajectories taken by the local optimiser. . In a future post, we&#39;ll look at trying to extend our visualisation to full GALLOP solutions including the particle swarm optimmisation steps. We&#39;ll make use of dimensionality reduction techniques such as Principal Component Analysis and Uniform Manifold Approximation to allow us to visualise all of the degrees of freedom at once! .",
            "url": "https://mspillman.github.io/blog/gallop/pxrd/python/2021/11/07/Slices-of-hypersurfaces.html",
            "relUrl": "/gallop/pxrd/python/2021/11/07/Slices-of-hypersurfaces.html",
            "date": " • Nov 7, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Solving structures with the GALLOP Python API - basic use",
            "content": "Introduction . In my previous post, I went over how to use the GALLOP browser interface to solve the crystal structure of verapamil hydrochloride. . In this post, I&#39;ll go over the basic way to use GALLOP Python API to solve the crystal structure of verapamil hydrochloride. The complete set of fit files I&#39;ll be using are available as a zip archive you can download here. . In a future post, I&#39;ll look at more sophisticated ways of using the GALLOP Python API to customise the optimisaition procedure. . Install GALLOP and import libraries . This cell is only needed if you are going to run this blog via colab - it will install gallop onto the colab virtual machine. . #!git clone https://github.com/mspillman/gallop.git &amp;&amp; cd gallop &amp;&amp; pip install . . Let&#39;s now import the libraries we&#39;ll need for our initial solution of verapamil hydrochloride. . import time import numpy as np import matplotlib.pyplot as plt from gallop.structure import Structure from gallop.optim import local from gallop.optim import Swarm . Create a Structure object and add the data to it . Our next job is to create a GALLOP structure object. The Structure class is used to store all of the information needed for the local optimisation procedure. We can specify a name for the Structure here, which will be used for any files we write out later (e.g. CIFs. We can also set the parameter that tells GALLOP to ignore the positions of hydrogen atoms during local optimisation. This significantly increases both the speed and the total number of particles that can be simulataneously evaluated, so only set this to False if you really need to! . mystructure = Structure(name=&quot;VerapamilHCl&quot;, ignore_H_atoms=True) . Next up, we should add some diffraction data to our Structure object. . Currently, GALLOP accepts data that has been fitted by DASH, GSAS-II and TOPAS. In the future, I&#39;m planning to add the ability to include SHELX-style data which may be of interest for those working with high-pressure single crystal diffraction data. . We need to tell the Structure object what program was used to fit the diffraction data so it knows what to look for: . If using diffraction data fitted by DASH, then we supply the filename for the .sdi and indicate that the source of the data is DASH | If using diffraction data fitted by GSAS-II, then we supply the filename for the .gpx and indicate that the source of the data is GSAS-II | If using diffraction data fitted by TOPAS, then we supply the filename for the .out and indicate that the source of the data is TOPAS | . We can check that the data have been read in correctly by printing out the unit cell parameters and the first few peak intensities. . mystructure.add_data(&quot;files/Verap.sdi&quot;, source=&quot;DASH&quot;) print(&quot;Unit cell:&quot;, mystructure.unit_cell) print(&quot;Intensities 1-5:&quot;,mystructure.intensities[:5]) . Unit cell: [ 7.08991 10.59464 19.20684 100.1068 93.7396 101.561 ] Intensities 1-5: [ 85.705 235.032 0.614 -6.39 225.05 ] . Next we need to add the Z-matrices to the structure object. The Z-matrices are expected to be in the format used by DASH. For more information on this format, see here. . This will automatically print out some information about the Z-matrices by default, though you can supply the argument verbose=False if you&#39;d like to suppress that. . mystructure.add_zmatrix(&quot;files/CURHOM_1.zmatrix&quot;) mystructure.add_zmatrix(&quot;files/CURHOM_2.zmatrix&quot;) . Added Z-matrix with Filename: files/CURHOM_1.zmatrix Non-H atoms: 1 Refineable torsions: 0 Degrees of freedom: 3 Added Z-matrix with Filename: files/CURHOM_2.zmatrix Non-H atoms: 33 Refineable torsions: 13 Degrees of freedom: 20 (7 + 13) . Initialise a Particle Swarm . Next thing we&#39;ll need is a Particle Swarm optimiser. To do this, we initialise a Swarm object, and then use it to generate the initial external and internal degrees of freedom for our structure. . We need to specify the total number of particles, and how many swarms these should be divided into. Verapamil hydrochloride is relatively simple for GALLOP, so let&#39;s go for 10,000 particles split into 10 swarms (i.e. 1000 particles per swarm). . swarm = Swarm(mystructure, n_particles=10000, n_swarms=10) . Now let&#39;s use the swarm to generate the initial external and internal degrees of freedom. By default, this will use Latin hypercube sampling rather than uniform sampling as it gives a more even coverage of the hypersurface. If you want to use uniform sampling you can supply the argument method=&quot;uniform&quot; to the function below. . We can also include Mogul Distribution Bias information to this function if available, which will bias the initial torsion angles to match the distribution obtained in the CSD. This is accomplished by using DASH to create a DASH batch file (.dbf) which we supply as an additional argument, MDB=&quot;filename.dbf&quot;. . external, internal = swarm.get_initial_positions(method=&quot;latin&quot;, MDB=None) . 100%|██████████| 10/10 [00:00&lt;00:00, 617.70it/s] . The degrees of freedom are organised as follows: . External: Position (x,y,z) for ZM 1 - N | Quaternions (q1,q2,q3,q4) for ZM 1 - N | | Internal: Torsion (t1,...,tn) for ZM 1 - N | | . So for verapamil hydrochloride, we have the following structure to the external DoFs: $$[x_{Cl},y_{Cl},z_{Cl},x_{V},y_{V},z_{V},q1_{V},q2_{V},q3_{V},q4_{V}]$$ . Only the verapamil Z-matrix has any torsion angles, so all DoFs in the internal array correspond to verapamil torsons. . Let&#39;s plot a couple of these DoFs to ensure we have the expected even distribution. Positions are generated in the range [0,1]. Quaternions are generated in the range [-1,1] and torsions are generated in the range [$- pi$, $ pi$]. This is particularly useful if we are using MDB to ensure the resultant distribution matches that expected. . fig, ax = plt.subplots(1,3, figsize=(12,4)) ax[0].hist(external[:,0], rwidth=0.7) ax[0].set_title(&quot;Chloride $x$&quot;) ax[0].set_xlabel(&quot;Fractional coordinate&quot;) ax[1].hist(external[:,7], rwidth=0.7) ax[1].set_title(&quot;Verapamil $q_2$&quot;) ax[1].set_xlabel(&quot;Quaternion&quot;) ax[2].hist(np.rad2deg(internal[:,0]), rwidth=0.7) ax[2].set_title(&quot;Verapamil $ tau_1$&quot;) ax[2].set_xlabel(&quot;Torsion angle&quot;) plt.show() . . These are reassuringly boring plots! . Set up the run parameters and find the learning rate . The next thing we&#39;ll need to do is set up the parameters we want to use for the runs (i.e. number of iterations etc) and also (optionally) use the learning rate finder to come up with a reasonable first attempt learning rate for this structure. . First thing we&#39;ll do is automatically generate a settings dictionary, and then modify those settings if desired. We&#39;ll print out the keys for the dictionary and see if anything needs changing. . minimiser_settings = local.get_minimiser_settings(mystructure) print(minimiser_settings.keys()) . dict_keys([&#39;n_reflections&#39;, &#39;include_dw_factors&#39;, &#39;chi2_solved&#39;, &#39;n_iterations&#39;, &#39;n_cooldown&#39;, &#39;learning_rate&#39;, &#39;learning_rate_schedule&#39;, &#39;verbose&#39;, &#39;use_progress_bar&#39;, &#39;print_every&#39;, &#39;check_min&#39;, &#39;dtype&#39;, &#39;device&#39;, &#39;optimizer&#39;, &#39;loss&#39;, &#39;eps&#39;, &#39;save_CIF&#39;, &#39;streamlit&#39;, &#39;torsion_shadowing&#39;, &#39;Z_prime&#39;, &#39;use_restraints&#39;, &#39;include_PO&#39;, &#39;PO_axis&#39;]) . Most of these should be fine left at their default values. In some cases, you may wish to try solving with fewer reflections than are available in your dataset (perhaps in order to reduce GPU memory use). In such a scenario, you can set the number of reflections to use by modifying the &#39;n_reflections&#39; dictionary value. You can find out about what the other parameters do in the docstring for the gallop.optim.local.minimise() function. . Here, we&#39;ll stick with the default values, which will use all reflections available in the data, the Adam optimiser, will run for 500 local optimisation iterations and will automatically save a CIF of the best structure found after each iteration. . Our next task will be to find a reasonable learning rate using the learning rate finder. Here we set multiplication_factor=None so it is calculated for us (as discussed below). . learning_rate = local.find_learning_rate(mystructure, external=external, internal=internal, minimiser_settings=minimiser_settings, multiplication_factor=None) plt.figure(figsize=(8,6)) plt.plot(learning_rate[0], learning_rate[1]) plt.xlabel(&quot;Learning rate&quot;) plt.ylabel(&quot;$ sum{ chi^2}$&quot;) plt.show() . GALLOP iter 0000 LO iter 0200 min chi2 625.0: 100%|██████████| 200/200 [00:13&lt;00:00, 15.12it/s] . As discussed in my introduction to GALLOP post, we will derive the learning rate from the minimum point on this curve. The learning_rate result obtained above is a list, which contains the following entries: . Trial learning rate values (x-axis) | Losses (y-axis) | The multiplication factor which scales the best learning rate found | The scaled learning rate - we can use this directly, by setting:minimiser_settings[&quot;learning_rate&quot;] = learning_rate[3] . | However, let&#39;s do the scaling process ourselves to see what it looks like. . lrs = learning_rate[0].copy() losses = learning_rate[1].copy() multiplication_factor = learning_rate[2] learning_rate_to_use = learning_rate[3] lrs -= lrs.min() lrs /= lrs.max() losses -= losses.min() losses /= losses.max() minpoint = np.argmin(losses) plt.plot(lrs[minpoint:]-lrs[minpoint:].min(), lrs[minpoint:]-lrs[minpoint:].min(),&quot;:&quot;,alpha=0.5,c=&quot;k&quot;) plt.plot(lrs[minpoint:]-lrs[minpoint:].min(), 0.5*(lrs[minpoint:]-lrs[minpoint:].min()),&quot;-.&quot;, alpha=0.5,c=&quot;k&quot;) plt.plot(lrs[minpoint:]-lrs[minpoint:].min(), 0.25*(lrs[minpoint:]-lrs[minpoint:].min()),&quot;--&quot;, alpha=0.5,c=&quot;k&quot;) plt.plot(lrs[minpoint:]-lrs[minpoint:].min(), losses[minpoint:]) gradient = ((losses[-1] - losses[minpoint]) / (lrs[-1] - lrs[minpoint])) plt.plot(lrs[minpoint:]-lrs[minpoint:].min(), gradient*(lrs[minpoint:]-lrs[minpoint:].min()), c=&quot;r&quot;) plt.xlabel(&#39;normalised learning rate&#39;) plt.ylabel(&#39;rescaled sum&#39;) plt.legend([&quot;y=x&quot;,&quot;y=0.5x&quot;,&quot;y=0.25x&quot;,&quot;rescaled sum&quot;, &quot;approx&quot;], loc=2, prop={&#39;size&#39;: 8}) plt.show() . . As can be seen, the gradient of the red line approximating the blue curve is fairly shallow - less than 0.25. As a result, this tells us that this particular structure is relatively insensitive to the learning rate, so we can use a relatively large learning rate and still expect good performance. . Therefore, we use a multiplication factor of 1.0, meaning that our learning rate will be $1.0 times alpha_{min}$ where $ alpha_{min}$ is the learning rate corresponding to the minimum point on the curve in the previous plot. . best_learning_rate = learning_rate[0][minpoint] minimiser_settings[&quot;learning_rate&quot;] = best_learning_rate . Running GALLOP . We&#39;ve now got everything we need sorted, all we need to do is write a very simple loop that will perform the GALLOP iterations. . The local.minimise() function returns a dictionary with keys external, internal, chi_2 and potentially others depending on arguments supplied. These results are read in by the Swarm object and used to generate a new set of external and internal degrees of freedom. . Let&#39;s have a go at running GALLOP for 10 iterations. . start_time = time.time() # Now we have the GALLOP loop for i in range(10): # First do the local optimisation - notice the **minimiser_settings argument # which takes in the dictionary we created earlier result = local.minimise(mystructure, external=external, internal=internal, run=i, start_time=start_time, **minimiser_settings) # Particle swarm update generates new positions to be optimised external, internal = swarm.update_position(result=result) . GALLOP iter 0001 LO iter 0500 min chi2 389.8: 100%|██████████| 500/500 [00:33&lt;00:00, 15.02it/s] GALLOP iter 0002 LO iter 0500 min chi2 56.3: 100%|██████████| 500/500 [00:32&lt;00:00, 15.21it/s] GALLOP iter 0003 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:32&lt;00:00, 15.24it/s] GALLOP iter 0004 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:32&lt;00:00, 15.62it/s] GALLOP iter 0005 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:33&lt;00:00, 15.13it/s] GALLOP iter 0006 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:31&lt;00:00, 15.68it/s] GALLOP iter 0007 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:32&lt;00:00, 15.59it/s] GALLOP iter 0008 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:33&lt;00:00, 15.03it/s] GALLOP iter 0009 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:33&lt;00:00, 15.04it/s] GALLOP iter 0010 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:32&lt;00:00, 15.30it/s] . Analysing the results . Let&#39;s re-plot the same histograms we made earlier and see how much things have changed. We&#39;ll plot the output directly from the local optimiser as well as the suggested next positions given by the particle swarm that would be used if we were running an additional GALLOP iteration. We&#39;ll also print out how many swarms reached a solution. . swarm_best_chi2 = np.array(swarm.best_subswarm_chi2) print(&quot;Number of swarms that solved the structure:&quot;, (swarm_best_chi2 &lt; 60).sum()) fig, ax = plt.subplots(1,3, figsize=(12,4)) ax[0].hist(result[&quot;external&quot;][:,0], rwidth=0.7) ax[0].hist(external[:,0], rwidth=0.7) ax[0].set_title(&quot;Chloride $x$&quot;) ax[0].set_xlabel(&quot;Fractional coordinate&quot;) ax[1].hist(result[&quot;external&quot;][:,7], rwidth=0.7) ax[1].hist(external[:,7], rwidth=0.7) ax[1].set_title(&quot;Verapamil $q_2$&quot;) ax[1].set_xlabel(&quot;Quaternion&quot;) ax[2].hist(np.rad2deg(result[&quot;internal&quot;][:,0]), rwidth=0.7) ax[2].hist(np.rad2deg(internal[:,0]), rwidth=0.7) ax[2].set_title(&quot;Verapamil $ tau_1$&quot;) ax[2].set_xlabel(&quot;Torsion angle&quot;) plt.legend([&quot;LO&quot;,&quot;PSO&quot;], loc=&quot;upper right&quot;, bbox_to_anchor=(1.4,1)) plt.show() . . Number of swarms that solved the structure: 2 . Unsurprisingly, these distributions have changed thanks to the optimisation. Also note that some of the blue bars sit outside the range of the orange bars - this is because the local optimiser is unbounded whereas the PSO is set to produce starting points within specified ranges. . We&#39;ll look at the distributions in more detail in a minute, however, let&#39;s first take a look at the structure. We can read over the CIFs of the best structure found after each iteration, and then pick one of these to visualise. . import glob cifs = glob.glob(mystructure.name+&quot;*.cif&quot;) for i, fn in enumerate(cifs): print(i+1, fn) . 1 VerapamilHCl_0001_389.846_chisqd_252_refs_0.6_mins.cif 2 VerapamilHCl_0002_56.267_chisqd_252_refs_1.1_mins.cif 3 VerapamilHCl_0003_56.247_chisqd_252_refs_1.7_mins.cif 4 VerapamilHCl_0004_56.245_chisqd_252_refs_2.2_mins.cif 5 VerapamilHCl_0005_56.244_chisqd_252_refs_2.8_mins.cif 6 VerapamilHCl_0006_56.243_chisqd_252_refs_3.3_mins.cif 7 VerapamilHCl_0007_56.244_chisqd_252_refs_3.8_mins.cif 8 VerapamilHCl_0008_56.244_chisqd_252_refs_4.4_mins.cif 9 VerapamilHCl_0009_56.244_chisqd_252_refs_5.0_mins.cif 10 VerapamilHCl_0010_56.244_chisqd_252_refs_5.5_mins.cif . Let&#39;s visualise the first solution with $ chi^2 &lt; 60$, which was obtained after iteration 2. . import py3Dmol from IPython.display import HTML hide_H = True structure_to_display = 2 print(cifs[structure_to_display-1]) with open(cifs[structure_to_display-1], &quot;r&quot;) as cif: lines = [] for line in cif: if hide_H: splitline = list(filter( None,line.strip().split(&quot; &quot;))) if splitline[0] != &quot;H&quot;: lines.append(line) else: lines.append(line) cif.close() cif = &quot; n&quot;.join(lines) view = py3Dmol.view() view.addModel(cif, &quot;cif&quot;, {&quot;doAssembly&quot; : True, &quot;normalizeAssembly&quot;:True, &#39;duplicateAssemblyAtoms&#39;:True}) view.setStyle({&#39;sphere&#39;:{&quot;scale&quot;:0.15}, &#39;stick&#39;:{&quot;radius&quot;:0.25}}) view.addUnitCell() view.zoomTo() view.render() HTML(view.startjs + &quot; n&quot; + view.endjs + &quot; n&quot;) . . VerapamilHCl_0002_56.267_chisqd_252_refs_1.1_mins.cif . You appear to be running in JupyterLab (or JavaScript failed to load for some other reason). You need to install the 3dmol extension: jupyter labextension install jupyterlab_3dmol . Finally, let&#39;s try and get a feel for how the optimised positions of the particles are distributed. The interactive plot below allows you to explore the distribution of optimised positions for each combination of the degrees of freedom. As you can see, the optimised particles tend to cluster around specific combinations of values - this isn&#39;t too surprising. Once a swarm has located the global minimum, all of the other particles in the swarm will begin to move in that direction causing large numbers of particles to have very similar degrees of freedom after a few additional iterations. . Note that due to a quirk of the library I&#39;m using to generate the plots and interactive widget, the first plot you see is the x-coordinate of the chloride ion plotted against itself. This effectively gives a diagonal line which is equivalent to a histogram of the chloride x-coordinate distribution. . import pandas as pd import plotly.graph_objects as go # Combine all the DoFs into a single DataFrame. all_df = pd.DataFrame(np.hstack([result[&quot;external&quot;], result[&quot;internal&quot;]])) # Label the columns so we know what each one is. all_df.columns=[&quot;x_cl&quot;,&quot;y_cl&quot;,&quot;z_cl&quot;, &quot;x_v&quot;,&quot;y_v&quot;,&quot;z_v&quot;, &quot;q1&quot;,&quot;q2&quot;,&quot;q3&quot;,&quot;q4&quot;, &quot;t1&quot;,&quot;t2&quot;,&quot;t3&quot;,&quot;t4&quot;,&quot;t5&quot;,&quot;t6&quot;,&quot;t7&quot;, &quot;t8&quot;,&quot;t9&quot;,&quot;t10&quot;,&quot;t11&quot;,&quot;t12&quot;,&quot;t13&quot;] positions = [&quot;x_cl&quot;,&quot;y_cl&quot;,&quot;z_cl&quot;,&quot;x_v&quot;,&quot;y_v&quot;,&quot;z_v&quot;] quaternions = [&quot;q1&quot;,&quot;q2&quot;,&quot;q3&quot;,&quot;q4&quot;] torsions = [&quot;t1&quot;,&quot;t2&quot;,&quot;t3&quot;,&quot;t4&quot;,&quot;t5&quot;,&quot;t6&quot;,&quot;t7&quot;, &quot;t8&quot;,&quot;t9&quot;,&quot;t10&quot;,&quot;t11&quot;,&quot;t12&quot;,&quot;t13&quot;] # Normalise the data so everything sits in its expected range. # Positions into range 0-1, quaternions set to be unit quaternions and torsions # into range -180 to 180. all_df[positions] = all_df[positions] % 1 all_df[quaternions] /= np.sqrt( (all_df[quaternions]**2).sum(axis=1).values.reshape(-1,1)) all_df[torsions] = np.rad2deg(np.arctan2(np.sin(all_df[torsions]), np.cos(all_df[torsions]))) # Now generate the figure fig = go.Figure() # We&#39;ll use a histogram2dContour plot fig.add_trace(go.Histogram2dContour( x=all_df[&quot;x_cl&quot;], y=all_df[&quot;x_cl&quot;], colorscale = &#39;Viridis&#39;, contours_showlabels = False, nbinsx=10, nbinsy=10, ncontours=20, )) # Add the drop-down menus for selecting the data to plot button_list_x = [] button_list_y = [] for dof in all_df.columns: button_list_x.append(dict( args=[&quot;x&quot;, [all_df[dof].values]], label=dof, method=&quot;restyle&quot; )) button_list_y.append(dict( args=[&quot;y&quot;, [all_df[dof].values]], label=dof, method=&quot;restyle&quot; )) fig.update_layout( updatemenus=[ dict( buttons=button_list_x, direction=&quot;up&quot;, pad={&quot;r&quot;: 10, &quot;t&quot;: 10}, showactive=True, x=0.45, xanchor=&quot;left&quot;, y=-.05, yanchor=&quot;top&quot; ), dict( buttons=button_list_y, direction=&quot;down&quot;, pad={&quot;r&quot;: 10, &quot;t&quot;: 10}, showactive=True, x=-0.18, xanchor=&quot;left&quot;, y=.95, yanchor=&quot;top&quot; ), ] ) # Add the annotations to label the drop-down menus fig.update_layout( annotations=[ dict(text=&quot;x axis&quot;, x=0.52, xref=&quot;paper&quot;, y=-.07, yref=&quot;paper&quot;, align=&quot;left&quot;, showarrow=False), dict(text=&quot;y axis&quot;, x=-.15, xref=&quot;paper&quot;, y=.98, yref=&quot;paper&quot;, showarrow=False), ]) fig.update_layout( width=700, height=700, autosize=False, margin=dict(t=100, b=0, l=0, r=0), ) fig.show() . . . . Let&#39;s compare one of these, say for example the x coordinate for the verapamil molecule and the y coordinate of the chloride ion, and see what the distribution looks like if we include or exclude particles with low values of $ chi^2$ from consideration. . import seaborn as sns xaxis = &quot;x_v&quot; yaxis = &quot;y_cl&quot; limit = 400 n_low = (result[&quot;chi_2&quot;]&lt;=limit).sum() n_high = (result[&quot;chi_2&quot;]&gt;limit).sum() fig, ax = plt.subplots(1,3,figsize=(18,6)) sns.kdeplot(ax=ax[0], x=all_df[xaxis], y=all_df[yaxis],) ax[0].set_title(&quot;All particles&quot;) sns.kdeplot(ax=ax[1], x=all_df[xaxis][result[&quot;chi_2&quot;]&lt;=limit], y=all_df[yaxis][result[&quot;chi_2&quot;]&lt;=limit],) ax[1].set_title(f&quot;{n_low} particles with $ chi^2 leq {limit}$&quot;) sns.kdeplot(ax=ax[2], x=all_df[xaxis][result[&quot;chi_2&quot;]&gt;limit], y=all_df[yaxis][result[&quot;chi_2&quot;]&gt;limit],) ax[2].set_title(f&quot;{n_high} particles with $ chi^2 &gt; {limit}$&quot;) plt.show() . . As we can see, the particles with higher $ chi^2$ values are not as tightly clustered as those with low $ chi^2$ values, and are therefore it&#39;s less likely that their swarms are stuck in deep minima. Reassuringly, there seem to be peaks in the densities at approximately the same coordinates as we see in the low $ chi^2$ distribution, which suggests that if we were to leave GALLOP running for longer, we&#39;d be in with a good chance of obtaining more solutions. . Conclusions . In this post, we&#39;ve been over how to use the GALLOP Python API to solve the crystal structure of verapamil hydrochloride, and done some preliminary exploration of the results. . In future posts, we&#39;ll look at more advanced methods of using the Python API and spend a bit more time diving into the results. .",
            "url": "https://mspillman.github.io/blog/gallop/pxrd/python/2021/11/03/Solving-structures-with-GALLOP-Python-API.html",
            "relUrl": "/gallop/pxrd/python/2021/11/03/Solving-structures-with-GALLOP-Python-API.html",
            "date": " • Nov 3, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Solving structures with the GALLOP browser interface",
            "content": "Introduction . In my previous post, I went over the rationale behind GALLOP and some details about the algorithm. . In this post, I&#39;ll go over how to use GALLOP to solve the crystal structure of verapamil hydrochloride, a calcium channel blocker used in the treatment of arryhythmias. If you want to have a go yourself, you can download the diffraction data in xye format here or my complete set of fit files in a zip archive here. If you don&#39;t have access to DASH, I recommend downloading the full set of fit files so you have access to the Z-matrices which may otherwise be tedious to generate by hand. . Whilst not relevant for verapamil hydrochloride which crystallises in $P bar{1}$, note that currently GALLOP requires the space group to be in the standard setting so if you have a known unit cell in a particular non-standard space group, you should transform it to the standard setting before fitting your diffraction data for GALLOP. . Fitting PXRD data . Fitting data with DASH . Follow the procedure laid out in the DASH user guide tutorial section [pdf] to fit the diffraction data. You should also use DASH to generate your Z-matrices if you do not have them already. To do this, you will need a 3D molecular representation of verapamil hydrochloride which can be read in by DASH and converted to a Z-matrix (ZM). If you don&#39;t want to generate one yourself, I recommend using the CSD reference code CURHOM as the basis for your ZMs. . Once the Pawley refinement has finished, save the files as normal, and ensure you have access to the resultant .sdi, .dsl, .hcv, .pik and .tic files. If you do not want to include Mogul Distribution Bias to bias the starting points for the torsion angles, and you already have ZMs available, you can close DASH at this stage. . If you wish to make use of MDB information and/or need to generate ZMs, continue on through the wizard to the settings for solving the structure with simulated annealing. Read in your molecule file to generate ZMs if required. If you wish to use MDB in your SDPD attempt, follow the instructions in section 10.3.4 in the DASH user guide (linked above). You will then need to generate a single DASH batch file using the Create batch file option highlighted in the figure below: . . The resultant .dbf file should be made available along with the Pawley fit files produced earlier and ZMs ready for use in GALLOP. . Fitting data with GSAS-II . As mentioned in the introduction, I recommend having the DASH produced Z-matrices (ZMs) available to save time generating one in the correct format from scratch. Eventually, I&#39;d like to include the ability to generate ZMs natively in GALLOP, but that&#39;s a few updates down the line at the moment! . For a reference of the format expected if you do want to have a go at making your own, please see here for an overview. . The GSAS-II tutorials should be used as a guide for how to proceed with indexing data and subsequent Pawley refinement - follow to end of Step 2 . GSAS-II by default uses the Analytic Hessian optimisation method. According to the documentation: &quot;It uses a custom-developed least-squares minimizer that uses singular-value decomposition (SVD) to reduce the errors caused by correlated variables and the Levenberg-Marquardt algorithm to up-weight diagonal Hessian terms when refinements fail to lower $ chi^{2}$&quot;. The alterations to the Pawley covariance matrix this introduces causes errors when used in GALLOP (for example, the covariance matrix cannot be inverted or the inverse covariance matrix is not positive-definite, resulting in the potential for negative values of $ chi^2$). To fix this issue, I suggest the following procedure: . Pawley fit the data as directed in the GSAS-II Tutorials - the analytic Hessian method can be used for this. | One happy with the fit, set all refined parameters apart from peak intensities (e.g. peak shape, background etc) as unrefineable. | See figure below: i. Reset all intensities using Pawley create, ii. The intensities will be set back to 100, but not flagged for refinement. iii. Set all intensities to refine (highlight column by clicking the refine column heading, then press Y on your keyboard), iii. Go to Controls and select Analytic Jacobian from the drop-down menu. | Refine | Once refinement is complete, save the project file, and ensure that the .gpx GSAS-II project file and the ZMs are available for use in GALLOP. | . Solving structures with the browser interface . Below is a video (direct link) showing the browser interface being used to solve the structure of CT-DMF2, which has 42 degrees of freedom: The video covers a few of the main features we&#39;ll be looking at. . If you have installed GALLOP locally, then to open the program interface, open a command prompt and type the following commands: . C: Users Username&gt; conda activate gallop (gallop) C: Users Username&gt; gallop . Your web browser will automatically open and the following interface should appear: . . If you are running GALLOP on a cloud environment, then follow the general procedure laid out in the notebooks linked here. . I will write another blog post in the future showing how to deploy GALLOP on the Google Compute Engine cloud environment. . Once GALLOP is running, we will be using the following steps to solve the crystal structure of verapamil hydrochloride: . Upload files | Modify GALLOP parameters | Solve the structure | Download solutions and close GALLOP | 1. Upload files . The radio button in the above screen shot already has &quot;Upload files&quot; selected, which then has an additional context menu to choose the Pawley refinement program you have used. . Select the program you used to fit the diffraction data, and then either drag and drop your DASH or GSAS-II fit files and Z-matrices (ZMs) onto the uploader widget, or select &quot;Browse files&quot; and navigate to the folder containing your fit files and ZMs and select them all for upload. If you wish to use MDB to bias the initial torsion angles used in GALLOP, you should also upload the .dbf file produce earlier. Note that this will only be used to set the MDB torsion angle biasing, and none of the other settings included in the MDB will be used by GALLOP. . You should end up with something that looks like this: . . 2. Modify GALLOP parameters . With only 23 degrees of freedom, verapamil hydrochloride is a relatively simple crystal structure for GALLOP and the default settings should be sufficient to solve it. However, we will make a small change to increase our chance of success. . Click on the Particle Swarm menu in the side bar to expand it. We will then increase the number of swarms from 10 to 20 either by using the + symbol to increment the number, or by deleting the 10 and typing in 20: . . We should also decrease the number of iterations GALLOP will do - 10 should be sufficient. Open the General menu in the side bar, and change the Total number of iterations per run to 10. . . Once we&#39;ve done this, then we should be ready to solve the structure. . 3. Solve the structure . Once you are happy that all the files needed have been uploaded, and you are satisified with the settings for GALLOP to use, press the Solve button. Note that from this point forward, changing any of the settings whilst GALLOP is running will stop the run. You can still open the expandable menus to view settings or extra information provided by GALLOP. . A number of expandable data menus will appear, and a progress bar will appear that tracks the progress of the learning rate finder discussed in my previous post. Once this has finished, the main GALLOP iterations will begin, with their own progress bar to track their progress. Once the first iteration has finished, some additional items will appear on screen. Two expandable boxes (discussed below) will appear, followed by a download link with the text &quot;CIFs for run 1&quot;. Lastly, a table of results for each iteration is displayed, as is an interactive figure showing the $ chi^2$ value found by each of the particles in each of the swarms. You can use your scroll wheel to zoom in, and click to drag to explore this plot without interrupting GALLOP. . . The Show structure expandable item allows you to view an interactive plot of the structure found during the last iteration. Click and drag to rotate, and use your scroll wheel to zoom in and out. This figure will automatically update after each iteration, and plots the best structure found during the last iteration - note that this is not necessarily the best structure found so far. . . If you are using DASH for Pawley fitting, the Show profile expandable item will also be visible. This allows you to see the fit to the diffraction data obtained in the last iteration. . . For the data fit files I have provided, a solution has $ chi^2$ &lt; 60. If you fitted your own data, this will differ. . 4. Download solutions and close GALLOP . You can download CIFs at any time using the link. If you wish to stop GALLOP at any point, you can press the Stop button that appears in the top right corner of the browser window when GALLOP is running. . The link will give you a zip archive containing a CIF of the best solution found after each iteration, and a .json file which gives details of the settings used for the GALLOP run. . Once you are finished, you can safely close the browser window. If running on your local machine, you can then close down the command line window you opened earlier. If running on a cloud notebook, you may wish to shut down the notebook (if using Colab or Kaggle) to conserve your useage quota, or if using a paid service, you may wish to shut down your VM in order to reduce costs. . Conclusions . We&#39;ve covered the general process of how to use GALLOP to solve crystal structures. There are many more settings that can be adjusted to tweak how GALLOP runs. I encourage you to have a poke about the side-bar menus to see if anything jumps out at you. In future blog posts, I&#39;ll highlight some of the advanced settings that can be used, which can provide performance improvements for certain types of crystal structure. . In my next post, I will cover how to use the Python API to solve crystal structures. Whilst this requires a bit of Python knowledge, it gives more flexibility than the browser interface and can be used to do some cool stuff! .",
            "url": "https://mspillman.github.io/blog/gallop/pxrd/python/2021/11/02/Solving-structures-with-GALLOP-browser-interface.html",
            "relUrl": "/gallop/pxrd/python/2021/11/02/Solving-structures-with-GALLOP-browser-interface.html",
            "date": " • Nov 2, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Introduction to GALLOP",
            "content": "Introduction . This post is intended to give an overview of GALLOP, an algorithm I recently published alongside my friend, PhD supervisor and colleague, Prof. Kenneth Shankland. If you aren&#39;t familiar with global optimisation (GO) methods for crystal structure determination from powder diffraction data (SDPD), I recommend the following resources: . Experimental Analysis of Powder Diﬀraction Data | An overview of currently used structure determination methods for powder diffraction data | The principles underlying the use of powder diffraction data in solving pharmaceutical crystal structures | . Abbreviations I&#39;ll be using: . SDPD = Crystal structure determination from powder diffraction data | PXRD = Powder X-ray diffraction data | GO = Global optimisation | LO = Local optimisation | GPU = Graphics processing unit | CPU = Central processing unit | SA = Simulated annealing | PSO = Particle swarm optimisation | ML = Machine learning | . Background . GALLOP is the culmination of several years of work, which kicked off back in 2010 with an article published by Kenneth Shankland and co-workers, who showed that contrary to the wisdom at the time, local optimisation (LO) algorithms were capable of solving the crystal structures of small molecules, provided that several tens of thousands of attempts from random starting positions on the $ chi^{2}$ hypersurface were performed. In addition to solving the crystal structure, this also gives the locations of the stationary points on the hypersurface. . Interestingly, they showed that using this method, the global minimum on the hypersurface was located more frequently than any other minimum. This indicates that &quot;the topology of the surface is such that the net ‘catchment’ area of stationary points with very low values of $ chi^{2}$ is significantly larger than that of the vast majority of stationary points.&quot; The figure below, taken from the article, shows the distribution of $ chi^{2}$ values for stationary points on the 15-dimensional hypersurface for capsaicin. . . I carried on investigating this method as part of my PhD, and my results confirmed that this approach is effective at solving crystal structures, even of high-complexity (up to 42 degrees of freedom!). However, despite the intriguing results, the approach was not adopted on a wide scale by the SDPD community, perhaps because the performance it offers is approximately the same existing GO-based programs. The code I was using was written in C++, a language I do not know at all well, so I was unable to contribute much to its further development. . A few years after finishing my PhD, I decided I wanted to try writing my own SDPD code in Python. Whilst Python is notoriously slow, my rationale was that Python is much easier to learn than C++, so should provide a lower barrier to entry for people seeking to try out new ideas for SDPD. My first prototype used numpy to try to speed up the code, and borrowed heavily from pymatgen, a fantastic open-source library with lots of crystallographic functionality. Eventually with some help from Kenneth, I had a system which allowed me to easily try out lots of different algorithms, such as those included in scipy.optimize, which features a variety of local and global optimisation algorithms. . In parallel to this, it seemed like every day incredible new results from the field of deep learning were coming out, showing state-of-the-art performance in wide variety of domains. Most neural networks are trained using backpropagation, an algorithm which makes use of automatic differentiation to calculate the gradient of the cost function (which provides a measure of how well the neural network is performing its task) with respect to the parameters of the neural network. Variants of stochastic gradient descent are then used to modify the parameters of the neural network in order to improve the performance of the neural network as measured by the cost function. Whilst neural networks have been in use for over half a century, part of the reason for the explosion in activity was the availability of GPUs and tools to leverage their parallel processing capabilities. . I took an interest in this, and quickly realised that most of the libraries used for this work had well supported python APIs. Some of them, such as PyTorch, are so similar to numpy that it seemed logical to try to port my code to make use of these libraries. This would give both GPU-acceleration and automatic differentiation capabilities for relatively little effort! . Rationale for GALLOP . With my code now capable of running on GPUs, it might seem obvious to implement GPU-versions of commonly used existing algorithms for SDPD such as simulated annealing (SA), parallel tempering and others. However, despite the parallel processing capabilities of GPUs, I found that the performance with GO methods is not particularly good (at least with my code!). Using SA as an example, then yes, it&#39;s possible to run thousands of simultaneous runs on a single GPU, but the number of iterations that can be performed per second is laughably slow in comparison to performance on a CPU. Therefore, because algorithms like SA take a large number of iterations converge, the performance benefits of parallel processing are offset by the amount of time needed to process the large number of iterations required to reach the global minimum. . In contrast to GO algorithms, LO algorithms with access to gradients converge much more rapidly. The automatic differentiation capabilities provided by PyTorch allow gradients to be calculated rapidly, without any additional code to be written. The gradients so obtained are exactly equivalent to the analytical gradient, and are obtained much more rapidly than the approximate gradient that would be obtained via the method of finite differences. Therefore, when processing large numbers of LO runs on a GPU, because they converge much more rapidly than GO methods, you don&#39;t need to wait for a long time to know if any of the runs have been successful! . The next piece of the puzzle is the idea that even if the global minimum is not located (i.e. the structure hasn&#39;t yet been solved), the previously optimised positions may contain some useful information about the crystal structure. This might not be obvious at first, but let&#39;s try to convince ourselves by taking a look at the hypersurface of verapamil hydrochloride, a structure with 23 degrees of freedom. This interactive figure shows a 2D slice through the hypersurface with all degrees of freedom set to their correct crystallographic values, apart from the fractional coordinates of the verapamil molecule along a and b, which form the axes plotted here. . . . I&#39;ll write another blog post in the future showing how to use the GALLOP code to generate plots like this. . There are several local minima present, each of which represents an incorrect structure that has either 21 or 22 of its 23 degrees of freedom correctly determined. Despite this, the $ chi^{2}$ at each local minimum gives no indication that the result is so close to correct. This means that even failed runs with high values of $ chi^{2}$ may be close to the global minimum. If we could accumulate and exploit this information to influence where the next set of local optimisations start, then it might be possible to save time in searching for the global minimum. . With this idea in mind, I tried a few things to attempt to recycle information from &quot;failed&quot; local optimisation attempts, including using kernel density estimation to try to resample various degrees of freedom depending based on the density of solutions that ended up with particular coordinates. This definitely showed an improvement relative to random sampling, but proved inconsistent in terms of the level of improvement obtained. Perhaps I&#39;ll revisit this in a future blog post as I still think there&#39;s something there that may be of use. . Eventually, I ended trying particle swarm optimisation (PSO) to attempt to recycle the optimised positions. A few things attracted me to PSO: . It&#39;s a simple algorithm to implement - just a few lines of code got it working as a proof of concept | The algorithm maintains a memory of &quot;good&quot; solutions so there&#39;s less risk of the algorithm moving in a bad direction and getting stuck there | It&#39;s shown great performance in a wide variety of domains | The performance improvement with PSO included was immediately obvious. . The last thing that was needed was a name. GPU-Accelerated LocaL Optimisation and Particle swarm provides both a description of the algorithm and an acronym that gives a hat-tip to DASH. Perfect! . The GALLOP algorithm . As described above, GALLOP is a hybrid algorithm which contains two components, a local optimiser and a particle swarm optimiser. These are described in more detail below. . Local optimisation . The local optimisation algorithm used in GALLOP by default is Adam. This algorithm is very popular for training neural networks, and efficient implementations are available in almost every deep learning library. . Adam incorporates two distinct innovations that improve its performance relative to (stochastic) gradient descent. . Adam has a per-parameter adaptive step size rather than a single global step size used for all parameters. This is useful as different degrees of freedom will have different effects on $ chi^2$ for the same percentage change in the parameter value. For example, the translation of a whole molecule within a unit cell affects the position of more scattering atoms than changing a torsion angle. What&#39;s nice is that Adam automatically adjusts the step size for each parameter as it goes, meaning that a suitable step size is used throughout optimisation. | Adam incorporates momentum, which helps it to escape shallow local minima, pass rapidly through flat regions of the hypersurface and dampens uncesessary oscillations in the optimisation trajectory. For an excellent overview of momentum (with a focus on ML applications), see this article: https://distill.pub/2017/momentum/ | Adam . Using the gradient obtained by automatic differentiation, $ bold{g}_t$, Adam stores exponentially decaying averages of the gradients, $ bold{m}_t$, and squared gradients, $ bold{v}_{t}$, which are then used in conjunction with the overall step size, $ alpha$, to give a suitable step size for each parameter being optimised. The parameters $ beta_1$ and $ beta_2$ are numbers less than one that control the rate at which the past gradients and squared gradients respectively decay. . $$ bold{m}_t = beta_{1} bold{m}_{t-1} + (1- beta_{1}) bold{g}_t $$ . $$ bold{v}_t = beta_{2} bold{v}_{t-1} + (1- beta_2) bold{g}_t^2$$ . Because $ bold{m}_t$ and $ bold{v}_{t}$ are initialised as vectors of zeros, the authors of Adam use the following corrective terms to reduce the effect of this biasing, which can be particularly problematic in the early stages of optimisation: . $$ hat{ bold{m}}_t = frac{ bold{m}_t}{1- beta_1}$$ . $$ hat{ bold{v}}_t = frac{ bold{v}_t}{1- beta_2}$$ . These bias corrected terms are then used to update the parameters to be optimised, $ bold{x}_t$, where $ epsilon$ is included to prevent numerical errors: . $$ bold{x}_{t+1} = bold{x}_t - frac{ alpha}{ sqrt{ hat{ bold{v}}_t} + epsilon} hat{ bold{m}}_t $$ . The authors of Adam suggest default parameters of $ beta_1 = 0.9$, $ beta_2 = 0.999$ and $ epsilon = 1 times 10^{-8}$. The step size, $ alpha$, must be set by the user. By default GALLOP sets $ beta_2 = 0.9$ which decays the past squared gradients more rapidly, and was found in our testing to be more effective than the default value. . Learning rate finder . To make life easy for end users (and myself), I wanted a way to avoid having to experiment to find a suitable step size ($ alpha$) to use in GALLOP. . The deep learning library, fast.ai includes a heuristic known as the learning rate finder, which is used to set the step size (referred to as the learning rate in ML-parlance) for deep learning experiments automatically. This is used in conjunction with a step-size alteration policy which is carried out during optimisation, as described here. . After some testing and experimentation, GALLOP now makes use of a slightly modified version, as described below. . A set of 200 log-linearly spaced learning rates are initialised, ranging from $1 times 10^{-4}$ and $0.15$. Starting with the smallest, GALLOP is run on the structure of interest, and the step size increased to the next value after every iteration. The sum of the $ chi^2$ values is recorded after each iteration, and subsequently plotted. . The minimum point on this plot, $ alpha_{min}$, is then used to give the step size. It may be scaled after considering the gradient of the line as the step size is increased beyond $ alpha_{min}$. . To do this, the step sizes and $ chi^2$ values are rescaled into the range 0-1. The x-axis is shifted such that $ alpha_{min}$ sits at 0, and the data plotted. If the gradient of the resultant curve (approximated by the red straight line below) is &gt; 0.5, then this is considered steep. A steep gradient implies a high sensitivity to the step size, and hence $ alpha_{min}$ is scaled by a factor of 0.5, i.e. GALLOP runs with a step size of $0.5 alpha_{min}$. A medium gradient (between 0.25 and 0.5) results in multiplication factor of 0.75, whilst a shallow gradient (less than 0.25) implies relative insensitivity to the step size, and hence results in a multiplication factor of 1.0. . In the GALLOP browser interface, this information is provided in a plot: . . In my testing, the learning rates obtained in this manner provide a good first attempt for GALLOP and provide reasonable performance over a wide variety of structures. However, this doesn&#39;t mean that they are optimal, and if a structure isn&#39;t solving, it might be worth looking at this parameter more closely. I tend to find that a learning rate of 0.03 - 0.05 tends to work well as a first attempt for most structures. . Particle Swarm optimisation . Particle Swarm Optimisation has previously been used in the context of SDPD in the program PeckCryst. The algorithm used in GALLOP is different to that used in PeckCryst in a number of ways which I&#39;ll try to highlight below. . The equations for the PSO are simple. The velocity of a particle at step $t+1$ $( bold{v}_{t+1})$, is calculated from the velocity at the previous step $( bold{v}_{t})$ and the position of the particle $( bold{x}_t)$ using the following equation: . $$ bold{v}_{t+1} = omega_{t} bold{v}_{t} + c_{1} bold{R}_1( bold{g}_{best} - bold{x}_t) + c_2 bold{R}_2( bold{x}_{best} - bold{x}_t)$$ . Where $ omega_{t}$ is the inertia of the particle (which controls how much the previous velocity influences the next velocity) and $c_1$ and $c_2$ control the maximum step size in the direction of the best particle in the swarm $( bold{g}_{best})$ and best position previously visited by the particle $( bold{x}_{best})$ respectively. In contrast to PeckCryst which uses scalars, in GALLOP, by default $ bold{R}_1$ and $ bold{R}_2$ are diagonal matrices with their elements drawn independently from a standard uniform distribution. This provides more variability in how the particles move which helps to improve the exploration. In addition, the maximum absolute velocity in GALLOP is limited to 1.0. . GALLOP calculates $ omega$ for particle $i$ by ranking all of the $N$ particles in the swarm in terms of their $ chi^2$ value, and the calculating their inertia using the following equation: . $$ omega_i = 0.4 + frac{Rank_i}{2N} $$ . This gives inertia values in the range 0.4 - 0.9, and means that the best particles slow down, whilst the worst particles in the swarm have higher inertias and hence are able to continue moving more rapidly towards (hopefully) promising areas of the hypersurface. . The degrees of freedom are then updated using the velocity and previous parameters according to: . $$ bold{x}_{t+1} = bold{x}_t + bold{v}_{t+1} $$ . Another difference to PeckCryst is the coordinate transform that is performed in GALLOP. The fractional coordinates are transformed to account for the repeating unit cell and the fact that coordinates of -0.1 and 0.9 are equivalent. The torsion angles are also transformed to ensure that the PSO treats angles of +180 and -180 degrees as equivalent. Molecular orientations, represented in GALLOP with quaternions, do not require any transformation. For local optimisation, the degrees of freedom that are optimised are: . $$ DoF_{(LO)} = [ bold{x}_{positions}, bold{x}_{quaternions}, bold{x}_{torsions}] $$ . These are then transformed as follows for use in the PSO: . $$ DoF_{(PSO)} = [ sin{2 pi bold{x}_{positions}}, cos{2 pi bold{x}_{positions}}, bold{x}_{quaternions}, sin{ bold{x}_{torsions}}, cos{ bold{x}_{torsions}}] $$ . Following the PSO update, these are transformed back for use in LO using the two-argument arctangent function which gives values in the range $- pi$ to $ pi$, which therefore necessitates additional scaling by a factor of $1/2 pi$ for the positions. . GALLOP . Bringing it all together, this flow chart shows how the GALLOP algorithm operates: . . Typically, 500 LO steps are performed prior to a single PSO step. . The results reported here demonstrate a significant improvement in performance relative to DASH. The success rate is &gt;30 times that of DASH, and the GPU-acceleration means that the time taken to process the runs is also significantly lower than can be accomplished without distributed computing for the DASH jobs. . In my next post, I&#39;ll go over how to use GALLOP to solve crystal structures. .",
            "url": "https://mspillman.github.io/blog/gallop/pxrd/python/2021/10/30/Introduction-to-GALLOP.html",
            "relUrl": "/gallop/pxrd/python/2021/10/30/Introduction-to-GALLOP.html",
            "date": " • Oct 30, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hi, I’m Mark. . I’m interested in solving the crystal structures of small molecules using powder diffraction data. . I also like learning about and applying machine learning to interesting problems. . In my spare time, I enjoy BJJ, playing the drums and cooking. .",
          "url": "https://mspillman.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Publications",
          "content": "Journal articles . Mark J. Spillman and Kenneth Shankland. “GALLOP: accelerated molecular crystal structure determination from powder diffraction data.” CrystEngComm (2021) doi: 10.1039/d1ce00978h . Okba Al Rahal, Mridul Majumder, Mark J. Spillman, Jacco van de Streek and Kenneth Shankland. “Co-Crystal Structures of Furosemide:Urea and Carbamazepine:Indomethacin Determined from Powder X-Ray Diffraction Data.” Crystals (2020). doi: 10.3390/cryst10010042 . Daniel Nicholls, Kenneth Shankland, Mark J. Spillman and Carole J. Elleman. “Rietveld-Based Quantitative Phase Analysis of Sugars in Confectionery.” Food Analytical Methods (2018) doi: 10.1007/s12161-018-1243-9 . Philippa B. Cranwell, Fred J. Davis, Joanne M. Elliott, John E. Mckendrick, Elizabeth M. Page and Mark J. Spillman. “Encouraging Independent Thought and Learning in First Year Practical Classes.” New directions in the teaching of physical sciences (2017) doi: 10.29311/NDTPS.V0I12.674 . Amanda R. Buist, David S. Edgeley, Elena A. Kabova, Alan R. Kennedy, Debbie Hooper, David G. Rollo, Kenneth Shankland, and Mark J. Spillman. “Salt and Ionic Cocrystalline Forms of Amides: Protonation of Carbamazepine in Aqueous Media.” Crystal Growth &amp; Design (2015) doi: 10.1021/acs.cgd.5b01223 . Mark J. Spillman, Kenneth Shankland, Adrian C. Williams and Jason C. Cole. “CDASH: a cloud-enabled program for structure solution from powder diffraction data.” Journal of Applied Crystallography (2015) doi: 10.1107/S160057671502049X . Kenneth Shankland, Mark J. Spillman, Elena A. Kabova, David S. Edgeley and Norman Shankland. “The Principles Underlying the Use of Powder Diffraction Data in Solving Pharmaceutical Crystal Structures.” Acta Crystallographica Section C (2013) doi: 10.1107/S0108270113028643 . Amanda R. Buist, Alan R. Kennedy, Kenneth Shankland, Norman Shankland and Mark J. Spillman. “Salt Forms of Amides: Protonation and Polymorphism of Carbamazepine and Cytenamide.” Crystal Growth &amp; Design (2013) doi: 10.1021/cg401341y . Mridul Majumder, Graham Buckton, Clare F. Rawlinson-Malone, Adrian C. Williams, Mark J. Spillman, Elna Pidcock and Kenneth Shankland. “Application of Hydrogen-Bond Propensity Calculations to an Indomethacin Nicotinamide (1 : 1) Co-Crystal.” CrystEngComm (2013) doi: 10.1039/C3CE40367J . Mridul Majumder, Graham Buckton, Clare F. Rawlinson-Malone, Adrian C Williams, Mark J. Spillman, Norman Shankland and Kenneth Shankland. “A Carbamazepine-Indomethacin (1 : 1) Cocrystal Produced by Milling.” CrystEngComm (2011) doi: 10.1039/C1CE05650F . . Books . Matthew J. Almond, Mark J. Spillman and Elizabeth M. Page. “Workbook in Inorganic Chemistry”, Oxford University Press (2017). ISBN: 9780198729501. Publisher . Chapter . Mark J. Spillman, Daniel Nicholls and Kenneth Shankland. “Experimental Analysis of Powder Diffraction Data.” (2020). doi: 10.1142/9789811204579_0001, chapter in “Handbook on Big Data and Machine Learning in the Physical Sciences”, World Scientific (2020) .",
          "url": "https://mspillman.github.io/blog/publications/",
          "relUrl": "/publications/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mspillman.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}