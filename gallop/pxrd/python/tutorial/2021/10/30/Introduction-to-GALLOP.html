<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Introduction to GALLOP | Mark Spillman</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Introduction to GALLOP" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An introduction to GALLOP, a recently published new method for solving the crystal structures of small molecules" />
<meta property="og:description" content="An introduction to GALLOP, a recently published new method for solving the crystal structures of small molecules" />
<link rel="canonical" href="https://mspillman.github.io/blog/gallop/pxrd/python/tutorial/2021/10/30/Introduction-to-GALLOP.html" />
<meta property="og:url" content="https://mspillman.github.io/blog/gallop/pxrd/python/tutorial/2021/10/30/Introduction-to-GALLOP.html" />
<meta property="og:site_name" content="Mark Spillman" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-10-30T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"An introduction to GALLOP, a recently published new method for solving the crystal structures of small molecules","url":"https://mspillman.github.io/blog/gallop/pxrd/python/tutorial/2021/10/30/Introduction-to-GALLOP.html","@type":"BlogPosting","headline":"Introduction to GALLOP","dateModified":"2021-10-30T00:00:00-05:00","datePublished":"2021-10-30T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://mspillman.github.io/blog/gallop/pxrd/python/tutorial/2021/10/30/Introduction-to-GALLOP.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://mspillman.github.io/blog/feed.xml" title="Mark Spillman" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Mark Spillman</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Introduction to GALLOP</h1><p class="page-description">An introduction to GALLOP, a recently published new method for solving the crystal structures of small molecules</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-10-30T00:00:00-05:00" itemprop="datePublished">
        Oct 30, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      14 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#GALLOP">GALLOP</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#PXRD">PXRD</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Python">Python</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#tutorial">tutorial</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Introduction">Introduction </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Background">Background </a></li>
<li class="toc-entry toc-h2"><a href="#Rationale-for-GALLOP">Rationale for GALLOP </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#The-GALLOP-algorithm">The GALLOP algorithm </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Local-optimisation-with-Adam">Local optimisation with Adam </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Adam-equations">Adam equations </a></li>
<li class="toc-entry toc-h3"><a href="#Learning-rate-finder">Learning rate finder </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Particle-Swarm-optimisation">Particle Swarm optimisation </a></li>
<li class="toc-entry toc-h2"><a href="#GALLOP">GALLOP </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-10-30-Introduction-to-GALLOP.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Introduction">
<a class="anchor" href="#Introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction<a class="anchor-link" href="#Introduction"> </a>
</h1>
<p>This post is intended to give an overview of <a href="https://github.com/mspillman/gallop"><em>GALLOP</em></a>, an algorithm I recently <a href="https://pubs.rsc.org/en/content/articlelanding/2021/ce/d1ce00978h">published</a> alongside my friend, PhD supervisor and colleague, <a href="https://www.reading.ac.uk/pharmacy/staff/professor-kenneth-shankland">Prof. Kenneth Shankland</a>. I assume you have some familiarity with global optimisation (GO) methods for crystal structure determination from powder diffraction data (SDPD). If you need a quick primer, I recommend the following resources:</p>
<ul>
<li><a href="https://www.worldscientific.com/doi/epdf/10.1142/9789811204579_0001">Experimental Analysis of Powder Diﬀraction Data</a></li>
<li><a href="https://it.iucr.org/Ha/ch4o1v0001/">An overview of currently used structure determination methods for powder diffraction data</a></li>
<li><a href="https://onlinelibrary.wiley.com/doi/abs/10.1107/S0108270113028643">The principles underlying the use of powder diffraction data in solving pharmaceutical crystal structures</a></li>
<li><a href="https://it.iucr.org/Ha/ch4o3v0001/">Real-space methods for structure solution from powder-diffraction data: application to molecular structures</a></li>
</ul>
<p>In the next post, I'll go over how to use <em>GALLOP</em> to solve the crystal structure of a small molecule.</p>
<p>Abbreviations I'll be using:</p>
<ul>
<li>SDPD = Crystal structure determination from powder diffraction data</li>
<li>PXRD = Powder X-ray diffraction data</li>
<li>GO = Global optimisation</li>
<li>LO = Local optimisation</li>
<li>GPU = Graphics processing unit</li>
<li>CPU = Central processing unit</li>
<li>SA = Simulated annealing</li>
<li>PSO = Particle swarm optimisation</li>
<li>ML = Machine learning</li>
</ul>
<h2 id="Background">
<a class="anchor" href="#Background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background<a class="anchor-link" href="#Background"> </a>
</h2>
<p><em>GALLOP</em> is the culmination of several years of work, which kicked off back in 2010 with <a href="http://scripts.iucr.org/cgi-bin/paper?S0021889810008113">an article published by Kenneth Shankland and co-workers</a>, who showed that contrary to the wisdom at the time, local optimisation (LO) algorithms were capable of solving the crystal structures of small molecules, provided that several tens of thousands of attempts from random starting positions on the $\chi^{2}$ hypersurface were performed.</p>
<p>Interestingly, they showed that using this method, the global minimum on the hypersurface was located far more frequently than was expected. This indicates that <em>"the topology of the surface is such that the net ‘catchment’ area of stationary points with very low values of $\chi^{2}$ is significantly larger than that of the vast majority of stationary points."</em> The figure below, taken from the article, shows the (binned) distribution of $\chi^{2}$ values for stationary points on the hypersurface for famotidine. Note the use of a log-scale on the y-axis.</p>
<p><img src="/blog/images/copied_from_nb/images/qNRM-famot.png" alt="Shankland et al, 2010"></p>
<p>I carried on investigating this method as part of my PhD, and my results confirmed that this approach is effective at solving crystal structures, even of high-complexity. However, despite the intriguing results, the approach was not adopted on a wide scale by the SDPD community, perhaps because the performance is approximately the same existing GO-based programs. The code I was using was written in C++, a language I do not know at all well, so I was unable to contribute much to its further development.</p>
<p>A few years after finishing my PhD, I decided I wanted to try writing my own SDPD code in Python. Whilst Python is notoriously slow, my rationale was that Python is much easier to learn than C++, so should provide a lower barrier to entry for PhD students seeking to try out new ideas for SDPD. My first prototype used <a href="https://numpy.org/">numpy</a> to try to speed up the code, and borrowed heavily from <a href="https://pymatgen.org/">pymatgen</a>, a fantastic open-source library with lots of crystallographic functionality. Eventually with some help from Kenneth, I had a system which allowed me to easily try out lots of different algorithms, such as those included in <a href="https://docs.scipy.org/doc/scipy/reference/optimize.html">scipy.optimize</a>, which features a variety of local and global optimisation algorithms.</p>
<p>In parallel to this, it seemed like every day incredible new results from the field of <em>deep learning</em> were coming out, showing state-of-the-art performance in wide variety of domains. Most neural networks are trained using <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a>, an algorithm which makes use of <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a> to calculate the gradient of the cost function with respect to the parameters of the neural network. Variants of stochastic gradient descent are then used to modify the parameters of the neural network in order to improve the performance of the neural network as measured by the cost function. Whilst neural networks have been in use <a href="https://arxiv.org/abs/1404.7828">for over half a century</a>, part of the reason for the explosion in activity was the availability of GPUs and tools to leverage their parallel processing capabilities.</p>
<p>I took an interest in this, and quickly realised that most of the libraries used for this work had well supported python APIs. Some of them, such as <a href="https://pytorch.org/">PyTorch</a>, are so similar to numpy that it seemed logical to try to port my code to make use of these libraries. This would give both GPU-acceleration and automatic differentiation capabilities for very little effort!</p>
<h2 id="Rationale-for-GALLOP">
<a class="anchor" href="#Rationale-for-GALLOP" aria-hidden="true"><span class="octicon octicon-link"></span></a>Rationale for GALLOP<a class="anchor-link" href="#Rationale-for-GALLOP"> </a>
</h2>
<p>With my code now capable of running on GPUs, it might seem obvious to implement GPU-versions of commonly used existing algorithms for SDPD such as simulated annealing (SA), parallel tempering and others. However, despite the parallel processing capabilities of GPUs, I found that the performance with GO methods is not particularly good (at least with my code!). Using SA as an example, then yes, it's possible to run thousands of simultaneous runs on a single GPU, but the number of iterations that can be performed per second is laughably slow in comparison to performance on a CPU. Therefore, because algorithms like SA take a large number of iterations converge, the performance benefits of parallel processing are offset by the sheer amount of time needed to process the number of iterations required to reach the global minimum - you end up waiting a long time for a result!</p>
<p>In contrast to GO algorithms, LO algorithms with access to gradients converge much more rapidly. The automatic differentiation capabilities provided by PyTorch allow gradients to be calculated rapidly, without any additional code to be written. The gradients so obtained are exactly equivalent to the analytical gradient, and are obtained much more rapidly than the <em>approximate</em> gradient that would be obtained via the method of finite differences. Therefore, when processing large numbers of LO runs on a GPU, because they converge much more rapidly than GO methods, you don't need to wait for a long time to know if any of the runs have been successful!</p>
<p>The next piece of the puzzle is the idea that even if the global minimum is not located (<em>i.e.</em> the structure hasn't yet been solved), the previously optimised positions may contain some useful information about the crystal structure. This might not be obvious at first, but let's try to convince ourselves by taking a look at the hypersurface of verapamil hydrochloride, a structure with 23 degrees of freedom. This figure shows a 2D slice through the hypersurface of verapamil with all degrees of freedom set to their correct crystallographic values apart from the fractional coordinates of the verapamil molecule along <em>a</em> and <em>b</em>, which form the axes plotted here.</p>
<p><img src="/blog/images/copied_from_nb/images/verap_ab_slice.png" alt="Verapmil hypersurface"></p>
<p>As you can see, the global minimum is located in the upper left of the image. There are 3 local minima present, each of which represents an incorrect structure that has either 21 or 22 of its 23 degrees of freedom correctly determined. Despite this, the $\chi^{2}$ at each local minimum gives no indication that the result is so close to correct. I'll write another blog post in the future showing how to use the <em>GALLOP</em> code to generate plots like this.</p>
<p>I tried a few things to attempt to recycle information from "failed" local optimisation attempts, including using <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">kernel density estimation</a> to try to resample various degrees of freedom depending based on the density of solutions that ended up with particular coordinates. This definitely showed an improvement relative to random sampling, but proved inconsistent in terms of the level of improvement offered. Perhaps I'll revisit this in a future blog post as I still think there's something there that may be of use.</p>
<p>Eventually, I ended trying particle swarm optimisation (PSO) to attempt to recycle the optimised positions. A few things attracted me to PSO:</p>
<ol>
<li>It's a simple algorithm to implement - literally just a couple of lines of code to get it working as a proof of concept</li>
<li>The algorithm maintains a memory of "good" solutions so there's less risk of the algorithm moving in a bad direction and getting stuck there</li>
<li>It's shown great performance in a wide variety of domains</li>
</ol>
<p>The performance improvement with PSO included was immediately obvious.</p>
<p>The last thing that was needed was a name. <strong>G</strong>PU-<strong>A</strong>ccelerated <strong>L</strong>oca<strong>L</strong> <strong>O</strong>ptimisation and <strong>P</strong>article swarm provides both a description of the algorithm and an acronym that gives a hat-tip to <em>DASH</em>. Perfect!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-GALLOP-algorithm">
<a class="anchor" href="#The-GALLOP-algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>The <em>GALLOP</em> algorithm<a class="anchor-link" href="#The-GALLOP-algorithm"> </a>
</h1>
<p>As described above, <em>GALLOP</em> is a hybrid algorithm which contains two components, a local optimiser and a particle swarm optimiser. These are described in more detail below.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Local-optimisation-with-Adam">
<a class="anchor" href="#Local-optimisation-with-Adam" aria-hidden="true"><span class="octicon octicon-link"></span></a>Local optimisation with Adam<a class="anchor-link" href="#Local-optimisation-with-Adam"> </a>
</h2>
<p>The local optimisation algorithm used in <em>GALLOP</em> by default is <a href="https://arxiv.org/abs/1412.6980">Adam</a>. This algorithm is very popular for training neural networks, and efficient implementations are available in almost every deep learning library.</p>
<p>Adam incorporates two distinct innovations that improve its performance relative to (stochastic) gradient descent.</p>
<ol>
<li>Adam has a per-parameter adaptive step size rather than a single global step size used for all parameters. This is useful as different degrees of freedom will have different effects on $\chi^2$ for the same percentage change in the parameter value. For example, the translation of a whole molecule within a unit cell affects the position of more scattering atoms than changing a torsion angle. What's nice is that Adam automatically adjusts the step size for each parameter as it goes, meaning that a suitable step size is used throughout optimisation.</li>
</ol>
<ol>
<li>Adam incorporates momentum, which helps it to escape shallow local minima, pass rapidly through flat regions of the hypersurface and dampens uncesessary oscillations in the optimisation trajectory. For an <em>excellent</em> overview of momentum (with a focus on ML applications), see this article: <a href="https://distill.pub/2017/momentum/">https://distill.pub/2017/momentum/</a>
</li>
</ol>
<h3 id="Adam-equations">
<a class="anchor" href="#Adam-equations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adam equations<a class="anchor-link" href="#Adam-equations"> </a>
</h3>
<p>Using the gradient obtained by automatic differentiation, $\bold{g}_t$, Adam stores exponentially decaying averages of the gradients, $\bold{m}_t$, and squared gradients, $\bold{ν}_t$, which are then used in conjunction with the overall step size, $\alpha$, to give a suitable step size for each parameter being optimised. The parameters $\beta_1$ and $\beta_2$ are numbers less than one that control the rate at which the past gradients and squared gradients respectively decay.</p>
<p>
$$ \bold{m}_t = \beta_{1} \bold{m}_{t-1} + (1-\beta_{1})\bold{g}_t $$
</p>
<p>
$$\bold{v}_t = \beta_{2} \bold{v}_{t-1} + (1-\beta_2)\bold{g}_t^2$$
</p>
<p>Because $\bold{m}_t$ and $\bold{ν}_t$ are initialised as vectors of zeros, the authors of Adam use the following corrective terms to reduce the effect of this biasing, which can be particularly problematic in the early stages of training:</p>
<p>
$$ \hat{\bold{m}}_t = \frac{\bold{m}_t}{1-\beta_1}$$
</p>
<p>
$$ \hat{\bold{v}}_t = \frac{\bold{v}_t}{1-\beta_2}$$
</p>
<p>These bias corrected terms are then used to update the parameters to be optimised, $\bold{\theta}_t$, where $\epsilon$ is included to prevent numerical errors:</p>
<p>
$$ \bold{\theta}_{t+1} = \bold{\theta}_t - \frac{\alpha}{\sqrt{\hat{\bold{v}}_t} + \epsilon}\hat{\bold{m}}_t $$
</p>
<p>The authors of Adam suggest default parameters of $\beta_1 = 0.9$, $\beta_2 = 0.999$ and $\epsilon = 1 \times 10^{-8}$. The step size, $\alpha$, must be set by the user.
By default GALLOP sets $\beta_2 = 0.9$ which decays the past squared gradients more rapidly, and was found in our testing to be more effective than the default value.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Learning-rate-finder">
<a class="anchor" href="#Learning-rate-finder" aria-hidden="true"><span class="octicon octicon-link"></span></a>Learning rate finder<a class="anchor-link" href="#Learning-rate-finder"> </a>
</h3>
<p>To make life easy for end users (and myself), I wanted a way to avoid having to experiment to find a suitable step size ($\alpha$) to use in <em>GALLOP</em>.</p>
<p>The deep learning library, <a href="https://www.fast.ai/">fast.ai</a> includes a heuristic known as the <a href="https://fastai1.fast.ai/callbacks.lr_finder.html">learning rate finder</a>, which is used to set the step size (referred to as the learning rate in ML-parlance) for deep learning experiments automatically. This is used in conjunction with a step-size alteration policy which is carried out during optimisation, as described <a href="https://arxiv.org/abs/1708.07120">here</a>.</p>
<p>After some testing and experimentation, GALLOP makes use of a slightly modified version, as described below.</p>
<p>A set of 200 log-linearly spaced learning rates are initialised, ranging from $1 \times 10^{-4}$ and 0.15. Starting with the smallest, GALLOP is run on the structure of interest, and the step size increased to the next value after every iteration. The sum of the $\chi^2$ values is recorded after each iteration, and subsequently plotted.</p>
<p>The minimum point on this plot, $\alpha_{min}$, is then used to give the step size. It may be scaled after considering the gradient of the line as the step size is increased beyond $\alpha_{min}$.</p>
<p>To do this, the step sizes and $\chi^2$ values are rescaled into the range 0-1. The x-axis is shifted such that $\alpha_{min}$ sits at 0, and the data plotted. If the gradient of the resultant curve (approximated by the red straight line below) is &gt; 1, then this is considered steep. A steep gradient implies a high sensitivity to the step size, and hence $\alpha_{min}$ is scaled by a factor of 0.5. A medium gradient (between 0.5 and 1) results in multiplication factor of 0.75, whilst a shallow gradient (less than 0.5) implies relative insensitivity to the step size, and hence results in a scaling factor of 1.0.</p>
<p>In the <em>GALLOP</em> browser interface, this is provided as a plot:</p>
<p><img src="/blog/images/copied_from_nb/images/lr_finder_output.png" alt="Step size finder"></p>
<p>In my testing, the learning rates obtained in this manner provide a good first attempt for GALLOP and provide reasonable performance over a wide variety of structures. However, this doesn't mean that they are optimal, and if a structure isn't solving, it is one thing that might merit adjustment.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Particle-Swarm-optimisation">
<a class="anchor" href="#Particle-Swarm-optimisation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Particle Swarm optimisation<a class="anchor-link" href="#Particle-Swarm-optimisation"> </a>
</h2>
<p><a href="https://en.wikipedia.org/wiki/Particle_swarm_optimization">Particle Swarm Optimisation</a> has previously been used in the context of SDPD in the program <a href="http://scripts.iucr.org/cgi-bin/paper?S0021889809034207">PeckCryst</a>. The algorithm used in GALLOP is different to that used in PeckCryst in a number of ways which I'll try to highlight below.</p>
<p>The equations for the PSO are simple. The velocity of the particles at step $t+1$, $\bold{v}_{t+1}$, is calculated from the velocity at the previous step $\bold{v}_{t}$ and the position of the particles $\bold{x}_t$ using the following equation:</p>
<p>
$$\bold{v}_{t+1} = \omega_{t} \bold{v}_{t} + c_{1}\bold{R}_1(\bold{g}_{best} - \bold{x}_t) + c_2\bold{R}_2(\bold{x}_{best} - \bold{x}_t)$$
</p>
<p>Where $\omega_{t}$ is the inertia of the particle which controls how much the previous velocity influences the next velocity. $c_1$ and $c_2$ control the maximum step size in the direction of the best particle in the swarm, $\bold{g}_{best}$ and best position previously visited by the particle, \bold{x}_{best}. In contrast to PeckCryst which uses scalars, in $GALLOP$, by default $\bold{R}_1$ and $\bold{R}_2$ are diagonal matrices with their elements drawn independently from a standard uniform distribution. This provides more variability in how the particles move which helps to improve the exploration.</p>
<p><em>GALLOP</em> calculates $\omega_{t}$ for each particle by randing all of the particles in the swarm in terms of their $\chi^2$ value, and then doing the following:</p>
<p>
$$ \omega_i = 0.4 + \frac{Rank_i}{2N} $$
</p>
<p>This gives values in the range 0.4 - 0.9, and means that the best particles (lowest rank) slow down, whilst the worst particles in the swarm have higher inertias and hence are able to continue moving more rapidly.</p>
<p>By default, the maximum absolute velocity is limited to 1.0.</p>
<p>The parameters are then updated using the velocity and previous parameters according to:</p>
<p>
$$\bold{x}_{t+1} = \bold{x}_t + \bold{v}_{t+1} $$
</p>
<p>Another difference to PeckCryst is the coordinate transform that is performed in <em>GALLOP</em>. The fractional coordinates are transformed to account for the repeating unit cell and hence coordinates of -0.1 and 0.9 are eqivalent. In addition to this, the torsion angles are also transformed to ensure that the PSO treats angles of +180 and -180 degrees as equivalent. GALLOP makes use of quaternions to represent molecular orientations, which do not require any transformation. For local optimisation, the degrees of freedom are:</p>
<p>
$$ DoF_{LO} = [\bold{x}_{pos}, \bold{x}_{quaternions}, \bold{x}_{torsions}] $$
</p>
<p>These are then transformed as follows for use in the PSO:</p>
<p>
$$ DoF_{PSO} = [\sin{2\pi\bold{x}_{pos}}, \cos{2\pi\bold{x}_{pos}}, \bold{x}_{quaternions}, \sin{\bold{x}_{torsions}}, \cos{\bold{x}_{torsions}}] $$
</p>
<p>Following the PSO update, these are transformed back for use in LO using the <a href="https://en.wikipedia.org/wiki/Atan2">two-argument arctangent function</a> which gives values in the range $-\pi$ to $\pi$, and additional scaling by a factor of $1/2\pi$ for the positions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="GALLOP">
<a class="anchor" href="#GALLOP" aria-hidden="true"><span class="octicon octicon-link"></span></a>GALLOP<a class="anchor-link" href="#GALLOP"> </a>
</h2>
<p>Bringing it all together, this flow chart shows how <em>GALLOP</em> works:</p>
<p><img src="/blog/images/copied_from_nb/images/flowchart.png" alt="Gallop flow chart"></p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="mspillman/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/gallop/pxrd/python/tutorial/2021/10/30/Introduction-to-GALLOP.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Musings on crystallography, powder diffraction and machine learning. All opinions my own.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/mspillman" title="mspillman"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/gallop_sdpd" title="gallop_sdpd"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
